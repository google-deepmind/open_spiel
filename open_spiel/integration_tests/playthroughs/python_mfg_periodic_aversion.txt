game: python_mfg_periodic_aversion

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.MEAN_FIELD
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Mean-Field Periodic Aversion Game"
GameType.max_num_players = 1
GameType.min_num_players = 1
GameType.parameter_specification = ["coef_aversion", "dt", "horizon", "n_actions_per_side", "size", "volatility", "xmax", "xmin"]
GameType.provides_information_state_string = False
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.REWARDS
GameType.short_name = "python_mfg_periodic_aversion"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 21
PolicyTensorShape() = [21]
MaxChanceOutcomes() = 21
GetParameters() = {coef_aversion=1.0,dt=0.01,horizon=20,n_actions_per_side=10,size=21,volatility=1.0,xmax=1.0,xmin=0.0}
NumPlayers() = 1
MinUtility() = -inf
MaxUtility() = inf
UtilitySum() = 0.0
ObservationTensorShape() = x: [21], t: [21]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 42
MaxGameLength() = 20
ToString() = "python_mfg_periodic_aversion(coef_aversion=1.0,dt=0.01,horizon=20,n_actions_per_side=10,size=21,volatility=1.0,xmax=1.0,xmin=0.0)"

# State 0
# initial
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = ""
ObservationString(0) = "initial"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
ObservationTensor(0).t: ◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
ChanceOutcomes() = [(0,0.047619), (1,0.047619), (2,0.047619), (3,0.047619), (4,0.047619), (5,0.047619), (6,0.047619), (7,0.047619), (8,0.047619), (9,0.047619), (10,0.047619), (11,0.047619), (12,0.047619), (13,0.047619), (14,0.047619), (15,0.047619), (16,0.047619), (17,0.047619), (18,0.047619), (19,0.047619), (20,0.047619)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
StringLegalActions() = ["-10", "-9", "-8", "-7", "-6", "-5", "-4", "-3", "-2", "-1", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]

# Apply action "5"
action: 15

# State 1
# (15, 0)
IsTerminal() = False
History() = [15]
HistoryString() = "15"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "15"
ObservationString(0) = "(15, 0)"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯
ObservationTensor(0).t: ◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
Rewards() = [-0.216904]
Returns() = [-0.216904]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
StringLegalActions() = ["-10", "-9", "-8", "-7", "-6", "-5", "-4", "-3", "-2", "-1", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]

# Apply action "-10"
action: 0

# State 2
# (5, 0)_a_mu
IsTerminal() = False
History() = [15, 0]
HistoryString() = "15, 0"
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = "15, 0"
ObservationString(0) = "(5, 0)_a_mu"
ObservationTensor(0).x: ◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
ObservationTensor(0).t: ◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
ChanceOutcomes() = [(0,7.4336e-07), (1,7.99187e-06), (2,6.69151e-05), (3,0.000436341), (4,0.00221592), (5,0.00876415), (6,0.0269955), (7,0.0647588), (8,0.120985), (9,0.176033), (10,0.199471), (11,0.176033), (12,0.120985), (13,0.0647588), (14,0.0269955), (15,0.00876415), (16,0.00221592), (17,0.000436341), (18,6.69151e-05), (19,7.99187e-06), (20,7.4336e-07)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
StringLegalActions() = ["-10", "-9", "-8", "-7", "-6", "-5", "-4", "-3", "-2", "-1", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]

# Apply action "10"
action: 20

# State 3
# (15, 1)_a
IsTerminal() = False
History() = [15, 0, 20]
HistoryString() = "15, 0, 20"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.MEAN_FIELD
InformationStateString(0) = "15, 0, 20"
ObservationString(0) = "(15, 1)_a"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯
ObservationTensor(0).t: ◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ['(0, 1)_a', '(1, 1)_a', '(2, 1)_a', '(3, 1)_a', '(4, 1)_a', '(5, 1)_a', '(6, 1)_a', '(7, 1)_a', '(8, 1)_a', '(9, 1)_a', '(10, 1)_a', '(11, 1)_a', '(12, 1)_a', '(13, 1)_a', '(14, 1)_a', '(15, 1)_a', '(16, 1)_a', '(17, 1)_a', '(18, 1)_a', '(19, 1)_a', '(20, 1)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 4
# (15, 1)
IsTerminal() = False
History() = [15, 0, 20]
HistoryString() = "15, 0, 20"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "15, 0, 20"
ObservationString(0) = "(15, 1)"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯
ObservationTensor(0).t: ◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
Rewards() = [-12.7169]
Returns() = [-12.7169]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
StringLegalActions() = ["-10", "-9", "-8", "-7", "-6", "-5", "-4", "-3", "-2", "-1", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]

# Apply action "-1"
action: 9

# State 5
# Apply action "6"
action: 16

# State 6
# (20, 2)_a
IsTerminal() = False
History() = [15, 0, 20, 9, 16]
HistoryString() = "15, 0, 20, 9, 16"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.MEAN_FIELD
InformationStateString(0) = "15, 0, 20, 9, 16"
ObservationString(0) = "(20, 2)_a"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉
ObservationTensor(0).t: ◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ['(0, 2)_a', '(1, 2)_a', '(2, 2)_a', '(3, 2)_a', '(4, 2)_a', '(5, 2)_a', '(6, 2)_a', '(7, 2)_a', '(8, 2)_a', '(9, 2)_a', '(10, 2)_a', '(11, 2)_a', '(12, 2)_a', '(13, 2)_a', '(14, 2)_a', '(15, 2)_a', '(16, 2)_a', '(17, 2)_a', '(18, 2)_a', '(19, 2)_a', '(20, 2)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 7
# (20, 2)
IsTerminal() = False
History() = [15, 0, 20, 9, 16]
HistoryString() = "15, 0, 20, 9, 16"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "15, 0, 20, 9, 16"
ObservationString(0) = "(20, 2)"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉
ObservationTensor(0).t: ◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
Rewards() = [-0.321904]
Returns() = [-0.321904]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
StringLegalActions() = ["-10", "-9", "-8", "-7", "-6", "-5", "-4", "-3", "-2", "-1", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]

# Apply action "4"
action: 14

# State 8
# Apply action "6"
action: 16

# State 9
# (9, 3)_a
IsTerminal() = False
History() = [15, 0, 20, 9, 16, 14, 16]
HistoryString() = "15, 0, 20, 9, 16, 14, 16"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.MEAN_FIELD
InformationStateString(0) = "15, 0, 20, 9, 16, 14, 16"
ObservationString(0) = "(9, 3)_a"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯
ObservationTensor(0).t: ◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ['(0, 3)_a', '(1, 3)_a', '(2, 3)_a', '(3, 3)_a', '(4, 3)_a', '(5, 3)_a', '(6, 3)_a', '(7, 3)_a', '(8, 3)_a', '(9, 3)_a', '(10, 3)_a', '(11, 3)_a', '(12, 3)_a', '(13, 3)_a', '(14, 3)_a', '(15, 3)_a', '(16, 3)_a', '(17, 3)_a', '(18, 3)_a', '(19, 3)_a', '(20, 3)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 10
# Apply action "7"
action: 17

# State 11
# Apply action "1"
action: 11

# State 12
# Set mean field distribution to be uniform
action: update_distribution

# State 13
# Apply action "-7"
action: 3

# State 14
# Apply action "-9"
action: 1

# State 15
# Set mean field distribution to be uniform
action: update_distribution

# State 16
# Apply action "3"
action: 13

# State 17
# Apply action "8"
action: 18

# State 18
# Set mean field distribution to be uniform
action: update_distribution

# State 19
# Apply action "-3"
action: 7

# State 20
# Apply action "-2"
action: 8

# State 21
# Set mean field distribution to be uniform
action: update_distribution

# State 22
# Apply action "7"
action: 17

# State 23
# Apply action "-3"
action: 7

# State 24
# Set mean field distribution to be uniform
action: update_distribution

# State 25
# Apply action "5"
action: 15

# State 26
# Apply action "-6"
action: 4

# State 27
# Set mean field distribution to be uniform
action: update_distribution

# State 28
# Apply action "-4"
action: 6

# State 29
# Apply action "-6"
action: 4

# State 30
# Set mean field distribution to be uniform
action: update_distribution

# State 31
# (0, 10)
IsTerminal() = False
History() = [15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4]
HistoryString() = "15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4"
ObservationString(0) = "(0, 10)"
ObservationTensor(0).x: ◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
ObservationTensor(0).t: ◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯
Rewards() = [-2.1969]
Returns() = [-2.1969]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
StringLegalActions() = ["-10", "-9", "-8", "-7", "-6", "-5", "-4", "-3", "-2", "-1", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]

# Apply action "-4"
action: 6

# State 32
# Apply action "0"
action: 10

# State 33
# (17, 11)_a
IsTerminal() = False
History() = [15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4, 6, 10]
HistoryString() = "15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4, 6, 10"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.MEAN_FIELD
InformationStateString(0) = "15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4, 6, 10"
ObservationString(0) = "(17, 11)_a"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯
ObservationTensor(0).t: ◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ['(0, 11)_a', '(1, 11)_a', '(2, 11)_a', '(3, 11)_a', '(4, 11)_a', '(5, 11)_a', '(6, 11)_a', '(7, 11)_a', '(8, 11)_a', '(9, 11)_a', '(10, 11)_a', '(11, 11)_a', '(12, 11)_a', '(13, 11)_a', '(14, 11)_a', '(15, 11)_a', '(16, 11)_a', '(17, 11)_a', '(18, 11)_a', '(19, 11)_a', '(20, 11)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 34
# Apply action "6"
action: 16

# State 35
# Apply action "1"
action: 11

# State 36
# Set mean field distribution to be uniform
action: update_distribution

# State 37
# Apply action "10"
action: 20

# State 38
# Apply action "10"
action: 20

# State 39
# Set mean field distribution to be uniform
action: update_distribution

# State 40
# Apply action "9"
action: 19

# State 41
# Apply action "8"
action: 18

# State 42
# Set mean field distribution to be uniform
action: update_distribution

# State 43
# Apply action "-10"
action: 0

# State 44
# Apply action "-2"
action: 8

# State 45
# Set mean field distribution to be uniform
action: update_distribution

# State 46
# Apply action "7"
action: 17

# State 47
# Apply action "4"
action: 14

# State 48
# Set mean field distribution to be uniform
action: update_distribution

# State 49
# Apply action "-4"
action: 6

# State 50
# Apply action "6"
action: 16

# State 51
# Set mean field distribution to be uniform
action: update_distribution

# State 52
# Apply action "-6"
action: 4

# State 53
# Apply action "0"
action: 10

# State 54
# Set mean field distribution to be uniform
action: update_distribution

# State 55
# Apply action "-8"
action: 2

# State 56
# Apply action "8"
action: 18

# State 57
# Set mean field distribution to be uniform
action: update_distribution

# State 58
# Apply action "-7"
action: 3

# State 59
# Apply action "10"
action: 20

# State 60
# (17, 20)_a
IsTerminal() = True
History() = [15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4, 6, 10, 16, 11, 20, 20, 19, 18, 0, 8, 17, 14, 6, 16, 4, 10, 2, 18, 3, 20]
HistoryString() = "15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4, 6, 10, 16, 11, 20, 20, 19, 18, 0, 8, 17, 14, 6, 16, 4, 10, 2, 18, 3, 20"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.TERMINAL
InformationStateString(0) = "15, 0, 20, 9, 16, 14, 16, 17, 11, 3, 1, 13, 18, 7, 8, 17, 7, 15, 4, 6, 4, 6, 10, 16, 11, 20, 20, 19, 18, 0, 8, 17, 14, 6, 16, 4, 10, 2, 18, 3, 20"
ObservationString(0) = "(17, 20)_a"
ObservationTensor(0).x: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯
ObservationTensor(0).t: ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉
Rewards() = [0]
Returns() = [0]
