game: negotiation(rng_seed=100,utterance_dim=2,num_symbols=3)

GameType.chance_mode = ChanceMode.SAMPLED_STOCHASTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.IMPERFECT_INFORMATION
GameType.long_name = "Negotiation"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = ["enable_proposals", "enable_utterances", "num_items", "num_symbols", "rng_seed", "utterance_dim"]
GameType.provides_information_state_string = False
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "negotiation"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 226
PolicyTensorShape() = [226]
MaxChanceOutcomes() = 1
GetParameters() = {enable_proposals=True,enable_utterances=True,num_items=3,num_symbols=3,rng_seed=100,utterance_dim=2}
NumPlayers() = 2
MinUtility() = -150.0
MaxUtility() = 150.0
UtilitySum() = None
ObservationTensorShape() = [81]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 81
MaxGameLength() = 20
ToString() = "negotiation(num_symbols=3,rng_seed=100,utterance_dim=2)"

# State 0
# Initial chance node
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
ObservationString(0) = "ChanceNode -- no observation"
ObservationString(1) = "ChanceNode -- no observation"
ObservationTensor(0): ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
ObservationTensor(1): ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
SerializeState() = "chance"
ChanceOutcomes() = [(0, 1.0)]
LegalActions() = [0]
StringLegalActions() = ["chance outcome 0"]

# Apply action "chance outcome 0"
action: 0

# State 1
# Max steps: 4
# Item pool: 0 1 4
# Agent 0 util vec: 2 9 4
# Agent 1 util vec: 1 9 6
# Current player: 0
# Turn Type: Proposal
IsTerminal() = False
History() = [0]
HistoryString() = "0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
ObservationString(0) = "Max steps: 4\nItem pool: 0 1 4\nAgent 0 util vec: 2 9 4\nCurrent player: 0\nTurn Type: Proposal\n"
ObservationString(1) = "Max steps: 4\nItem pool: 0 1 4\nAgent 1 util vec: 1 9 6\nCurrent player: 0\nTurn Type: Proposal\n"
ObservationTensor(0): ◉◯◉◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
ObservationTensor(1): ◉◯◉◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯
SerializeState() = "4\n0 1 4\n2 9 4\n1 9 6\n0\n"
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]
StringLegalActions() = ["Proposal: [0, 0, 0]", "Proposal: [0, 0, 1]", "Proposal: [0, 0, 2]", "Proposal: [0, 0, 3]", "Proposal: [0, 0, 4]", "Proposal: [0, 1, 0]", "Proposal: [0, 1, 1]", "Proposal: [0, 1, 2]", "Proposal: [0, 1, 3]", "Proposal: [0, 1, 4]"]

# Apply action "Proposal: [0, 1, 3]"
action: 9

# State 2
# Max steps: 4
# Item pool: 0 1 4
# Agent 0 util vec: 2 9 4
# Agent 1 util vec: 1 9 6
# Current player: 0
# Turn Type: Utterance
# Player 0 proposes: [0, 1, 3]
IsTerminal() = False
History() = [0, 9]
HistoryString() = "0, 9"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
ObservationString(0) = "Max steps: 4\nItem pool: 0 1 4\nAgent 0 util vec: 2 9 4\nCurrent player: 0\nTurn Type: Utterance\nMost recent proposal: [0, 1, 3]\n"
ObservationString(1) = "Max steps: 4\nItem pool: 0 1 4\nAgent 1 util vec: 1 9 6\nCurrent player: 0\nTurn Type: Utterance\nMost recent proposal: [0, 1, 3]\n"
ObservationTensor(0): ◉◯◯◉◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯
ObservationTensor(1): ◉◯◯◉◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯
SerializeState() = "4\n0 1 4\n2 9 4\n1 9 6\n0, 9\n"
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [217, 218, 219, 220, 221, 222, 223, 224, 225]
StringLegalActions() = [", Utterance: [0, 0]", ", Utterance: [0, 1]", ", Utterance: [0, 2]", ", Utterance: [1, 0]", ", Utterance: [1, 1]", ", Utterance: [1, 2]", ", Utterance: [2, 0]", ", Utterance: [2, 1]", ", Utterance: [2, 2]"]

# Apply action ", Utterance: [2, 0]"
action: 223

# State 3
# Max steps: 4
# Item pool: 0 1 4
# Agent 0 util vec: 2 9 4
# Agent 1 util vec: 1 9 6
# Current player: 1
# Turn Type: Proposal
# Player 0 proposes: [0, 1, 3] utters: [2, 0]
IsTerminal() = False
History() = [0, 9, 223]
HistoryString() = "0, 9, 223"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
ObservationString(0) = "Max steps: 4\nItem pool: 0 1 4\nAgent 0 util vec: 2 9 4\nCurrent player: 1\nTurn Type: Proposal\nMost recent proposal: [0, 1, 3]\nMost recent utterance: [2, 0]\n"
ObservationString(1) = "Max steps: 4\nItem pool: 0 1 4\nAgent 1 util vec: 1 9 6\nCurrent player: 1\nTurn Type: Proposal\nMost recent proposal: [0, 1, 3]\nMost recent utterance: [2, 0]\n"
ObservationTensor(0): ◯◉◉◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◉◯◯
ObservationTensor(1): ◯◉◉◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◉◯◯
SerializeState() = "4\n0 1 4\n2 9 4\n1 9 6\n0, 9, 223\n"
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 216]
StringLegalActions() = ["Proposal: [0, 0, 0]", "Proposal: [0, 0, 1]", "Proposal: [0, 0, 2]", "Proposal: [0, 0, 3]", "Proposal: [0, 0, 4]", "Proposal: [0, 1, 0]", "Proposal: [0, 1, 1]", "Proposal: [0, 1, 2]", "Proposal: [0, 1, 3]", "Proposal: [0, 1, 4]", "Proposal: Agreement reached!"]

# Apply action "Proposal: [0, 0, 1]"
action: 1

# State 4
# Max steps: 4
# Item pool: 0 1 4
# Agent 0 util vec: 2 9 4
# Agent 1 util vec: 1 9 6
# Current player: 1
# Turn Type: Utterance
# Player 0 proposes: [0, 1, 3] utters: [2, 0]
# Player 1 proposes: [0, 0, 1]
IsTerminal() = False
History() = [0, 9, 223, 1]
HistoryString() = "0, 9, 223, 1"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
ObservationString(0) = "Max steps: 4\nItem pool: 0 1 4\nAgent 0 util vec: 2 9 4\nCurrent player: 1\nTurn Type: Utterance\nMost recent proposal: [0, 0, 1]\nMost recent utterance: [2, 0]\n"
ObservationString(1) = "Max steps: 4\nItem pool: 0 1 4\nAgent 1 util vec: 1 9 6\nCurrent player: 1\nTurn Type: Utterance\nMost recent proposal: [0, 0, 1]\nMost recent utterance: [2, 0]\n"
ObservationTensor(0): ◯◉◯◉◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◉◯◯
ObservationTensor(1): ◯◉◯◉◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◉◯◯
SerializeState() = "4\n0 1 4\n2 9 4\n1 9 6\n0, 9, 223, 1\n"
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [217, 218, 219, 220, 221, 222, 223, 224, 225]
StringLegalActions() = [", Utterance: [0, 0]", ", Utterance: [0, 1]", ", Utterance: [0, 2]", ", Utterance: [1, 0]", ", Utterance: [1, 1]", ", Utterance: [1, 2]", ", Utterance: [2, 0]", ", Utterance: [2, 1]", ", Utterance: [2, 2]"]

# Apply action ", Utterance: [2, 2]"
action: 225

# State 5
# Max steps: 4
# Item pool: 0 1 4
# Agent 0 util vec: 2 9 4
# Agent 1 util vec: 1 9 6
# Current player: 0
# Turn Type: Proposal
# Player 0 proposes: [0, 1, 3] utters: [2, 0]
# Player 1 proposes: [0, 0, 1] utters: [2, 2]
IsTerminal() = False
History() = [0, 9, 223, 1, 225]
HistoryString() = "0, 9, 223, 1, 225"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
ObservationString(0) = "Max steps: 4\nItem pool: 0 1 4\nAgent 0 util vec: 2 9 4\nCurrent player: 0\nTurn Type: Proposal\nMost recent proposal: [0, 0, 1]\nMost recent utterance: [2, 2]\n"
ObservationString(1) = "Max steps: 4\nItem pool: 0 1 4\nAgent 1 util vec: 1 9 6\nCurrent player: 0\nTurn Type: Proposal\nMost recent proposal: [0, 0, 1]\nMost recent utterance: [2, 2]\n"
ObservationTensor(0): ◉◯◉◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◉
ObservationTensor(1): ◉◯◉◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◉
SerializeState() = "4\n0 1 4\n2 9 4\n1 9 6\n0, 9, 223, 1, 225\n"
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 216]
StringLegalActions() = ["Proposal: [0, 0, 0]", "Proposal: [0, 0, 1]", "Proposal: [0, 0, 2]", "Proposal: [0, 0, 3]", "Proposal: [0, 0, 4]", "Proposal: [0, 1, 0]", "Proposal: [0, 1, 1]", "Proposal: [0, 1, 2]", "Proposal: [0, 1, 3]", "Proposal: [0, 1, 4]", "Proposal: Agreement reached!"]

# Apply action "Proposal: [0, 1, 1]"
action: 7

# State 6
# Apply action ", Utterance: [1, 0]"
action: 220

# State 7
# Max steps: 4
# Item pool: 0 1 4
# Agent 0 util vec: 2 9 4
# Agent 1 util vec: 1 9 6
# Current player: 1
# Turn Type: Proposal
# Player 0 proposes: [0, 1, 3] utters: [2, 0]
# Player 1 proposes: [0, 0, 1] utters: [2, 2]
# Player 0 proposes: [0, 1, 1] utters: [1, 0]
IsTerminal() = False
History() = [0, 9, 223, 1, 225, 7, 220]
HistoryString() = "0, 9, 223, 1, 225, 7, 220"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
ObservationString(0) = "Max steps: 4\nItem pool: 0 1 4\nAgent 0 util vec: 2 9 4\nCurrent player: 1\nTurn Type: Proposal\nMost recent proposal: [0, 1, 1]\nMost recent utterance: [1, 0]\n"
ObservationString(1) = "Max steps: 4\nItem pool: 0 1 4\nAgent 1 util vec: 1 9 6\nCurrent player: 1\nTurn Type: Proposal\nMost recent proposal: [0, 1, 1]\nMost recent utterance: [1, 0]\n"
ObservationTensor(0): ◯◉◉◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◉◯◉◯◯
ObservationTensor(1): ◯◉◉◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◉◯◉◯◯
SerializeState() = "4\n0 1 4\n2 9 4\n1 9 6\n0, 9, 223, 1, 225, 7, 220\n"
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 216]
StringLegalActions() = ["Proposal: [0, 0, 0]", "Proposal: [0, 0, 1]", "Proposal: [0, 0, 2]", "Proposal: [0, 0, 3]", "Proposal: [0, 0, 4]", "Proposal: [0, 1, 0]", "Proposal: [0, 1, 1]", "Proposal: [0, 1, 2]", "Proposal: [0, 1, 3]", "Proposal: [0, 1, 4]", "Proposal: Agreement reached!"]

# Apply action "Proposal: [0, 1, 3]"
action: 9

# State 8
# Apply action ", Utterance: [2, 0]"
action: 223

# State 9
# Max steps: 4
# Item pool: 0 1 4
# Agent 0 util vec: 2 9 4
# Agent 1 util vec: 1 9 6
# Current player: 0
# Turn Type: Proposal
# Player 0 proposes: [0, 1, 3] utters: [2, 0]
# Player 1 proposes: [0, 0, 1] utters: [2, 2]
# Player 0 proposes: [0, 1, 1] utters: [1, 0]
# Player 1 proposes: [0, 1, 3] utters: [2, 0]
IsTerminal() = True
History() = [0, 9, 223, 1, 225, 7, 220, 9, 223]
HistoryString() = "0, 9, 223, 1, 225, 7, 220, 9, 223"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
ObservationString(0) = "Max steps: 4\nItem pool: 0 1 4\nAgent 0 util vec: 2 9 4\nCurrent player: -4\nTurn Type: Proposal\nMost recent proposal: [0, 1, 3]\nMost recent utterance: [2, 0]\n"
ObservationString(1) = "Max steps: 4\nItem pool: 0 1 4\nAgent 1 util vec: 1 9 6\nCurrent player: -4\nTurn Type: Proposal\nMost recent proposal: [0, 1, 3]\nMost recent utterance: [2, 0]\n"
ObservationTensor(0): ◯◯◉◯◉◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◉◯◯
ObservationTensor(1): ◯◯◉◯◉◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◉◉◯◯
SerializeState() = "4\n0 1 4\n2 9 4\n1 9 6\n0, 9, 223, 1, 225, 7, 220, 9, 223\n"
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
