game: mean_field_lin_quad

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.MEAN_FIELD
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Mean-Field Linear Quadratic Game"
GameType.max_num_players = 1
GameType.min_num_players = 1
GameType.parameter_specification = ["cross_q", "dt", "horizon", "kappa", "mean_revert", "n_actions_per_side", "size", "spatial_bias", "terminal_cost", "volatility"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.REWARDS
GameType.short_name = "mean_field_lin_quad"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 7
PolicyTensorShape() = [7]
MaxChanceOutcomes() = 7
GetParameters() = {cross_q=0.01,dt=1.0,horizon=10,kappa=0.5,mean_revert=0.0,n_actions_per_side=3,size=10,spatial_bias=0,terminal_cost=1.0,volatility=1.0}
NumPlayers() = 1
MinUtility() = -inf
MaxUtility() = inf
UtilitySum() = 0.0
ObservationTensorShape() = x: [], t: [], observation: [2]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 2
MaxGameLength() = 10
ToString() = "mean_field_lin_quad(cross_q=0.01,dt=1.0,horizon=10,kappa=0.5,mean_revert=0.0,n_actions_per_side=3,size=10,spatial_bias=0,terminal_cost=1.0,volatility=1.0)"

# State 0
# initial
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
InformationStateString(0) = ""
ObservationString(0) = "initial"
ObservationTensor(0).x = [0.0]
ObservationTensor(0).t: ◯
ObservationTensor(0) = [nan, 0.0]
ChanceOutcomes() = [(0,0.1), (1,0.1), (2,0.1), (3,0.1), (4,0.1), (5,0.1), (6,0.1), (7,0.1), (8,0.1), (9,0.1)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
StringLegalActions() = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]

# Apply action "0"
action: 0

# State 1
# (0, 0)
IsTerminal() = False
History() = [0]
HistoryString() = "0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "0"
ObservationString(0) = "(0, 0)"
ObservationTensor(0).x: ◯
ObservationTensor(0).t: ◯
ObservationTensor(0): ◯◯
Rewards() = [-5.0625]
Returns() = [-5.0625]
LegalActions() = [0, 1, 2, 3, 4, 5, 6]
StringLegalActions() = ["0", "1", "2", "3", "4", "5", "6"]

# Apply action "3"
action: 3

# State 2
# (0, 0)_a_mu
IsTerminal() = False
History() = [0, 3]
HistoryString() = "0, 3"
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
InformationStateString(0) = "0, 3"
ObservationString(0) = "(0, 0)_a_mu"
ObservationTensor(0).x: ◯
ObservationTensor(0).t: ◯
ObservationTensor(0): ◯◯
ChanceOutcomes() = [(0,0.00620967), (1,0.0605975), (2,0.24173), (3,0.382925), (4,0.24173), (5,0.0605975), (6,0.00620967)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6]
StringLegalActions() = ["0", "1", "2", "3", "4", "5", "6"]

# Apply action "5"
action: 5

# State 3
# (2, 1)_a
IsTerminal() = False
History() = [0, 3, 5]
HistoryString() = "0, 3, 5"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "0, 3, 5"
ObservationString(0) = "(2, 1)_a"
ObservationTensor(0).x = [2]
ObservationTensor(0).t: ◉
ObservationTensor(0) = [2.0, 1.0]
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ['(0, 1)_a', '(1, 1)_a', '(2, 1)_a', '(3, 1)_a', '(4, 1)_a', '(5, 1)_a', '(6, 1)_a', '(7, 1)_a', '(8, 1)_a', '(9, 1)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 4
# (2, 1)
IsTerminal() = False
History() = [0, 3, 5]
HistoryString() = "0, 3, 5"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "0, 3, 5"
ObservationString(0) = "(2, 1)"
ObservationTensor(0).x = [2]
ObservationTensor(0).t: ◉
ObservationTensor(0) = [2.0, 1.0]
Rewards() = [-1.5625]
Returns() = [-1.5625]
LegalActions() = [0, 1, 2, 3, 4, 5, 6]
StringLegalActions() = ["0", "1", "2", "3", "4", "5", "6"]

# Apply action "0"
action: 0

# State 5
# Apply action "1"
action: 1

# State 6
# (7, 2)_a
IsTerminal() = False
History() = [0, 3, 5, 0, 1]
HistoryString() = "0, 3, 5, 0, 1"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "0, 3, 5, 0, 1"
ObservationString(0) = "(7, 2)_a"
ObservationTensor(0).x = [7]
ObservationTensor(0).t = [2.0]
ObservationTensor(0) = [7.0, 2.0]
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ['(0, 2)_a', '(1, 2)_a', '(2, 2)_a', '(3, 2)_a', '(4, 2)_a', '(5, 2)_a', '(6, 2)_a', '(7, 2)_a', '(8, 2)_a', '(9, 2)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 7
# (7, 2)
IsTerminal() = False
History() = [0, 3, 5, 0, 1]
HistoryString() = "0, 3, 5, 0, 1"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "0, 3, 5, 0, 1"
ObservationString(0) = "(7, 2)"
ObservationTensor(0).x = [7]
ObservationTensor(0).t = [2.0]
ObservationTensor(0) = [7.0, 2.0]
Rewards() = [-5.9875]
Returns() = [-5.9875]
LegalActions() = [0, 1, 2, 3, 4, 5, 6]
StringLegalActions() = ["0", "1", "2", "3", "4", "5", "6"]

# Apply action "6"
action: 6

# State 8
# Apply action "5"
action: 5

# State 9
# (2, 3)_a
IsTerminal() = False
History() = [0, 3, 5, 0, 1, 6, 5]
HistoryString() = "0, 3, 5, 0, 1, 6, 5"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "0, 3, 5, 0, 1, 6, 5"
ObservationString(0) = "(2, 3)_a"
ObservationTensor(0).x = [2]
ObservationTensor(0).t = [3.0]
ObservationTensor(0) = [2.0, 3.0]
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ['(0, 3)_a', '(1, 3)_a', '(2, 3)_a', '(3, 3)_a', '(4, 3)_a', '(5, 3)_a', '(6, 3)_a', '(7, 3)_a', '(8, 3)_a', '(9, 3)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 10
# Apply action "3"
action: 3

# State 11
# Apply action "3"
action: 3

# State 12
# Set mean field distribution to be uniform
action: update_distribution

# State 13
# Apply action "3"
action: 3

# State 14
# Apply action "3"
action: 3

# State 15
# Set mean field distribution to be uniform
action: update_distribution

# State 16
# Apply action "5"
action: 5

# State 17
# Apply action "3"
action: 3

# State 18
# Set mean field distribution to be uniform
action: update_distribution

# State 19
# Apply action "4"
action: 4

# State 20
# Apply action "1"
action: 1

# State 21
# Set mean field distribution to be uniform
action: update_distribution

# State 22
# Apply action "3"
action: 3

# State 23
# Apply action "0"
action: 0

# State 24
# Set mean field distribution to be uniform
action: update_distribution

# State 25
# Apply action "4"
action: 4

# State 26
# Apply action "1"
action: 1

# State 27
# Set mean field distribution to be uniform
action: update_distribution

# State 28
# Apply action "6"
action: 6

# State 29
# Apply action "2"
action: 2

# State 30
# (1, 10)_a
IsTerminal() = True
History() = [0, 3, 5, 0, 1, 6, 5, 3, 3, 3, 3, 5, 3, 4, 1, 3, 0, 4, 1, 6, 2]
HistoryString() = "0, 3, 5, 0, 1, 6, 5, 3, 3, 3, 3, 5, 3, 4, 1, 3, 0, 4, 1, 6, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "0, 3, 5, 0, 1, 6, 5, 3, 3, 3, 3, 5, 3, 4, 1, 3, 0, 4, 1, 6, 2"
ObservationString(0) = "(1, 10)_a"
ObservationTensor(0).x: ◉
ObservationTensor(0).t = [10.0]
ObservationTensor(0) = [1.0, 10.0]
Rewards() = [0]
Returns() = [0]
