game: mfg_crowd_modelling_2d

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.MEAN_FIELD
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Mean Field Crowd Modelling 2D"
GameType.max_num_players = 1
GameType.min_num_players = 1
GameType.parameter_specification = ["crowd_aversion_coef", "forbidden_states", "horizon", "initial_distribution", "initial_distribution_value", "noise_intensity", "only_distribution_reward", "positional_reward", "positional_reward_value", "size", "with_congestion"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.REWARDS
GameType.short_name = "mfg_crowd_modelling_2d"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 5
PolicyTensorShape() = [5]
MaxChanceOutcomes() = 100
GetParameters() = {crowd_aversion_coef=1.0,forbidden_states=[],horizon=10,initial_distribution=[],initial_distribution_value=[],noise_intensity=1.0,only_distribution_reward=False,positional_reward=[],positional_reward_value=[],size=10,with_congestion=False}
NumPlayers() = 1
MinUtility() = -inf
MaxUtility() = inf
UtilitySum() = 0.0
ObservationTensorShape() = [31]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 31
MaxGameLength() = 10
ToString() = "mfg_crowd_modelling_2d()"

# State 0
# initial
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
InformationStateString(0) = ""
ObservationString(0) = "initial"
ObservationTensor(0): ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯
ChanceOutcomes() = [(0, 0.01), (1, 0.01), (2, 0.01), (3, 0.01), (4, 0.01), (5, 0.01), (6, 0.01), (7, 0.01), (8, 0.01), (9, 0.01), (10, 0.01), (11, 0.01), (12, 0.01), (13, 0.01), (14, 0.01), (15, 0.01), (16, 0.01), (17, 0.01), (18, 0.01), (19, 0.01), (20, 0.01), (21, 0.01), (22, 0.01), (23, 0.01), (24, 0.01), (25, 0.01), (26, 0.01), (27, 0.01), (28, 0.01), (29, 0.01), (30, 0.01), (31, 0.01), (32, 0.01), (33, 0.01), (34, 0.01), (35, 0.01), (36, 0.01), (37, 0.01), (38, 0.01), (39, 0.01), (40, 0.01), (41, 0.01), (42, 0.01), (43, 0.01), (44, 0.01), (45, 0.01), (46, 0.01), (47, 0.01), (48, 0.01), (49, 0.01), (50, 0.01), (51, 0.01), (52, 0.01), (53, 0.01), (54, 0.01), (55, 0.01), (56, 0.01), (57, 0.01), (58, 0.01), (59, 0.01), (60, 0.01), (61, 0.01), (62, 0.01), (63, 0.01), (64, 0.01), (65, 0.01), (66, 0.01), (67, 0.01), (68, 0.01), (69, 0.01), (70, 0.01), (71, 0.01), (72, 0.01), (73, 0.01), (74, 0.01), (75, 0.01), (76, 0.01), (77, 0.01), (78, 0.01), (79, 0.01), (80, 0.01), (81, 0.01), (82, 0.01), (83, 0.01), (84, 0.01), (85, 0.01), (86, 0.01), (87, 0.01), (88, 0.01), (89, 0.01), (90, 0.01), (91, 0.01), (92, 0.01), (93, 0.01), (94, 0.01), (95, 0.01), (96, 0.01), (97, 0.01), (98, 0.01), (99, 0.01)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
StringLegalActions() = ["init_state=0", "init_state=1", "init_state=2", "init_state=3", "init_state=4", "init_state=5", "init_state=6", "init_state=7", "init_state=8", "init_state=9", "init_state=10", "init_state=11", "init_state=12", "init_state=13", "init_state=14", "init_state=15", "init_state=16", "init_state=17", "init_state=18", "init_state=19", "init_state=20", "init_state=21", "init_state=22", "init_state=23", "init_state=24", "init_state=25", "init_state=26", "init_state=27", "init_state=28", "init_state=29", "init_state=30", "init_state=31", "init_state=32", "init_state=33", "init_state=34", "init_state=35", "init_state=36", "init_state=37", "init_state=38", "init_state=39", "init_state=40", "init_state=41", "init_state=42", "init_state=43", "init_state=44", "init_state=45", "init_state=46", "init_state=47", "init_state=48", "init_state=49", "init_state=50", "init_state=51", "init_state=52", "init_state=53", "init_state=54", "init_state=55", "init_state=56", "init_state=57", "init_state=58", "init_state=59", "init_state=60", "init_state=61", "init_state=62", "init_state=63", "init_state=64", "init_state=65", "init_state=66", "init_state=67", "init_state=68", "init_state=69", "init_state=70", "init_state=71", "init_state=72", "init_state=73", "init_state=74", "init_state=75", "init_state=76", "init_state=77", "init_state=78", "init_state=79", "init_state=80", "init_state=81", "init_state=82", "init_state=83", "init_state=84", "init_state=85", "init_state=86", "init_state=87", "init_state=88", "init_state=89", "init_state=90", "init_state=91", "init_state=92", "init_state=93", "init_state=94", "init_state=95", "init_state=96", "init_state=97", "init_state=98", "init_state=99"]

# Apply action "init_state=44"
action: 44

# State 1
# (4, 4, 0)
IsTerminal() = False
History() = [44]
HistoryString() = "44"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44"
ObservationString(0) = "(4, 4, 0)"
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯
Rewards() = [6.2051701859880914]
Returns() = [6.2051701859880914]
LegalActions() = [0, 1, 2, 3, 4]
StringLegalActions() = ["(0,-1)", "(-1,0)", "(0,0)", "(1,0)", "(0,1)"]

# Apply action "(0,0)"
action: 2

# State 2
# (4, 4, 0)_a_mu
IsTerminal() = False
History() = [44, 2]
HistoryString() = "44, 2"
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
InformationStateString(0) = "44, 2"
ObservationString(0) = "(4, 4, 0)_a_mu"
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯
ChanceOutcomes() = [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]
LegalActions() = [0, 1, 2, 3, 4]
StringLegalActions() = ["(0,-1)", "(-1,0)", "(0,0)", "(1,0)", "(0,1)"]

# Apply action "(0,0)"
action: 2

# State 3
# (4, 4, 1)_a
IsTerminal() = False
History() = [44, 2, 2]
HistoryString() = "44, 2, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "44, 2, 2"
ObservationString(0) = "(4, 4, 1)_a"
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯
Rewards() = [0.0]
Returns() = [6.2051701859880914]
DistributionSupport() = ['(0, 0, 1)_a', '(0, 1, 1)_a', '(0, 2, 1)_a', '(0, 3, 1)_a', '(0, 4, 1)_a', '(0, 5, 1)_a', '(0, 6, 1)_a', '(0, 7, 1)_a', '(0, 8, 1)_a', '(0, 9, 1)_a', '(1, 0, 1)_a', '(1, 1, 1)_a', '(1, 2, 1)_a', '(1, 3, 1)_a', '(1, 4, 1)_a', '(1, 5, 1)_a', '(1, 6, 1)_a', '(1, 7, 1)_a', '(1, 8, 1)_a', '(1, 9, 1)_a', '(2, 0, 1)_a', '(2, 1, 1)_a', '(2, 2, 1)_a', '(2, 3, 1)_a', '(2, 4, 1)_a', '(2, 5, 1)_a', '(2, 6, 1)_a', '(2, 7, 1)_a', '(2, 8, 1)_a', '(2, 9, 1)_a', '(3, 0, 1)_a', '(3, 1, 1)_a', '(3, 2, 1)_a', '(3, 3, 1)_a', '(3, 4, 1)_a', '(3, 5, 1)_a', '(3, 6, 1)_a', '(3, 7, 1)_a', '(3, 8, 1)_a', '(3, 9, 1)_a', '(4, 0, 1)_a', '(4, 1, 1)_a', '(4, 2, 1)_a', '(4, 3, 1)_a', '(4, 4, 1)_a', '(4, 5, 1)_a', '(4, 6, 1)_a', '(4, 7, 1)_a', '(4, 8, 1)_a', '(4, 9, 1)_a', '(5, 0, 1)_a', '(5, 1, 1)_a', '(5, 2, 1)_a', '(5, 3, 1)_a', '(5, 4, 1)_a', '(5, 5, 1)_a', '(5, 6, 1)_a', '(5, 7, 1)_a', '(5, 8, 1)_a', '(5, 9, 1)_a', '(6, 0, 1)_a', '(6, 1, 1)_a', '(6, 2, 1)_a', '(6, 3, 1)_a', '(6, 4, 1)_a', '(6, 5, 1)_a', '(6, 6, 1)_a', '(6, 7, 1)_a', '(6, 8, 1)_a', '(6, 9, 1)_a', '(7, 0, 1)_a', '(7, 1, 1)_a', '(7, 2, 1)_a', '(7, 3, 1)_a', '(7, 4, 1)_a', '(7, 5, 1)_a', '(7, 6, 1)_a', '(7, 7, 1)_a', '(7, 8, 1)_a', '(7, 9, 1)_a', '(8, 0, 1)_a', '(8, 1, 1)_a', '(8, 2, 1)_a', '(8, 3, 1)_a', '(8, 4, 1)_a', '(8, 5, 1)_a', '(8, 6, 1)_a', '(8, 7, 1)_a', '(8, 8, 1)_a', '(8, 9, 1)_a', '(9, 0, 1)_a', '(9, 1, 1)_a', '(9, 2, 1)_a', '(9, 3, 1)_a', '(9, 4, 1)_a', '(9, 5, 1)_a', '(9, 6, 1)_a', '(9, 7, 1)_a', '(9, 8, 1)_a', '(9, 9, 1)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 4
# (4, 4, 1)
IsTerminal() = False
History() = [44, 2, 2]
HistoryString() = "44, 2, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44, 2, 2"
ObservationString(0) = "(4, 4, 1)"
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯
Rewards() = [6.2051701859880914]
Returns() = [12.410340371976183]
LegalActions() = [0, 1, 2, 3, 4]
StringLegalActions() = ["(0,-1)", "(-1,0)", "(0,0)", "(1,0)", "(0,1)"]

# Apply action "(1,0)"
action: 3

# State 5
# Apply action "(0,1)"
action: 4

# State 6
# (5, 5, 2)_a
IsTerminal() = False
History() = [44, 2, 2, 3, 4]
HistoryString() = "44, 2, 2, 3, 4"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "44, 2, 2, 3, 4"
ObservationString(0) = "(5, 5, 2)_a"
ObservationTensor(0): ◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯
Rewards() = [0.0]
Returns() = [12.410340371976183]
DistributionSupport() = ['(0, 0, 2)_a', '(0, 1, 2)_a', '(0, 2, 2)_a', '(0, 3, 2)_a', '(0, 4, 2)_a', '(0, 5, 2)_a', '(0, 6, 2)_a', '(0, 7, 2)_a', '(0, 8, 2)_a', '(0, 9, 2)_a', '(1, 0, 2)_a', '(1, 1, 2)_a', '(1, 2, 2)_a', '(1, 3, 2)_a', '(1, 4, 2)_a', '(1, 5, 2)_a', '(1, 6, 2)_a', '(1, 7, 2)_a', '(1, 8, 2)_a', '(1, 9, 2)_a', '(2, 0, 2)_a', '(2, 1, 2)_a', '(2, 2, 2)_a', '(2, 3, 2)_a', '(2, 4, 2)_a', '(2, 5, 2)_a', '(2, 6, 2)_a', '(2, 7, 2)_a', '(2, 8, 2)_a', '(2, 9, 2)_a', '(3, 0, 2)_a', '(3, 1, 2)_a', '(3, 2, 2)_a', '(3, 3, 2)_a', '(3, 4, 2)_a', '(3, 5, 2)_a', '(3, 6, 2)_a', '(3, 7, 2)_a', '(3, 8, 2)_a', '(3, 9, 2)_a', '(4, 0, 2)_a', '(4, 1, 2)_a', '(4, 2, 2)_a', '(4, 3, 2)_a', '(4, 4, 2)_a', '(4, 5, 2)_a', '(4, 6, 2)_a', '(4, 7, 2)_a', '(4, 8, 2)_a', '(4, 9, 2)_a', '(5, 0, 2)_a', '(5, 1, 2)_a', '(5, 2, 2)_a', '(5, 3, 2)_a', '(5, 4, 2)_a', '(5, 5, 2)_a', '(5, 6, 2)_a', '(5, 7, 2)_a', '(5, 8, 2)_a', '(5, 9, 2)_a', '(6, 0, 2)_a', '(6, 1, 2)_a', '(6, 2, 2)_a', '(6, 3, 2)_a', '(6, 4, 2)_a', '(6, 5, 2)_a', '(6, 6, 2)_a', '(6, 7, 2)_a', '(6, 8, 2)_a', '(6, 9, 2)_a', '(7, 0, 2)_a', '(7, 1, 2)_a', '(7, 2, 2)_a', '(7, 3, 2)_a', '(7, 4, 2)_a', '(7, 5, 2)_a', '(7, 6, 2)_a', '(7, 7, 2)_a', '(7, 8, 2)_a', '(7, 9, 2)_a', '(8, 0, 2)_a', '(8, 1, 2)_a', '(8, 2, 2)_a', '(8, 3, 2)_a', '(8, 4, 2)_a', '(8, 5, 2)_a', '(8, 6, 2)_a', '(8, 7, 2)_a', '(8, 8, 2)_a', '(8, 9, 2)_a', '(9, 0, 2)_a', '(9, 1, 2)_a', '(9, 2, 2)_a', '(9, 3, 2)_a', '(9, 4, 2)_a', '(9, 5, 2)_a', '(9, 6, 2)_a', '(9, 7, 2)_a', '(9, 8, 2)_a', '(9, 9, 2)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 7
# (5, 5, 2)
IsTerminal() = False
History() = [44, 2, 2, 3, 4]
HistoryString() = "44, 2, 2, 3, 4"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44, 2, 2, 3, 4"
ObservationString(0) = "(5, 5, 2)"
ObservationTensor(0): ◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯
Rewards() = [6.50517018598809]
Returns() = [18.915510557964275]
LegalActions() = [0, 1, 2, 3, 4]
StringLegalActions() = ["(0,-1)", "(-1,0)", "(0,0)", "(1,0)", "(0,1)"]

# Apply action "(-1,0)"
action: 1

# State 8
# Apply action "(1,0)"
action: 3

# State 9
# (5, 5, 3)_a
IsTerminal() = False
History() = [44, 2, 2, 3, 4, 1, 3]
HistoryString() = "44, 2, 2, 3, 4, 1, 3"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "44, 2, 2, 3, 4, 1, 3"
ObservationString(0) = "(5, 5, 3)_a"
ObservationTensor(0): ◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯
Rewards() = [0.0]
Returns() = [18.915510557964275]
DistributionSupport() = ['(0, 0, 3)_a', '(0, 1, 3)_a', '(0, 2, 3)_a', '(0, 3, 3)_a', '(0, 4, 3)_a', '(0, 5, 3)_a', '(0, 6, 3)_a', '(0, 7, 3)_a', '(0, 8, 3)_a', '(0, 9, 3)_a', '(1, 0, 3)_a', '(1, 1, 3)_a', '(1, 2, 3)_a', '(1, 3, 3)_a', '(1, 4, 3)_a', '(1, 5, 3)_a', '(1, 6, 3)_a', '(1, 7, 3)_a', '(1, 8, 3)_a', '(1, 9, 3)_a', '(2, 0, 3)_a', '(2, 1, 3)_a', '(2, 2, 3)_a', '(2, 3, 3)_a', '(2, 4, 3)_a', '(2, 5, 3)_a', '(2, 6, 3)_a', '(2, 7, 3)_a', '(2, 8, 3)_a', '(2, 9, 3)_a', '(3, 0, 3)_a', '(3, 1, 3)_a', '(3, 2, 3)_a', '(3, 3, 3)_a', '(3, 4, 3)_a', '(3, 5, 3)_a', '(3, 6, 3)_a', '(3, 7, 3)_a', '(3, 8, 3)_a', '(3, 9, 3)_a', '(4, 0, 3)_a', '(4, 1, 3)_a', '(4, 2, 3)_a', '(4, 3, 3)_a', '(4, 4, 3)_a', '(4, 5, 3)_a', '(4, 6, 3)_a', '(4, 7, 3)_a', '(4, 8, 3)_a', '(4, 9, 3)_a', '(5, 0, 3)_a', '(5, 1, 3)_a', '(5, 2, 3)_a', '(5, 3, 3)_a', '(5, 4, 3)_a', '(5, 5, 3)_a', '(5, 6, 3)_a', '(5, 7, 3)_a', '(5, 8, 3)_a', '(5, 9, 3)_a', '(6, 0, 3)_a', '(6, 1, 3)_a', '(6, 2, 3)_a', '(6, 3, 3)_a', '(6, 4, 3)_a', '(6, 5, 3)_a', '(6, 6, 3)_a', '(6, 7, 3)_a', '(6, 8, 3)_a', '(6, 9, 3)_a', '(7, 0, 3)_a', '(7, 1, 3)_a', '(7, 2, 3)_a', '(7, 3, 3)_a', '(7, 4, 3)_a', '(7, 5, 3)_a', '(7, 6, 3)_a', '(7, 7, 3)_a', '(7, 8, 3)_a', '(7, 9, 3)_a', '(8, 0, 3)_a', '(8, 1, 3)_a', '(8, 2, 3)_a', '(8, 3, 3)_a', '(8, 4, 3)_a', '(8, 5, 3)_a', '(8, 6, 3)_a', '(8, 7, 3)_a', '(8, 8, 3)_a', '(8, 9, 3)_a', '(9, 0, 3)_a', '(9, 1, 3)_a', '(9, 2, 3)_a', '(9, 3, 3)_a', '(9, 4, 3)_a', '(9, 5, 3)_a', '(9, 6, 3)_a', '(9, 7, 3)_a', '(9, 8, 3)_a', '(9, 9, 3)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 10
# Apply action "(0,0)"
action: 2

# State 11
# Apply action "(0,0)"
action: 2

# State 12
# Set mean field distribution to be uniform
action: update_distribution

# State 13
# Apply action "(0,1)"
action: 4

# State 14
# Apply action "(1,0)"
action: 3

# State 15
# Set mean field distribution to be uniform
action: update_distribution

# State 16
# Apply action "(0,0)"
action: 2

# State 17
# Apply action "(0,-1)"
action: 0

# State 18
# Set mean field distribution to be uniform
action: update_distribution

# State 19
# Apply action "(0,1)"
action: 4

# State 20
# Apply action "(0,0)"
action: 2

# State 21
# Set mean field distribution to be uniform
action: update_distribution

# State 22
# Apply action "(0,0)"
action: 2

# State 23
# Apply action "(0,0)"
action: 2

# State 24
# Set mean field distribution to be uniform
action: update_distribution

# State 25
# Apply action "(1,0)"
action: 3

# State 26
# Apply action "(0,1)"
action: 4

# State 27
# Set mean field distribution to be uniform
action: update_distribution

# State 28
# Apply action "(0,1)"
action: 4

# State 29
# Apply action "(1,0)"
action: 3

# State 30
# (8, 8, 10)_a
IsTerminal() = True
History() = [44, 2, 2, 3, 4, 1, 3, 2, 2, 4, 3, 2, 0, 4, 2, 2, 2, 3, 4, 4, 3]
HistoryString() = "44, 2, 2, 3, 4, 1, 3, 2, 2, 4, 3, 2, 0, 4, 2, 2, 2, 3, 4, 4, 3"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "44, 2, 2, 3, 4, 1, 3, 2, 2, 4, 3, 2, 0, 4, 2, 2, 2, 3, 4, 4, 3"
ObservationString(0) = "(8, 8, 10)_a"
ObservationTensor(0): ◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉
Rewards() = [0.0]
Returns() = [62.551701859880914]
