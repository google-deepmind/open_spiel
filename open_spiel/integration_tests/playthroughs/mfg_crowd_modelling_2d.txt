game: mfg_crowd_modelling_2d

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.MEAN_FIELD
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Mean Field Crowd Modelling 2D"
GameType.max_num_players = 1
GameType.min_num_players = 1
GameType.parameter_specification = ["crowd_aversion_coef", "forbidden_states", "horizon", "initial_distribution", "initial_distribution_value", "noise_intensity", "only_distribution_reward", "positional_reward", "positional_reward_value", "size", "with_congestion"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.REWARDS
GameType.short_name = "mfg_crowd_modelling_2d"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 5
PolicyTensorShape() = [5]
MaxChanceOutcomes() = 100
GetParameters() = {crowd_aversion_coef=1.0,forbidden_states=[],horizon=10,initial_distribution=[],initial_distribution_value=[],noise_intensity=1.0,only_distribution_reward=False,positional_reward=[],positional_reward_value=[],size=10,with_congestion=False}
NumPlayers() = 1
MinUtility() = -inf
MaxUtility() = inf
UtilitySum() = None
ObservationTensorShape() = [31]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 31
MaxGameLength() = 10
ToString() = "mfg_crowd_modelling_2d()"

# State 0
# initial
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
InformationStateString(0) = ""
ObservationString(0) = "initial"
ObservationTensor(0): ◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯
ChanceOutcomes() = [(0,0.01), (1,0.01), (2,0.01), (3,0.01), (4,0.01), (5,0.01), (6,0.01), (7,0.01), (8,0.01), (9,0.01), (10,0.01), (11,0.01), (12,0.01), (13,0.01), (14,0.01), (15,0.01), (16,0.01), (17,0.01), (18,0.01), (19,0.01), (20,0.01), (21,0.01), (22,0.01), (23,0.01), (24,0.01), (25,0.01), (26,0.01), (27,0.01), (28,0.01), (29,0.01), (30,0.01), (31,0.01), (32,0.01), (33,0.01), (34,0.01), (35,0.01), (36,0.01), (37,0.01), (38,0.01), (39,0.01), (40,0.01), (41,0.01), (42,0.01), (43,0.01), (44,0.01), (45,0.01), (46,0.01), (47,0.01), (48,0.01), (49,0.01), (50,0.01), (51,0.01), (52,0.01), (53,0.01), (54,0.01), (55,0.01), (56,0.01), (57,0.01), (58,0.01), (59,0.01), (60,0.01), (61,0.01), (62,0.01), (63,0.01), (64,0.01), (65,0.01), (66,0.01), (67,0.01), (68,0.01), (69,0.01), (70,0.01), (71,0.01), (72,0.01), (73,0.01), (74,0.01), (75,0.01), (76,0.01), (77,0.01), (78,0.01), (79,0.01), (80,0.01), (81,0.01), (82,0.01), (83,0.01), (84,0.01), (85,0.01), (86,0.01), (87,0.01), (88,0.01), (89,0.01), (90,0.01), (91,0.01), (92,0.01), (93,0.01), (94,0.01), (95,0.01), (96,0.01), (97,0.01), (98,0.01), (99,0.01)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
StringLegalActions() = ["init_state=0", "init_state=1", "init_state=2", "init_state=3", "init_state=4", "init_state=5", "init_state=6", "init_state=7", "init_state=8", "init_state=9", "init_state=10", "init_state=11", "init_state=12", "init_state=13", "init_state=14", "init_state=15", "init_state=16", "init_state=17", "init_state=18", "init_state=19", "init_state=20", "init_state=21", "init_state=22", "init_state=23", "init_state=24", "init_state=25", "init_state=26", "init_state=27", "init_state=28", "init_state=29", "init_state=30", "init_state=31", "init_state=32", "init_state=33", "init_state=34", "init_state=35", "init_state=36", "init_state=37", "init_state=38", "init_state=39", "init_state=40", "init_state=41", "init_state=42", "init_state=43", "init_state=44", "init_state=45", "init_state=46", "init_state=47", "init_state=48", "init_state=49", "init_state=50", "init_state=51", "init_state=52", "init_state=53", "init_state=54", "init_state=55", "init_state=56", "init_state=57", "init_state=58", "init_state=59", "init_state=60", "init_state=61", "init_state=62", "init_state=63", "init_state=64", "init_state=65", "init_state=66", "init_state=67", "init_state=68", "init_state=69", "init_state=70", "init_state=71", "init_state=72", "init_state=73", "init_state=74", "init_state=75", "init_state=76", "init_state=77", "init_state=78", "init_state=79", "init_state=80", "init_state=81", "init_state=82", "init_state=83", "init_state=84", "init_state=85", "init_state=86", "init_state=87", "init_state=88", "init_state=89", "init_state=90", "init_state=91", "init_state=92", "init_state=93", "init_state=94", "init_state=95", "init_state=96", "init_state=97", "init_state=98", "init_state=99"]

# Apply action "init_state=44"
action: 44

# State 1
# (4, 4, 0)
IsTerminal() = False
History() = [44]
HistoryString() = "44"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44"
ObservationString(0) = "(4, 4, 0)"
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯
Rewards() = [6.20517]
Returns() = [6.20517]
LegalActions() = [0, 1, 2, 3, 4]
StringLegalActions() = ["(0,-1)", "(-1,0)", "(0,0)", "(1,0)", "(0,1)"]

# Apply action "(0,0)"
action: 2

# State 2
# (4, 4, 0)_a_mu
IsTerminal() = False
History() = [44, 2]
HistoryString() = "44, 2"
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
InformationStateString(0) = "44, 2"
ObservationString(0) = "(4, 4, 0)_a_mu"
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯
ChanceOutcomes() = [(0,0.2), (1,0.2), (2,0.2), (3,0.2), (4,0.2)]
LegalActions() = [0, 1, 2, 3, 4]
StringLegalActions() = ["(0,-1)", "(-1,0)", "(0,0)", "(1,0)", "(0,1)"]

# Apply action "(0,0)"
action: 2

# State 3
# (4, 4, 1)_a
IsTerminal() = False
History() = [44, 2, 2]
HistoryString() = "44, 2, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "44, 2, 2"
ObservationString(0) = "(4, 4, 1)_a"
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯
Rewards() = [0]
Returns() = [6.20517]
DistributionSupport() = ['(0, 0, 1)_a', '(0, 1, 1)_a', '(0, 2, 1)_a', '(0, 3, 1)_a', '(0, 4, 1)_a', '(0, 5, 1)_a', '(0, 6, 1)_a', '(0, 7, 1)_a', '(0, 8, 1)_a', '(0, 9, 1)_a', '(1, 0, 1)_a', '(1, 1, 1)_a', '(1, 2, 1)_a', '(1, 3, 1)_a', '(1, 4, 1)_a', '(1, 5, 1)_a', '(1, 6, 1)_a', '(1, 7, 1)_a', '(1, 8, 1)_a', '(1, 9, 1)_a', '(2, 0, 1)_a', '(2, 1, 1)_a', '(2, 2, 1)_a', '(2, 3, 1)_a', '(2, 4, 1)_a', '(2, 5, 1)_a', '(2, 6, 1)_a', '(2, 7, 1)_a', '(2, 8, 1)_a', '(2, 9, 1)_a', '(3, 0, 1)_a', '(3, 1, 1)_a', '(3, 2, 1)_a', '(3, 3, 1)_a', '(3, 4, 1)_a', '(3, 5, 1)_a', '(3, 6, 1)_a', '(3, 7, 1)_a', '(3, 8, 1)_a', '(3, 9, 1)_a', '(4, 0, 1)_a', '(4, 1, 1)_a', '(4, 2, 1)_a', '(4, 3, 1)_a', '(4, 4, 1)_a', '(4, 5, 1)_a', '(4, 6, 1)_a', '(4, 7, 1)_a', '(4, 8, 1)_a', '(4, 9, 1)_a', '(5, 0, 1)_a', '(5, 1, 1)_a', '(5, 2, 1)_a', '(5, 3, 1)_a', '(5, 4, 1)_a', '(5, 5, 1)_a', '(5, 6, 1)_a', '(5, 7, 1)_a', '(5, 8, 1)_a', '(5, 9, 1)_a', '(6, 0, 1)_a', '(6, 1, 1)_a', '(6, 2, 1)_a', '(6, 3, 1)_a', '(6, 4, 1)_a', '(6, 5, 1)_a', '(6, 6, 1)_a', '(6, 7, 1)_a', '(6, 8, 1)_a', '(6, 9, 1)_a', '(7, 0, 1)_a', '(7, 1, 1)_a', '(7, 2, 1)_a', '(7, 3, 1)_a', '(7, 4, 1)_a', '(7, 5, 1)_a', '(7, 6, 1)_a', '(7, 7, 1)_a', '(7, 8, 1)_a', '(7, 9, 1)_a', '(8, 0, 1)_a', '(8, 1, 1)_a', '(8, 2, 1)_a', '(8, 3, 1)_a', '(8, 4, 1)_a', '(8, 5, 1)_a', '(8, 6, 1)_a', '(8, 7, 1)_a', '(8, 8, 1)_a', '(8, 9, 1)_a', '(9, 0, 1)_a', '(9, 1, 1)_a', '(9, 2, 1)_a', '(9, 3, 1)_a', '(9, 4, 1)_a', '(9, 5, 1)_a', '(9, 6, 1)_a', '(9, 7, 1)_a', '(9, 8, 1)_a', '(9, 9, 1)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 4
# (4, 4, 1)
IsTerminal() = False
History() = [44, 2, 2]
HistoryString() = "44, 2, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44, 2, 2"
ObservationString(0) = "(4, 4, 1)"
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯
Rewards() = [6.20517]
Returns() = [12.4103]
LegalActions() = [0, 1, 2, 3, 4]
StringLegalActions() = ["(0,-1)", "(-1,0)", "(0,0)", "(1,0)", "(0,1)"]

# Apply action "(1,0)"
action: 3

# State 5
# Apply action "(0,1)"
action: 4

# State 6
# (5, 5, 2)_a
IsTerminal() = False
History() = [44, 2, 2, 3, 4]
HistoryString() = "44, 2, 2, 3, 4"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "44, 2, 2, 3, 4"
ObservationString(0) = "(5, 5, 2)_a"
ObservationTensor(0): ◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯
Rewards() = [0]
Returns() = [12.4103]
DistributionSupport() = ['(0, 0, 2)_a', '(0, 1, 2)_a', '(0, 2, 2)_a', '(0, 3, 2)_a', '(0, 4, 2)_a', '(0, 5, 2)_a', '(0, 6, 2)_a', '(0, 7, 2)_a', '(0, 8, 2)_a', '(0, 9, 2)_a', '(1, 0, 2)_a', '(1, 1, 2)_a', '(1, 2, 2)_a', '(1, 3, 2)_a', '(1, 4, 2)_a', '(1, 5, 2)_a', '(1, 6, 2)_a', '(1, 7, 2)_a', '(1, 8, 2)_a', '(1, 9, 2)_a', '(2, 0, 2)_a', '(2, 1, 2)_a', '(2, 2, 2)_a', '(2, 3, 2)_a', '(2, 4, 2)_a', '(2, 5, 2)_a', '(2, 6, 2)_a', '(2, 7, 2)_a', '(2, 8, 2)_a', '(2, 9, 2)_a', '(3, 0, 2)_a', '(3, 1, 2)_a', '(3, 2, 2)_a', '(3, 3, 2)_a', '(3, 4, 2)_a', '(3, 5, 2)_a', '(3, 6, 2)_a', '(3, 7, 2)_a', '(3, 8, 2)_a', '(3, 9, 2)_a', '(4, 0, 2)_a', '(4, 1, 2)_a', '(4, 2, 2)_a', '(4, 3, 2)_a', '(4, 4, 2)_a', '(4, 5, 2)_a', '(4, 6, 2)_a', '(4, 7, 2)_a', '(4, 8, 2)_a', '(4, 9, 2)_a', '(5, 0, 2)_a', '(5, 1, 2)_a', '(5, 2, 2)_a', '(5, 3, 2)_a', '(5, 4, 2)_a', '(5, 5, 2)_a', '(5, 6, 2)_a', '(5, 7, 2)_a', '(5, 8, 2)_a', '(5, 9, 2)_a', '(6, 0, 2)_a', '(6, 1, 2)_a', '(6, 2, 2)_a', '(6, 3, 2)_a', '(6, 4, 2)_a', '(6, 5, 2)_a', '(6, 6, 2)_a', '(6, 7, 2)_a', '(6, 8, 2)_a', '(6, 9, 2)_a', '(7, 0, 2)_a', '(7, 1, 2)_a', '(7, 2, 2)_a', '(7, 3, 2)_a', '(7, 4, 2)_a', '(7, 5, 2)_a', '(7, 6, 2)_a', '(7, 7, 2)_a', '(7, 8, 2)_a', '(7, 9, 2)_a', '(8, 0, 2)_a', '(8, 1, 2)_a', '(8, 2, 2)_a', '(8, 3, 2)_a', '(8, 4, 2)_a', '(8, 5, 2)_a', '(8, 6, 2)_a', '(8, 7, 2)_a', '(8, 8, 2)_a', '(8, 9, 2)_a', '(9, 0, 2)_a', '(9, 1, 2)_a', '(9, 2, 2)_a', '(9, 3, 2)_a', '(9, 4, 2)_a', '(9, 5, 2)_a', '(9, 6, 2)_a', '(9, 7, 2)_a', '(9, 8, 2)_a', '(9, 9, 2)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 7
# (5, 5, 2)
IsTerminal() = False
History() = [44, 2, 2, 3, 4]
HistoryString() = "44, 2, 2, 3, 4"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44, 2, 2, 3, 4"
ObservationString(0) = "(5, 5, 2)"
ObservationTensor(0): ◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯
Rewards() = [6.50517]
Returns() = [18.9155]
LegalActions() = [0, 1, 2, 3, 4]
StringLegalActions() = ["(0,-1)", "(-1,0)", "(0,0)", "(1,0)", "(0,1)"]

# Apply action "(-1,0)"
action: 1

# State 8
# Apply action "(1,0)"
action: 3

# State 9
# (5, 5, 3)_a
IsTerminal() = False
History() = [44, 2, 2, 3, 4, 1, 3]
HistoryString() = "44, 2, 2, 3, 4, 1, 3"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -5
InformationStateString(0) = "44, 2, 2, 3, 4, 1, 3"
ObservationString(0) = "(5, 5, 3)_a"
ObservationTensor(0): ◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯
Rewards() = [0]
Returns() = [18.9155]
DistributionSupport() = ['(0, 0, 3)_a', '(0, 1, 3)_a', '(0, 2, 3)_a', '(0, 3, 3)_a', '(0, 4, 3)_a', '(0, 5, 3)_a', '(0, 6, 3)_a', '(0, 7, 3)_a', '(0, 8, 3)_a', '(0, 9, 3)_a', '(1, 0, 3)_a', '(1, 1, 3)_a', '(1, 2, 3)_a', '(1, 3, 3)_a', '(1, 4, 3)_a', '(1, 5, 3)_a', '(1, 6, 3)_a', '(1, 7, 3)_a', '(1, 8, 3)_a', '(1, 9, 3)_a', '(2, 0, 3)_a', '(2, 1, 3)_a', '(2, 2, 3)_a', '(2, 3, 3)_a', '(2, 4, 3)_a', '(2, 5, 3)_a', '(2, 6, 3)_a', '(2, 7, 3)_a', '(2, 8, 3)_a', '(2, 9, 3)_a', '(3, 0, 3)_a', '(3, 1, 3)_a', '(3, 2, 3)_a', '(3, 3, 3)_a', '(3, 4, 3)_a', '(3, 5, 3)_a', '(3, 6, 3)_a', '(3, 7, 3)_a', '(3, 8, 3)_a', '(3, 9, 3)_a', '(4, 0, 3)_a', '(4, 1, 3)_a', '(4, 2, 3)_a', '(4, 3, 3)_a', '(4, 4, 3)_a', '(4, 5, 3)_a', '(4, 6, 3)_a', '(4, 7, 3)_a', '(4, 8, 3)_a', '(4, 9, 3)_a', '(5, 0, 3)_a', '(5, 1, 3)_a', '(5, 2, 3)_a', '(5, 3, 3)_a', '(5, 4, 3)_a', '(5, 5, 3)_a', '(5, 6, 3)_a', '(5, 7, 3)_a', '(5, 8, 3)_a', '(5, 9, 3)_a', '(6, 0, 3)_a', '(6, 1, 3)_a', '(6, 2, 3)_a', '(6, 3, 3)_a', '(6, 4, 3)_a', '(6, 5, 3)_a', '(6, 6, 3)_a', '(6, 7, 3)_a', '(6, 8, 3)_a', '(6, 9, 3)_a', '(7, 0, 3)_a', '(7, 1, 3)_a', '(7, 2, 3)_a', '(7, 3, 3)_a', '(7, 4, 3)_a', '(7, 5, 3)_a', '(7, 6, 3)_a', '(7, 7, 3)_a', '(7, 8, 3)_a', '(7, 9, 3)_a', '(8, 0, 3)_a', '(8, 1, 3)_a', '(8, 2, 3)_a', '(8, 3, 3)_a', '(8, 4, 3)_a', '(8, 5, 3)_a', '(8, 6, 3)_a', '(8, 7, 3)_a', '(8, 8, 3)_a', '(8, 9, 3)_a', '(9, 0, 3)_a', '(9, 1, 3)_a', '(9, 2, 3)_a', '(9, 3, 3)_a', '(9, 4, 3)_a', '(9, 5, 3)_a', '(9, 6, 3)_a', '(9, 7, 3)_a', '(9, 8, 3)_a', '(9, 9, 3)_a']

# Set mean field distribution to be uniform
action: update_distribution

# State 10
# Apply action "(0,0)"
action: 2

# State 11
# Apply action "(0,0)"
action: 2

# State 12
# Set mean field distribution to be uniform
action: update_distribution

# State 13
# Apply action "(0,1)"
action: 4

# State 14
# Apply action "(1,0)"
action: 3

# State 15
# Set mean field distribution to be uniform
action: update_distribution

# State 16
# Apply action "(0,0)"
action: 2

# State 17
# Apply action "(0,-1)"
action: 0

# State 18
# Set mean field distribution to be uniform
action: update_distribution

# State 19
# Apply action "(0,1)"
action: 4

# State 20
# Apply action "(0,0)"
action: 2

# State 21
# Set mean field distribution to be uniform
action: update_distribution

# State 22
# Apply action "(0,0)"
action: 2

# State 23
# Apply action "(0,0)"
action: 2

# State 24
# Set mean field distribution to be uniform
action: update_distribution

# State 25
# Apply action "(1,0)"
action: 3

# State 26
# Apply action "(0,1)"
action: 4

# State 27
# Set mean field distribution to be uniform
action: update_distribution

# State 28
# Apply action "(0,1)"
action: 4

# State 29
# Apply action "(1,0)"
action: 3

# State 30
# (8, 8, 10)_a
IsTerminal() = True
History() = [44, 2, 2, 3, 4, 1, 3, 2, 2, 4, 3, 2, 0, 4, 2, 2, 2, 3, 4, 4, 3]
HistoryString() = "44, 2, 2, 3, 4, 1, 3, 2, 2, 4, 3, 2, 0, 4, 2, 2, 2, 3, 4, 4, 3"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "44, 2, 2, 3, 4, 1, 3, 2, 2, 4, 3, 2, 0, 4, 2, 2, 2, 3, 4, 4, 3"
ObservationString(0) = "(8, 8, 10)_a"
ObservationTensor(0): ◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉
Rewards() = [0]
Returns() = [62.5517]
