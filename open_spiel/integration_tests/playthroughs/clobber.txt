game: clobber

GameType.chance_mode = ChanceMode.DETERMINISTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Clobber"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = ["columns", "rows"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "clobber"
GameType.utility = Utility.ZERO_SUM

NumDistinctActions() = 120
PolicyTensorShape() = [120]
MaxChanceOutcomes() = 0
GetParameters() = {columns=6,rows=5}
NumPlayers() = 2
MinUtility() = -1.0
MaxUtility() = 1.0
UtilitySum() = 0.0
ObservationTensorShape() = [3, 5, 6]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 90
MaxGameLength() = 29
ToString() = "clobber()"

# State 0
# 5oxoxox
# 4xoxoxo
# 3oxoxox
# 2xoxoxo
# 1oxoxox
#  abcdef
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = ""
InformationStateString(1) = ""
ObservationString(0) = "5oxoxox\n4xoxoxo\n3oxoxox\n2xoxoxo\n1oxoxox\n abcdef\n"
ObservationString(1) = "5oxoxox\n4xoxoxo\n3oxoxox\n2xoxoxo\n1oxoxox\n abcdef\n"
ObservationTensor(0):
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
ObservationTensor(1):
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [1, 2, 9, 10, 11, 17, 18, 19, 28, 29, 30, 31, 36, 37, 38, 39, 44, 46, 47, 48, 49, 50, 56, 57, 58, 59, 64, 65, 66, 67, 76, 77, 78, 79, 84, 85, 86, 87, 92, 94, 95, 96, 97, 104, 105, 107, 112, 113, 115]
StringLegalActions() = ["a5b5", "a5a4", "c5d5", "c5c4", "c5b5", "e5f5", "e5e4", "e5d5", "b4b5", "b4c4", "b4b3", "b4a4", "d4d5", "d4e4", "d4d3", "d4c4", "f4f5", "f4f3", "f4e4", "a3a4", "a3b3", "a3a2", "c3c4", "c3d3", "c3c2", "c3b3", "e3e4", "e3f3", "e3e2", "e3d3", "b2b3", "b2c2", "b2b1", "b2a2", "d2d3", "d2e2", "d2d1", "d2c2", "f2f3", "f2f1", "f2e2", "a1a2", "a1b1", "c1c2", "c1d1", "c1b1", "e1e2", "e1f1", "e1d1"]

# Apply action "c5c4"
action: 10

# State 1
# 5ox.xox
# 4xoooxo
# 3oxoxox
# 2xoxoxo
# 1oxoxox
#  abcdef
IsTerminal() = False
History() = [10]
HistoryString() = "10"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "10"
InformationStateString(1) = "10"
ObservationString(0) = "5ox.xox\n4xoooxo\n3oxoxox\n2xoxoxo\n1oxoxox\n abcdef\n"
ObservationString(1) = "5ox.xox\n4xoooxo\n3oxoxox\n2xoxoxo\n1oxoxox\n abcdef\n"
ObservationTensor(0):
◉◯◯◯◉◯  ◯◉◯◉◯◉  ◯◯◉◯◯◯
◯◉◉◉◯◉  ◉◯◯◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
ObservationTensor(1):
◯◉◯◉◯◉  ◉◯◯◯◉◯  ◯◯◉◯◯◯
◉◯◯◯◉◯  ◯◉◉◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [6, 7, 13, 14, 22, 23, 24, 25, 26, 40, 41, 42, 43, 52, 53, 54, 55, 60, 61, 62, 63, 68, 70, 71, 72, 73, 74, 80, 81, 82, 83, 88, 89, 90, 91, 100, 101, 103, 108, 109, 111, 116, 119]
StringLegalActions() = ["b5b4", "b5a5", "d5e5", "d5d4", "f5f4", "f5e5", "a4a5", "a4b4", "a4a3", "e4e5", "e4f4", "e4e3", "e4d4", "b3b4", "b3c3", "b3b2", "b3a3", "d3d4", "d3e3", "d3d2", "d3c3", "f3f4", "f3f2", "f3e3", "a2a3", "a2b2", "a2a1", "c2c3", "c2d2", "c2c1", "c2b2", "e2e3", "e2f2", "e2e1", "e2d2", "b1b2", "b1c1", "b1a1", "d1d2", "d1e1", "d1c1", "f1f2", "f1e1"]

# Apply action "b1a1"
action: 103

# State 2
# 5ox.xox
# 4xoooxo
# 3oxoxox
# 2xoxoxo
# 1x.oxox
#  abcdef
IsTerminal() = False
History() = [10, 103]
HistoryString() = "10, 103"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "10, 103"
InformationStateString(1) = "10, 103"
ObservationString(0) = "5ox.xox\n4xoooxo\n3oxoxox\n2xoxoxo\n1x.oxox\n abcdef\n"
ObservationString(1) = "5ox.xox\n4xoooxo\n3oxoxox\n2xoxoxo\n1x.oxox\n abcdef\n"
ObservationTensor(0):
◉◯◯◯◉◯  ◯◉◯◉◯◉  ◯◯◉◯◯◯
◯◉◉◉◯◉  ◉◯◯◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◯◯◉◯◉◯  ◉◯◯◉◯◉  ◯◉◯◯◯◯
ObservationTensor(1):
◯◉◯◉◯◉  ◉◯◯◯◉◯  ◯◯◉◯◯◯
◉◯◯◯◉◯  ◯◉◉◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◉◯◯◉◯◉  ◯◯◉◯◉◯  ◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [1, 2, 17, 18, 19, 28, 30, 31, 36, 37, 38, 44, 46, 47, 48, 49, 50, 57, 58, 59, 64, 65, 66, 67, 76, 77, 79, 84, 85, 86, 87, 92, 94, 95, 104, 105, 112, 113, 115]
StringLegalActions() = ["a5b5", "a5a4", "e5f5", "e5e4", "e5d5", "b4b5", "b4b3", "b4a4", "d4d5", "d4e4", "d4d3", "f4f5", "f4f3", "f4e4", "a3a4", "a3b3", "a3a2", "c3d3", "c3c2", "c3b3", "e3e4", "e3f3", "e3e2", "e3d3", "b2b3", "b2c2", "b2a2", "d2d3", "d2e2", "d2d1", "d2c2", "f2f3", "f2f1", "f2e2", "c1c2", "c1d1", "e1e2", "e1f1", "e1d1"]

# Apply action "e5f5"
action: 17

# State 3
# 5ox.x.o
# 4xoooxo
# 3oxoxox
# 2xoxoxo
# 1x.oxox
#  abcdef
IsTerminal() = False
History() = [10, 103, 17]
HistoryString() = "10, 103, 17"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "10, 103, 17"
InformationStateString(1) = "10, 103, 17"
ObservationString(0) = "5ox.x.o\n4xoooxo\n3oxoxox\n2xoxoxo\n1x.oxox\n abcdef\n"
ObservationString(1) = "5ox.x.o\n4xoooxo\n3oxoxox\n2xoxoxo\n1x.oxox\n abcdef\n"
ObservationTensor(0):
◉◯◯◯◯◉  ◯◉◯◉◯◯  ◯◯◉◯◉◯
◯◉◉◉◯◉  ◉◯◯◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◯◯◉◯◉◯  ◉◯◯◉◯◉  ◯◉◯◯◯◯
ObservationTensor(1):
◯◉◯◉◯◯  ◉◯◯◯◯◉  ◯◯◉◯◉◯
◉◯◯◯◉◯  ◯◉◉◉◯◉  ◯◯◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◉◯◯◉◯◉  ◯◯◉◯◉◯  ◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [6, 7, 14, 24, 25, 26, 41, 42, 43, 52, 53, 54, 55, 60, 61, 62, 63, 68, 70, 71, 72, 73, 80, 81, 82, 83, 88, 89, 90, 91, 108, 109, 111, 116, 119]
StringLegalActions() = ["b5b4", "b5a5", "d5d4", "a4a5", "a4b4", "a4a3", "e4f4", "e4e3", "e4d4", "b3b4", "b3c3", "b3b2", "b3a3", "d3d4", "d3e3", "d3d2", "d3c3", "f3f4", "f3f2", "f3e3", "a2a3", "a2b2", "c2c3", "c2d2", "c2c1", "c2b2", "e2e3", "e2f2", "e2e1", "e2d2", "d1d2", "d1e1", "d1c1", "f1f2", "f1e1"]

# Apply action "b3a3"
action: 55

# State 4
# 5ox.x.o
# 4xoooxo
# 3x.oxox
# 2xoxoxo
# 1x.oxox
#  abcdef
IsTerminal() = False
History() = [10, 103, 17, 55]
HistoryString() = "10, 103, 17, 55"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "10, 103, 17, 55"
InformationStateString(1) = "10, 103, 17, 55"
ObservationString(0) = "5ox.x.o\n4xoooxo\n3x.oxox\n2xoxoxo\n1x.oxox\n abcdef\n"
ObservationString(1) = "5ox.x.o\n4xoooxo\n3x.oxox\n2xoxoxo\n1x.oxox\n abcdef\n"
ObservationTensor(0):
◉◯◯◯◯◉  ◯◉◯◉◯◯  ◯◯◉◯◉◯
◯◉◉◉◯◉  ◉◯◯◯◉◯  ◯◯◯◯◯◯
◯◯◉◯◉◯  ◉◯◯◉◯◉  ◯◉◯◯◯◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◯◯◉◯◉◯  ◉◯◯◉◯◉  ◯◉◯◯◯◯
ObservationTensor(1):
◯◉◯◉◯◯  ◉◯◯◯◯◉  ◯◯◉◯◉◯
◉◯◯◯◉◯  ◯◉◉◉◯◉  ◯◯◯◯◯◯
◉◯◯◉◯◉  ◯◯◉◯◉◯  ◯◉◯◯◯◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◉◯◯◉◯◉  ◯◯◉◯◉◯  ◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [1, 2, 28, 31, 36, 37, 38, 46, 47, 57, 58, 64, 65, 66, 67, 77, 79, 84, 85, 86, 87, 92, 94, 95, 104, 105, 112, 113, 115]
StringLegalActions() = ["a5b5", "a5a4", "b4b5", "b4a4", "d4d5", "d4e4", "d4d3", "f4f3", "f4e4", "c3d3", "c3c2", "e3e4", "e3f3", "e3e2", "e3d3", "b2c2", "b2a2", "d2d3", "d2e2", "d2d1", "d2c2", "f2f3", "f2f1", "f2e2", "c1c2", "c1d1", "e1e2", "e1f1", "e1d1"]

# Apply action "e3e4"
action: 64

# State 5
# 5ox.x.o
# 4xooooo
# 3x.ox.x
# 2xoxoxo
# 1x.oxox
#  abcdef
IsTerminal() = False
History() = [10, 103, 17, 55, 64]
HistoryString() = "10, 103, 17, 55, 64"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "10, 103, 17, 55, 64"
InformationStateString(1) = "10, 103, 17, 55, 64"
ObservationString(0) = "5ox.x.o\n4xooooo\n3x.ox.x\n2xoxoxo\n1x.oxox\n abcdef\n"
ObservationString(1) = "5ox.x.o\n4xooooo\n3x.ox.x\n2xoxoxo\n1x.oxox\n abcdef\n"
ObservationTensor(0):
◉◯◯◯◯◉  ◯◉◯◉◯◯  ◯◯◉◯◉◯
◯◉◉◉◉◉  ◉◯◯◯◯◯  ◯◯◯◯◯◯
◯◯◉◯◯◯  ◉◯◯◉◯◉  ◯◉◯◯◉◯
◯◉◯◉◯◉  ◉◯◉◯◉◯  ◯◯◯◯◯◯
◯◯◉◯◉◯  ◉◯◯◉◯◉  ◯◉◯◯◯◯
ObservationTensor(1):
◯◉◯◉◯◯  ◉◯◯◯◯◉  ◯◯◉◯◉◯
◉◯◯◯◯◯  ◯◉◉◉◉◉  ◯◯◯◯◯◯
◉◯◯◉◯◉  ◯◯◉◯◯◯  ◯◉◯◯◉◯
◉◯◉◯◉◯  ◯◉◯◉◯◉  ◯◯◯◯◯◯
◉◯◯◉◯◉  ◯◯◉◯◉◯  ◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [6, 7, 14, 24, 25, 60, 62, 63, 68, 70, 73, 80, 81, 82, 83, 89, 90, 91, 108, 109, 111, 116, 119]
StringLegalActions() = ["b5b4", "b5a5", "d5d4", "a4a5", "a4b4", "d3d4", "d3d2", "d3c3", "f3f4", "f3f2", "a2b2", "c2c3", "c2d2", "c2c1", "c2b2", "e2f2", "e2e1", "e2d2", "d1d2", "d1e1", "d1c1", "f1f2", "f1e1"]

# Apply action "f3f4"
action: 68

# State 6
# Apply action "d4d3"
action: 38

# State 7
# Apply action "f1f2"
action: 116

# State 8
# Apply action "c1c2"
action: 104

# State 9
# Apply action "d1e1"
action: 109

# State 10
# Apply action "e4f4"
action: 41

# State 11
# Apply action "b5a5"
action: 7

# State 12
# Apply action "d2e2"
action: 85

# State 13
# Apply action "f2e2"
action: 95

# State 14
# Apply action "b4a4"
action: 31

# State 15
# Apply action "a3a4"
action: 48

# State 16
# Apply action "b2a2"
action: 79

# State 17
# Apply action "a1a2"
action: 96

# State 18
# 5x..x.o
# 4x.o..o
# 3..oo..
# 2x.o.x.
# 1....x.
#  abcdef
IsTerminal() = True
History() = [10, 103, 17, 55, 64, 68, 38, 116, 104, 109, 41, 7, 85, 95, 31, 48, 79, 96]
HistoryString() = "10, 103, 17, 55, 64, 68, 38, 116, 104, 109, 41, 7, 85, 95, 31, 48, 79, 96"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "10, 103, 17, 55, 64, 68, 38, 116, 104, 109, 41, 7, 85, 95, 31, 48, 79, 96"
InformationStateString(1) = "10, 103, 17, 55, 64, 68, 38, 116, 104, 109, 41, 7, 85, 95, 31, 48, 79, 96"
ObservationString(0) = "5x..x.o\n4x.o..o\n3..oo..\n2x.o.x.\n1....x.\n abcdef\n"
ObservationString(1) = "5x..x.o\n4x.o..o\n3..oo..\n2x.o.x.\n1....x.\n abcdef\n"
ObservationTensor(0):
◯◯◯◯◯◉  ◉◯◯◉◯◯  ◯◉◉◯◉◯
◯◯◉◯◯◉  ◉◯◯◯◯◯  ◯◉◯◉◉◯
◯◯◉◉◯◯  ◯◯◯◯◯◯  ◉◉◯◯◉◉
◯◯◉◯◯◯  ◉◯◯◯◉◯  ◯◉◯◉◯◉
◯◯◯◯◯◯  ◯◯◯◯◉◯  ◉◉◉◉◯◉
ObservationTensor(1):
◉◯◯◉◯◯  ◯◯◯◯◯◉  ◯◉◉◯◉◯
◉◯◯◯◯◯  ◯◯◉◯◯◉  ◯◉◯◉◉◯
◯◯◯◯◯◯  ◯◯◉◉◯◯  ◉◉◯◯◉◉
◉◯◯◯◉◯  ◯◯◉◯◯◯  ◯◉◯◉◯◉
◯◯◯◯◉◯  ◯◯◯◯◯◯  ◉◉◉◉◯◉
Rewards() = [-1.0, 1.0]
Returns() = [-1.0, 1.0]
