game: python_param_social_dilemma

GameType.chance_mode = ChanceMode.DETERMINISTIC
GameType.dynamics = Dynamics.SIMULTANEOUS
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Python Parameterized Social Dilemma"
GameType.max_num_players = 10
GameType.min_num_players = 2
GameType.parameter_specification = ["dynamic_payoffs", "max_game_length", "num_actions", "payoff_change_prob", "payoff_matrix", "players", "reward_noise_std"]
GameType.provides_information_state_string = False
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = False
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.REWARDS
GameType.short_name = "python_param_social_dilemma"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 2
PolicyTensorShape() = [2]
MaxChanceOutcomes() = 0
GetParameters() = {dynamic_payoffs=False,max_game_length=10,num_actions=2,payoff_change_prob=0.0,payoff_matrix=default,players=3,reward_noise_std=0.0}
NumPlayers() = 3
MinUtility() = 0.0
MaxUtility() = 33.333
UtilitySum() = None
MaxGameLength() = 10
ToString() = "python_param_social_dilemma(dynamic_payoffs=False,max_game_length=10,num_actions=2,payoff_change_prob=0.0,payoff_matrix=default,players=3,reward_noise_std=0.0)"

# State 0
# initial
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = -2
InformationStateString(0) = "initial"
InformationStateString(1) = "initial"
InformationStateString(2) = "initial"
ObservationString(0) = "initial"
ObservationString(1) = "initial"
ObservationString(2) = "initial"
Rewards() = [0, 0, 0]
Returns() = [0, 0, 0]
LegalActions(0) = [0, 1]
LegalActions(1) = [0, 1]
LegalActions(2) = [0, 1]
StringLegalActions(0) = ["C", "D"]
StringLegalActions(1) = ["C", "D"]
StringLegalActions(2) = ["C", "D"]

# Apply joint action ["D", "C", "D"]
actions: [1, 0, 1]

# State 1
# t0:[D,C,D]
IsTerminal() = False
History() = [1, 0, 1]
HistoryString() = "1, 0, 1"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = -2
InformationStateString(0) = "t0:[D,C,D]"
InformationStateString(1) = "t0:[D,C,D]"
InformationStateString(2) = "t0:[D,C,D]"
ObservationString(0) = "t0:[D,C,D]"
ObservationString(1) = "t0:[D,C,D]"
ObservationString(2) = "t0:[D,C,D]"
Rewards() = [1.66667, 1, 1.66667]
Returns() = [1.66667, 1, 1.66667]
LegalActions(0) = [0, 1]
LegalActions(1) = [0, 1]
LegalActions(2) = [0, 1]
StringLegalActions(0) = ["C", "D"]
StringLegalActions(1) = ["C", "D"]
StringLegalActions(2) = ["C", "D"]

# Apply joint action ["C", "C", "C"]
actions: [0, 0, 0]

# State 2
# t0:[D,C,D] t1:[C,C,C]
IsTerminal() = False
History() = [1, 0, 1, 0, 0, 0]
HistoryString() = "1, 0, 1, 0, 0, 0"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = -2
InformationStateString(0) = "t0:[D,C,D] t1:[C,C,C]"
InformationStateString(1) = "t0:[D,C,D] t1:[C,C,C]"
InformationStateString(2) = "t0:[D,C,D] t1:[C,C,C]"
ObservationString(0) = "t0:[D,C,D] t1:[C,C,C]"
ObservationString(1) = "t0:[D,C,D] t1:[C,C,C]"
ObservationString(2) = "t0:[D,C,D] t1:[C,C,C]"
Rewards() = [3, 3, 3]
Returns() = [4.66667, 4, 4.66667]
LegalActions(0) = [0, 1]
LegalActions(1) = [0, 1]
LegalActions(2) = [0, 1]
StringLegalActions(0) = ["C", "D"]
StringLegalActions(1) = ["C", "D"]
StringLegalActions(2) = ["C", "D"]

# Apply joint action ["D", "C", "C"]
actions: [1, 0, 0]

# State 3
# Apply joint action ["C", "D", "C"]
actions: [0, 1, 0]

# State 4
# Apply joint action ["D", "D", "C"]
actions: [1, 1, 0]

# State 5
# Apply joint action ["D", "C", "C"]
actions: [1, 0, 0]

# State 6
# Apply joint action ["D", "C", "C"]
actions: [1, 0, 0]

# State 7
# Apply joint action ["D", "C", "C"]
actions: [1, 0, 0]

# State 8
# Apply joint action ["D", "C", "C"]
actions: [1, 0, 0]

# State 9
# Apply joint action ["C", "C", "D"]
actions: [0, 0, 1]

# State 10
# t0:[D,C,D] t1:[C,C,C] t2:[D,C,C] t3:[C,D,C] t4:[D,D,C] t5:[D,C,C] t6:[D,C,C] t7:[D,C,C] t8:[D,C,C] t9:[C,C,D]
IsTerminal() = True
History() = [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]
HistoryString() = "1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "t0:[D,C,D] t1:[C,C,C] t2:[D,C,C] t3:[C,D,C] t4:[D,D,C] t5:[D,C,C] t6:[D,C,C] t7:[D,C,C] t8:[D,C,C] t9:[C,C,D]"
InformationStateString(1) = "t0:[D,C,D] t1:[C,C,C] t2:[D,C,C] t3:[C,D,C] t4:[D,D,C] t5:[D,C,C] t6:[D,C,C] t7:[D,C,C] t8:[D,C,C] t9:[C,C,D]"
InformationStateString(2) = "t0:[D,C,D] t1:[C,C,C] t2:[D,C,C] t3:[C,D,C] t4:[D,D,C] t5:[D,C,C] t6:[D,C,C] t7:[D,C,C] t8:[D,C,C] t9:[C,C,D]"
ObservationString(0) = "t0:[D,C,D] t1:[C,C,C] t2:[D,C,C] t3:[C,D,C] t4:[D,D,C] t5:[D,C,C] t6:[D,C,C] t7:[D,C,C] t8:[D,C,C] t9:[C,C,D]"
ObservationString(1) = "t0:[D,C,D] t1:[C,C,C] t2:[D,C,C] t3:[C,D,C] t4:[D,D,C] t5:[D,C,C] t6:[D,C,C] t7:[D,C,C] t8:[D,C,C] t9:[C,C,D]"
ObservationString(2) = "t0:[D,C,D] t1:[C,C,C] t2:[D,C,C] t3:[C,D,C] t4:[D,D,C] t5:[D,C,C] t6:[D,C,C] t7:[D,C,C] t8:[D,C,C] t9:[C,C,D]"
Rewards() = [2, 2, 3.33333]
Returns() = [27, 21, 21]
