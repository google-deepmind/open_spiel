game: start_at(history=4;3;3;2;0;4;4;4;4;0,game=connect_four())

GameType.chance_mode = ChanceMode.DETERMINISTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "StartAt history=4;3;3;2;0;4;4;4;4;0 game=Connect Four"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = []
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "start_at"
GameType.utility = Utility.ZERO_SUM

NumDistinctActions() = 7
PolicyTensorShape() = [7]
MaxChanceOutcomes() = 0
GetParameters() = {game=connect_four(),history=4;3;3;2;0;4;4;4;4;0}
NumPlayers() = 2
MinUtility() = -1.0
MaxUtility() = 1.0
UtilitySum() = 0.0
ObservationTensorShape() = [3, 6, 7]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 126
MaxGameLength() = 42
ToString() = "start_at(game=connect_four(),history=4;3;3;2;0;4;4;4;4;0)"

# State 0
# .......
# ....x..
# ....o..
# ....x..
# o..xo..
# x.oox..
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0"
InformationStateString(1) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0"
ObservationString(0) = ".......\n....x..\n....o..\n....x..\no..xo..\nx.oox..\n"
ObservationString(1) = ".......\n....x..\n....o..\n....x..\no..xo..\nx.oox..\n"
ObservationTensor(0):
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◉◯◯◯◉◯◯  ◯◯◯◉◯◯◯  ◯◉◉◯◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◉
ObservationTensor(1):
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◯◯◯◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◉◯◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 4, 5, 6]
StringLegalActions() = ["x0", "x1", "x2", "x3", "x4", "x5", "x6"]

# Apply action "x2"
action: 2

# State 1
# .......
# ....x..
# ....o..
# ....x..
# o.xxo..
# x.oox..
IsTerminal() = False
History() = [2]
HistoryString() = "2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2"
InformationStateString(1) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2"
ObservationString(0) = ".......\n....x..\n....o..\n....x..\no.xxo..\nx.oox..\n"
ObservationString(1) = ".......\n....x..\n....o..\n....x..\no.xxo..\nx.oox..\n"
ObservationTensor(0):
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◉
ObservationTensor(1):
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 4, 5, 6]
StringLegalActions() = ["o0", "o1", "o2", "o3", "o4", "o5", "o6"]

# Apply action "o0"
action: 0

# State 2
# .......
# ....x..
# ....o..
# o...x..
# o.xxo..
# x.oox..
IsTerminal() = False
History() = [2, 0]
HistoryString() = "2, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0"
InformationStateString(1) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0"
ObservationString(0) = ".......\n....x..\n....o..\no...x..\no.xxo..\nx.oox..\n"
ObservationString(1) = ".......\n....x..\n....o..\no...x..\no.xxo..\nx.oox..\n"
ObservationTensor(0):
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◉◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◯◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◉
ObservationTensor(1):
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◯◯◯◯◉◯◯  ◉◯◯◯◯◯◯  ◯◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 4, 5, 6]
StringLegalActions() = ["x0", "x1", "x2", "x3", "x4", "x5", "x6"]

# Apply action "x4"
action: 4

# State 3
# ....x..
# ....x..
# ....o..
# o...x..
# o.xxo..
# x.oox..
IsTerminal() = False
History() = [2, 0, 4]
HistoryString() = "2, 0, 4"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0, 4"
InformationStateString(1) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0, 4"
ObservationString(0) = "....x..\n....x..\n....o..\no...x..\no.xxo..\nx.oox..\n"
ObservationString(1) = "....x..\n....x..\n....o..\no...x..\no.xxo..\nx.oox..\n"
ObservationTensor(0):
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◉◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◯◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
ObservationTensor(1):
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◯◯◯◯◉◯◯  ◉◯◯◯◯◯◯  ◯◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 5, 6]
StringLegalActions() = ["o0", "o1", "o2", "o3", "o5", "o6"]

# Apply action "o1"
action: 1

# State 4
# ....x..
# ....x..
# ....o..
# o...x..
# o.xxo..
# xooox..
IsTerminal() = False
History() = [2, 0, 4, 1]
HistoryString() = "2, 0, 4, 1"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0, 4, 1"
InformationStateString(1) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0, 4, 1"
ObservationString(0) = "....x..\n....x..\n....o..\no...x..\no.xxo..\nxooox..\n"
ObservationString(1) = "....x..\n....x..\n....o..\no...x..\no.xxo..\nxooox..\n"
ObservationTensor(0):
◯◉◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◯◯◯◯◉◉
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◉◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◯◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
ObservationTensor(1):
◉◯◯◯◉◯◯  ◯◉◉◉◯◯◯  ◯◯◯◯◯◉◉
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◯◯◯◯◉◯◯  ◉◯◯◯◯◯◯  ◯◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 5, 6]
StringLegalActions() = ["x0", "x1", "x2", "x3", "x5", "x6"]

# Apply action "x2"
action: 2

# State 5
# ....x..
# ....x..
# ....o..
# o.x.x..
# o.xxo..
# xooox..
IsTerminal() = False
History() = [2, 0, 4, 1, 2]
HistoryString() = "2, 0, 4, 1, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0, 4, 1, 2"
InformationStateString(1) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0, 4, 1, 2"
ObservationString(0) = "....x..\n....x..\n....o..\no.x.x..\no.xxo..\nxooox..\n"
ObservationString(1) = "....x..\n....x..\n....o..\no.x.x..\no.xxo..\nxooox..\n"
ObservationTensor(0):
◯◉◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◯◯◯◯◉◉
◉◯◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◉◯◯◯◉◉
◉◯◯◯◯◯◯  ◯◯◉◯◉◯◯  ◯◉◯◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
ObservationTensor(1):
◉◯◯◯◉◯◯  ◯◉◉◉◯◯◯  ◯◯◯◯◯◉◉
◯◯◉◉◯◯◯  ◉◯◯◯◉◯◯  ◯◉◯◯◯◉◉
◯◯◉◯◉◯◯  ◉◯◯◯◯◯◯  ◯◉◯◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 5, 6]
StringLegalActions() = ["o0", "o1", "o2", "o3", "o5", "o6"]

# Apply action "o6"
action: 6

# State 6
# Apply action "x3"
action: 3

# State 7
# Apply action "o6"
action: 6

# State 8
# Apply action "x2"
action: 2

# State 9
# Apply action "o3"
action: 3

# State 10
# Apply action "x6"
action: 6

# State 11
# Apply action "o1"
action: 1

# State 12
# Apply action "x2"
action: 2

# State 13
# ....x..
# ..x.x..
# ..xoo..
# o.xxx.x
# ooxxo.o
# xooox.o
IsTerminal() = True
History() = [2, 0, 4, 1, 2, 6, 3, 6, 2, 3, 6, 1, 2]
HistoryString() = "2, 0, 4, 1, 2, 6, 3, 6, 2, 3, 6, 1, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0, 4, 1, 2, 6, 3, 6, 2, 3, 6, 1, 2"
InformationStateString(1) = "4, 3, 3, 2, 0, 4, 4, 4, 4, 0, 2, 0, 4, 1, 2, 6, 3, 6, 2, 3, 6, 1, 2"
ObservationString(0) = "....x..\n..x.x..\n..xoo..\no.xxx.x\nooxxo.o\nxooox.o\n"
ObservationString(1) = "....x..\n..x.x..\n..xoo..\no.xxx.x\nooxxo.o\nxooox.o\n"
ObservationTensor(0):
◯◉◉◉◯◯◉  ◉◯◯◯◉◯◯  ◯◯◯◯◯◉◯
◉◉◯◯◉◯◉  ◯◯◉◉◯◯◯  ◯◯◯◯◯◉◯
◉◯◯◯◯◯◯  ◯◯◉◉◉◯◉  ◯◉◯◯◯◉◯
◯◯◯◉◉◯◯  ◯◯◉◯◯◯◯  ◉◉◯◯◯◉◉
◯◯◯◯◯◯◯  ◯◯◉◯◉◯◯  ◉◉◯◉◯◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯  ◉◉◉◉◯◉◉
ObservationTensor(1):
◉◯◯◯◉◯◯  ◯◉◉◉◯◯◉  ◯◯◯◯◯◉◯
◯◯◉◉◯◯◯  ◉◉◯◯◉◯◉  ◯◯◯◯◯◉◯
◯◯◉◉◉◯◉  ◉◯◯◯◯◯◯  ◯◉◯◯◯◉◯
◯◯◉◯◯◯◯  ◯◯◯◉◉◯◯  ◉◉◯◯◯◉◉
◯◯◉◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◯◉◯◉◉
◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◉◉
Rewards() = [1.0, -1.0]
Returns() = [1.0, -1.0]
