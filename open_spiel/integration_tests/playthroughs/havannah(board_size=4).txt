game: havannah(board_size=4)

GameType.chance_mode = ChanceMode.DETERMINISTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Havannah"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = ["ansi_color_output", "board_size", "swap"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "havannah"
GameType.utility = Utility.ZERO_SUM

NumDistinctActions() = 49
PolicyTensorShape() = [49]
MaxChanceOutcomes() = 0
GetParameters() = {ansi_color_output=False,board_size=4,swap=False}
NumPlayers() = 2
MinUtility() = -1.0
MaxUtility() = 1.0
UtilitySum() = 0.0
ObservationTensorShape() = [3, 7, 7]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 147
MaxGameLength() = 37
ToString() = "havannah(board_size=4)"

# State 0
#         a b c d
#      1 . . . . e
#     2 . . . . . f
#    3 . . . . . . g
#   4 . . . . . . .
#    5 . . . . . .
#     6 . . . . .
#      7 . . . .
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = ""
InformationStateString(1) = ""
ObservationString(0) = "        a b c d\n     1 . . . . e\n    2 . . . . . f\n   3 . . . . . . g\n  4 . . . . . . .\n   5 . . . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationString(1) = "        a b c d\n     1 . . . . e\n    2 . . . . . f\n   3 . . . . . . g\n  4 . . . . . . .\n   5 . . . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationTensor(0):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 37, 38, 39, 40, 41, 45, 46, 47, 48]
StringLegalActions() = ["a1", "b1", "c1", "d1", "a2", "b2", "c2", "d2", "e2", "a3", "b3", "c3", "d3", "e3", "f3", "a4", "b4", "c4", "d4", "e4", "f4", "g4", "b5", "c5", "d5", "e5", "f5", "g5", "c6", "d6", "e6", "f6", "g6", "d7", "e7", "f7", "g7"]

# Apply action "c4"
action: 23

# State 1
#         a b c d
#      1 . . . . e
#     2 . . . . . f
#    3 . . . . . . g
#   4 . .[O]. . . .
#    5 . . . . . .
#     6 . . . . .
#      7 . . . .
IsTerminal() = False
History() = [23]
HistoryString() = "23"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "23"
InformationStateString(1) = "23"
ObservationString(0) = "        a b c d\n     1 . . . . e\n    2 . . . . . f\n   3 . . . . . . g\n  4 . .[O]. . . .\n   5 . . . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationString(1) = "        a b c d\n     1 . . . . e\n    2 . . . . . f\n   3 . . . . . . g\n  4 . .[O]. . . .\n   5 . . . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationTensor(0):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◉◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◯◯◯◯◯  ◯◯◉◯◯◯◯  ◉◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 37, 38, 39, 40, 41, 45, 46, 47, 48]
StringLegalActions() = ["a1", "b1", "c1", "d1", "a2", "b2", "c2", "d2", "e2", "a3", "b3", "c3", "d3", "e3", "f3", "a4", "b4", "d4", "e4", "f4", "g4", "b5", "c5", "d5", "e5", "f5", "g5", "c6", "d6", "e6", "f6", "g6", "d7", "e7", "f7", "g7"]

# Apply action "d2"
action: 10

# State 2
#         a b c d
#      1 . . . . e
#     2 . . .[@]. f
#    3 . . . . . . g
#   4 . . O . . . .
#    5 . . . . . .
#     6 . . . . .
#      7 . . . .
IsTerminal() = False
History() = [23, 10]
HistoryString() = "23, 10"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "23, 10"
InformationStateString(1) = "23, 10"
ObservationString(0) = "        a b c d\n     1 . . . . e\n    2 . . .[@]. f\n   3 . . . . . . g\n  4 . . O . . . .\n   5 . . . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationString(1) = "        a b c d\n     1 . . . . e\n    2 . . .[@]. f\n   3 . . . . . . g\n  4 . . O . . . .\n   5 . . . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationTensor(0):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◉◯◯◯  ◉◉◉◯◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◉◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◉◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◯◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◯◯◯◯◯  ◯◯◉◯◯◯◯  ◉◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 7, 8, 9, 11, 14, 15, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 37, 38, 39, 40, 41, 45, 46, 47, 48]
StringLegalActions() = ["a1", "b1", "c1", "d1", "a2", "b2", "c2", "e2", "a3", "b3", "c3", "d3", "e3", "f3", "a4", "b4", "d4", "e4", "f4", "g4", "b5", "c5", "d5", "e5", "f5", "g5", "c6", "d6", "e6", "f6", "g6", "d7", "e7", "f7", "g7"]

# Apply action "c5"
action: 30

# State 3
#         a b c d
#      1 . . . . e
#     2 . . . @ . f
#    3 . . . . . . g
#   4 . . O . . . .
#    5 .[O]. . . .
#     6 . . . . .
#      7 . . . .
IsTerminal() = False
History() = [23, 10, 30]
HistoryString() = "23, 10, 30"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "23, 10, 30"
InformationStateString(1) = "23, 10, 30"
ObservationString(0) = "        a b c d\n     1 . . . . e\n    2 . . . @ . f\n   3 . . . . . . g\n  4 . . O . . . .\n   5 .[O]. . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationString(1) = "        a b c d\n     1 . . . . e\n    2 . . . @ . f\n   3 . . . . . . g\n  4 . . O . . . .\n   5 .[O]. . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationTensor(0):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◉◯◯◯  ◉◉◉◯◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◉◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◯◉◉◉◉
◯◯◉◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◉◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◯◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◯◯◯◯◯  ◯◯◉◯◯◯◯  ◉◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◉◯◯◯◯  ◯◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 7, 8, 9, 11, 14, 15, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 29, 31, 32, 33, 34, 37, 38, 39, 40, 41, 45, 46, 47, 48]
StringLegalActions() = ["a1", "b1", "c1", "d1", "a2", "b2", "c2", "e2", "a3", "b3", "c3", "d3", "e3", "f3", "a4", "b4", "d4", "e4", "f4", "g4", "b5", "d5", "e5", "f5", "g5", "c6", "d6", "e6", "f6", "g6", "d7", "e7", "f7", "g7"]

# Apply action "b4"
action: 22

# State 4
#         a b c d
#      1 . . . . e
#     2 . . . @ . f
#    3 . . . . . . g
#   4 .[@]O . . . .
#    5 . O . . . .
#     6 . . . . .
#      7 . . . .
IsTerminal() = False
History() = [23, 10, 30, 22]
HistoryString() = "23, 10, 30, 22"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "23, 10, 30, 22"
InformationStateString(1) = "23, 10, 30, 22"
ObservationString(0) = "        a b c d\n     1 . . . . e\n    2 . . . @ . f\n   3 . . . . . . g\n  4 .[@]O . . . .\n   5 . O . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationString(1) = "        a b c d\n     1 . . . . e\n    2 . . . @ . f\n   3 . . . . . . g\n  4 .[@]O . . . .\n   5 . O . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationTensor(0):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◉◯◯◯  ◉◉◉◯◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◉◯◯◯◯  ◯◉◯◯◯◯◯  ◉◯◯◉◉◉◉
◯◯◉◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◉◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◯◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◉◯◯◯◯◯  ◯◯◉◯◯◯◯  ◉◯◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◉◯◯◯◯  ◯◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 7, 8, 9, 11, 14, 15, 16, 17, 18, 19, 21, 24, 25, 26, 27, 29, 31, 32, 33, 34, 37, 38, 39, 40, 41, 45, 46, 47, 48]
StringLegalActions() = ["a1", "b1", "c1", "d1", "a2", "b2", "c2", "e2", "a3", "b3", "c3", "d3", "e3", "f3", "a4", "d4", "e4", "f4", "g4", "b5", "d5", "e5", "f5", "g5", "c6", "d6", "e6", "f6", "g6", "d7", "e7", "f7", "g7"]

# Apply action "f4"
action: 26

# State 5
#         a b c d
#      1 . . . . e
#     2 . . . @ . f
#    3 . . . . . . g
#   4 . @ O . .[O].
#    5 . O . . . .
#     6 . . . . .
#      7 . . . .
IsTerminal() = False
History() = [23, 10, 30, 22, 26]
HistoryString() = "23, 10, 30, 22, 26"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "23, 10, 30, 22, 26"
InformationStateString(1) = "23, 10, 30, 22, 26"
ObservationString(0) = "        a b c d\n     1 . . . . e\n    2 . . . @ . f\n   3 . . . . . . g\n  4 . @ O . .[O].\n   5 . O . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationString(1) = "        a b c d\n     1 . . . . e\n    2 . . . @ . f\n   3 . . . . . . g\n  4 . @ O . .[O].\n   5 . O . . . .\n    6 . . . . .\n     7 . . . .\n"
ObservationTensor(0):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◉◯◯◯  ◉◉◉◯◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◯◉◯◯◉◯  ◯◉◯◯◯◯◯  ◉◯◯◉◉◯◉
◯◯◉◯◯◯◯  ◯◯◯◯◯◯◯  ◯◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◯◯◯
◯◯◯◉◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◯◉◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◉◉◉◉◉◉◯
◯◉◯◯◯◯◯  ◯◯◉◯◯◉◯  ◉◯◯◉◉◯◉
◯◯◯◯◯◯◯  ◯◯◉◯◯◯◯  ◯◉◯◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◉◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2, 3, 7, 8, 9, 11, 14, 15, 16, 17, 18, 19, 21, 24, 25, 27, 29, 31, 32, 33, 34, 37, 38, 39, 40, 41, 45, 46, 47, 48]
StringLegalActions() = ["a1", "b1", "c1", "d1", "a2", "b2", "c2", "e2", "a3", "b3", "c3", "d3", "e3", "f3", "a4", "d4", "e4", "g4", "b5", "d5", "e5", "f5", "g5", "c6", "d6", "e6", "f6", "g6", "d7", "e7", "f7", "g7"]

# Apply action "b3"
action: 15

# State 6
# Apply action "e2"
action: 11

# State 7
# Apply action "f5"
action: 33

# State 8
# Apply action "g4"
action: 27

# State 9
# Apply action "e5"
action: 32

# State 10
# Apply action "b2"
action: 8

# State 11
# Apply action "g5"
action: 34

# State 12
# Apply action "a4"
action: 21

# State 13
# Apply action "a3"
action: 14

# State 14
# Apply action "d4"
action: 24

# State 15
# Apply action "f3"
action: 19

# State 16
# Apply action "d1"
action: 3

# State 17
# Apply action "f6"
action: 40

# State 18
# Apply action "d5"
action: 31

# State 19
# Apply action "a1"
action: 0

# State 20
#         a b c d
#      1[@]. . O e
#     2 . O . @ O f
#    3 @ @ . . . @ g
#   4 O @ O O . O O
#    5 . O O @ @ @
#     6 . . . @ .
#      7 . . . .
IsTerminal() = False
History() = [23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0]
HistoryString() = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0"
InformationStateString(1) = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0"
ObservationString(0) = "        a b c d\n     1[@]. . O e\n    2 . O . @ O f\n   3 @ @ . . . @ g\n  4 O @ O O . O O\n   5 . O O @ @ @\n    6 . . . @ .\n     7 . . . .\n"
ObservationString(1) = "        a b c d\n     1[@]. . O e\n    2 . O . @ O f\n   3 @ @ . . . @ g\n  4 O @ O O . O O\n   5 . O O @ @ @\n    6 . . . @ .\n     7 . . . .\n"
ObservationTensor(0):
◯◯◯◉◯◯◯  ◉◯◯◯◯◯◯  ◯◉◉◯◯◯◯
◯◉◯◯◉◯◯  ◯◯◯◉◯◯◯  ◉◯◉◯◯◯◯
◯◯◯◯◯◯◯  ◉◉◯◯◯◉◯  ◯◯◉◉◉◯◯
◉◯◉◉◯◉◉  ◯◉◯◯◯◯◯  ◯◯◯◯◉◯◯
◯◯◉◉◯◯◯  ◯◯◯◯◉◉◉  ◯◉◯◯◯◯◯
◯◯◯◯◯◯◯  ◯◯◯◯◯◉◯  ◯◯◉◉◉◯◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
ObservationTensor(1):
◉◯◯◯◯◯◯  ◯◯◯◉◯◯◯  ◯◉◉◯◯◯◯
◯◯◯◉◯◯◯  ◯◉◯◯◉◯◯  ◉◯◉◯◯◯◯
◉◉◯◯◯◉◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◯◯
◯◉◯◯◯◯◯  ◉◯◉◉◯◉◉  ◯◯◯◯◉◯◯
◯◯◯◯◉◉◉  ◯◯◉◉◯◯◯  ◯◉◯◯◯◯◯
◯◯◯◯◯◉◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◯◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [1, 2, 7, 9, 16, 17, 18, 25, 29, 37, 38, 39, 41, 45, 46, 47, 48]
StringLegalActions() = ["b1", "c1", "a2", "c2", "c3", "d3", "e3", "e4", "b5", "c6", "d6", "e6", "g6", "d7", "e7", "f7", "g7"]

# Apply action "d6"
action: 38

# State 21
#         a b c d
#      1 @ . . O e
#     2 . O . @ O f
#    3 @ @ . . . @ g
#   4 O @ O O . O O
#    5 . O O @ @ @
#     6 .[O]. @ .
#      7 . . . .
IsTerminal() = False
History() = [23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0, 38]
HistoryString() = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0, 38"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0, 38"
InformationStateString(1) = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0, 38"
ObservationString(0) = "        a b c d\n     1 @ . . O e\n    2 . O . @ O f\n   3 @ @ . . . @ g\n  4 O @ O O . O O\n   5 . O O @ @ @\n    6 .[O]. @ .\n     7 . . . .\n"
ObservationString(1) = "        a b c d\n     1 @ . . O e\n    2 . O . @ O f\n   3 @ @ . . . @ g\n  4 O @ O O . O O\n   5 . O O @ @ @\n    6 .[O]. @ .\n     7 . . . .\n"
ObservationTensor(0):
◯◯◯◉◯◯◯  ◉◯◯◯◯◯◯  ◯◉◉◯◯◯◯
◯◉◯◯◉◯◯  ◯◯◯◉◯◯◯  ◉◯◉◯◯◯◯
◯◯◯◯◯◯◯  ◉◉◯◯◯◉◯  ◯◯◉◉◉◯◯
◉◯◉◉◯◉◉  ◯◉◯◯◯◯◯  ◯◯◯◯◉◯◯
◯◯◉◉◯◯◯  ◯◯◯◯◉◉◉  ◯◉◯◯◯◯◯
◯◯◯◉◯◯◯  ◯◯◯◯◯◉◯  ◯◯◉◯◉◯◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
ObservationTensor(1):
◉◯◯◯◯◯◯  ◯◯◯◉◯◯◯  ◯◉◉◯◯◯◯
◯◯◯◉◯◯◯  ◯◉◯◯◉◯◯  ◉◯◉◯◯◯◯
◉◉◯◯◯◉◯  ◯◯◯◯◯◯◯  ◯◯◉◉◉◯◯
◯◉◯◯◯◯◯  ◉◯◉◉◯◉◉  ◯◯◯◯◉◯◯
◯◯◯◯◉◉◉  ◯◯◉◉◯◯◯  ◯◉◯◯◯◯◯
◯◯◯◯◯◉◯  ◯◯◯◉◯◯◯  ◯◯◉◯◉◯◉
◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯  ◯◯◯◉◉◉◉
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [1, 2, 7, 9, 16, 17, 18, 25, 29, 37, 39, 41, 45, 46, 47, 48]
StringLegalActions() = ["b1", "c1", "a2", "c2", "c3", "d3", "e3", "e4", "b5", "c6", "e6", "g6", "d7", "e7", "f7", "g7"]

# Apply action "d7"
action: 45

# State 22
# Apply action "e6"
action: 39

# State 23
# Apply action "e3"
action: 18

# State 24
# Apply action "f7"
action: 47

# State 25
# Apply action "g6"
action: 41

# State 26
# Apply action "c1"
action: 2

# State 27
# Apply action "c2"
action: 9

# State 28
# Apply action "c6"
action: 37

# State 29
# Apply action "d3"
action: 17

# State 30
# Apply action "b1"
action: 1

# State 31
# Apply action "g7"
action: 48

# State 32
# Apply action "a2"
action: 7

# State 33
#         a b c d
#      1 @ O O O e
#     2[O]O @ @ O f
#    3 @ @ . @ @ @ g
#   4 O @ O O . O O
#    5 . O O @ @ @
#     6 O O O @ @
#      7 @ . O @
IsTerminal() = True
History() = [23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0, 38, 45, 39, 18, 47, 41, 2, 9, 37, 17, 1, 48, 7]
HistoryString() = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0, 38, 45, 39, 18, 47, 41, 2, 9, 37, 17, 1, 48, 7"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0, 38, 45, 39, 18, 47, 41, 2, 9, 37, 17, 1, 48, 7"
InformationStateString(1) = "23, 10, 30, 22, 26, 15, 11, 33, 27, 32, 8, 34, 21, 14, 24, 19, 3, 40, 31, 0, 38, 45, 39, 18, 47, 41, 2, 9, 37, 17, 1, 48, 7"
ObservationString(0) = "        a b c d\n     1 @ O O O e\n    2[O]O @ @ O f\n   3 @ @ . @ @ @ g\n  4 O @ O O . O O\n   5 . O O @ @ @\n    6 O O O @ @\n     7 @ . O @\n"
ObservationString(1) = "        a b c d\n     1 @ O O O e\n    2[O]O @ @ O f\n   3 @ @ . @ @ @ g\n  4 O @ O O . O O\n   5 . O O @ @ @\n    6 O O O @ @\n     7 @ . O @\n"
ObservationTensor(0):
◯◉◉◉◯◯◯  ◉◯◯◯◯◯◯  ◯◯◯◯◯◯◯
◉◉◯◯◉◯◯  ◯◯◉◉◯◯◯  ◯◯◯◯◯◯◯
◯◯◯◯◯◯◯  ◉◉◯◉◉◉◯  ◯◯◉◯◯◯◯
◉◯◉◉◯◉◉  ◯◉◯◯◯◯◯  ◯◯◯◯◉◯◯
◯◯◉◉◯◯◯  ◯◯◯◯◉◉◉  ◯◉◯◯◯◯◯
◯◯◉◉◉◯◯  ◯◯◯◯◯◉◉  ◯◯◯◯◯◯◯
◯◯◯◯◯◉◯  ◯◯◯◉◯◯◉  ◯◯◯◯◉◯◯
ObservationTensor(1):
◉◯◯◯◯◯◯  ◯◉◉◉◯◯◯  ◯◯◯◯◯◯◯
◯◯◉◉◯◯◯  ◉◉◯◯◉◯◯  ◯◯◯◯◯◯◯
◉◉◯◉◉◉◯  ◯◯◯◯◯◯◯  ◯◯◉◯◯◯◯
◯◉◯◯◯◯◯  ◉◯◉◉◯◉◉  ◯◯◯◯◉◯◯
◯◯◯◯◉◉◉  ◯◯◉◉◯◯◯  ◯◉◯◯◯◯◯
◯◯◯◯◯◉◉  ◯◯◉◉◉◯◯  ◯◯◯◯◯◯◯
◯◯◯◉◯◯◉  ◯◯◯◯◯◉◯  ◯◯◯◯◉◯◯
Rewards() = [1.0, -1.0]
Returns() = [1.0, -1.0]
