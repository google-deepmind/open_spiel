game: python_mfg_dynamic_routing

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.MEAN_FIELD
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Python Mean Field Routing Game"
GameType.max_num_players = 1
GameType.min_num_players = 1
GameType.parameter_specification = ["max_num_time_step", "players", "time_step_length"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = True
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = True
GameType.reward_model = RewardModel.REWARDS
GameType.short_name = "python_mfg_dynamic_routing"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 8
PolicyTensorShape() = [8]
MaxChanceOutcomes() = 1
GetParameters() = {max_num_time_step=10,players=-1,time_step_length=0.5}
NumPlayers() = 1
MinUtility() = -11.0
MaxUtility() = 0.0
UtilitySum() = None
ObservationTensorShape() = location: [8], destination: [8], time: [11], waiting: [1]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 28
MaxGameLength() = 10
ToString() = "python_mfg_dynamic_routing(max_num_time_step=10,players=-1,time_step_length=0.5)"

# State 0
# Before initial chance node
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = ""
ObservationString(0) = "Before initial chance node"
ObservationTensor(0).location: ◉◯◯◯◯◯◯◯
ObservationTensor(0).destination: ◉◯◯◯◯◯◯◯
ObservationTensor(0).time: ◉◯◯◯◯◯◯◯◯◯◯
ObservationTensor(0).waiting: ◯
ChanceOutcomes() = [(0,1)]
LegalActions() = [0]
StringLegalActions() = ["Vehicle is assigned to population 0."]

# Apply action "Vehicle is assigned to population 0."
action: 0

# State 1
# Location=O->A, waiting_time=0, t=0, destination='D->E'
IsTerminal() = False
History() = [0]
HistoryString() = "0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "0"
ObservationString(0) = "Location=O->A, waiting_time=0, t=0, destination='D->E'"
ObservationTensor(0).location: ◯◯◯◯◯◯◯◉
ObservationTensor(0).destination: ◯◯◯◯◯◯◉◯
ObservationTensor(0).time: ◉◯◯◯◯◯◯◯◯◯◯
ObservationTensor(0).waiting: ◯
Rewards() = [0]
Returns() = [0]
LegalActions() = [1, 2]
StringLegalActions() = ["Vehicle 0 would like to move to A->B.", "Vehicle 0 would like to move to A->C."]

# Apply action "Vehicle 0 would like to move to A->C."
action: 2

# State 2
# Location=A->C, waiting_time=-1, t=1_mean_field, destination='D->E'
IsTerminal() = False
History() = [0, 2]
HistoryString() = "0, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.MEAN_FIELD
InformationStateString(0) = "0, 2"
ObservationString(0) = "Location=A->C, waiting_time=-1, t=1_mean_field, destination='D->E'"
ObservationTensor(0).location: ◯◯◉◯◯◯◯◯
ObservationTensor(0).destination: ◯◯◯◯◯◯◉◯
ObservationTensor(0).time: ◯◉◯◯◯◯◯◯◯◯◯
ObservationTensor(0).waiting: ◯
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ["Location=A->C, waiting_time=-1, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=0, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=1, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=2, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=3, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=4, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=5, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=6, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=7, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=8, t=1_mean_field, destination='D->E'", "Location=A->C, waiting_time=9, t=1_mean_field, destination='D->E'"]

# Set mean field distribution to be uniform
action: update_distribution

# State 3
# Location=A->C, waiting_time=3, t=1, destination='D->E'
IsTerminal() = False
History() = [0, 2]
HistoryString() = "0, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "0, 2"
ObservationString(0) = "Location=A->C, waiting_time=3, t=1, destination='D->E'"
ObservationTensor(0).location: ◯◯◉◯◯◯◯◯
ObservationTensor(0).destination: ◯◯◯◯◯◯◉◯
ObservationTensor(0).time: ◯◉◯◯◯◯◯◯◯◯◯
ObservationTensor(0).waiting: ◉
Rewards() = [0]
Returns() = [0]
LegalActions() = [0]
StringLegalActions() = ["Vehicle 0 reach a sink node or its destination."]

# Apply action "Vehicle 0 reach a sink node or its destination."
action: 0

# State 4
# Location=A->C, waiting_time=2, t=2_mean_field, destination='D->E'
IsTerminal() = False
History() = [0, 2, 0]
HistoryString() = "0, 2, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.MEAN_FIELD
InformationStateString(0) = "0, 2, 0"
ObservationString(0) = "Location=A->C, waiting_time=2, t=2_mean_field, destination='D->E'"
ObservationTensor(0).location: ◯◯◉◯◯◯◯◯
ObservationTensor(0).destination: ◯◯◯◯◯◯◉◯
ObservationTensor(0).time: ◯◯◉◯◯◯◯◯◯◯◯
ObservationTensor(0).waiting: ◉
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ["Location=A->C, waiting_time=-1, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=0, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=1, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=2, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=3, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=4, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=5, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=6, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=7, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=8, t=2_mean_field, destination='D->E'", "Location=A->C, waiting_time=9, t=2_mean_field, destination='D->E'"]

# Set mean field distribution to be uniform
action: update_distribution

# State 5
# Location=A->C, waiting_time=2, t=2, destination='D->E'
IsTerminal() = False
History() = [0, 2, 0]
HistoryString() = "0, 2, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "0, 2, 0"
ObservationString(0) = "Location=A->C, waiting_time=2, t=2, destination='D->E'"
ObservationTensor(0).location: ◯◯◉◯◯◯◯◯
ObservationTensor(0).destination: ◯◯◯◯◯◯◉◯
ObservationTensor(0).time: ◯◯◉◯◯◯◯◯◯◯◯
ObservationTensor(0).waiting: ◉
Rewards() = [0]
Returns() = [0]
LegalActions() = [0]
StringLegalActions() = ["Vehicle 0 reach a sink node or its destination."]

# Apply action "Vehicle 0 reach a sink node or its destination."
action: 0

# State 6
# Location=A->C, waiting_time=1, t=3_mean_field, destination='D->E'
IsTerminal() = False
History() = [0, 2, 0, 0]
HistoryString() = "0, 2, 0, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.MEAN_FIELD
InformationStateString(0) = "0, 2, 0, 0"
ObservationString(0) = "Location=A->C, waiting_time=1, t=3_mean_field, destination='D->E'"
ObservationTensor(0).location: ◯◯◉◯◯◯◯◯
ObservationTensor(0).destination: ◯◯◯◯◯◯◉◯
ObservationTensor(0).time: ◯◯◯◉◯◯◯◯◯◯◯
ObservationTensor(0).waiting: ◉
Rewards() = [0]
Returns() = [0]
DistributionSupport() = ["Location=A->C, waiting_time=-1, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=0, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=1, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=2, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=3, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=4, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=5, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=6, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=7, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=8, t=3_mean_field, destination='D->E'", "Location=A->C, waiting_time=9, t=3_mean_field, destination='D->E'"]

# Set mean field distribution to be uniform
action: update_distribution

# State 7
# Apply action "Vehicle 0 reach a sink node or its destination."
action: 0

# State 8
# Set mean field distribution to be uniform
action: update_distribution

# State 9
# Apply action "Vehicle 0 would like to move to C->D."
action: 5

# State 10
# Set mean field distribution to be uniform
action: update_distribution

# State 11
# Apply action "Vehicle 0 reach a sink node or its destination."
action: 0

# State 12
# Set mean field distribution to be uniform
action: update_distribution

# State 13
# Apply action "Vehicle 0 reach a sink node or its destination."
action: 0

# State 14
# Set mean field distribution to be uniform
action: update_distribution

# State 15
# Apply action "Vehicle 0 reach a sink node or its destination."
action: 0

# State 16
# Set mean field distribution to be uniform
action: update_distribution

# State 17
# Apply action "Vehicle 0 would like to move to D->E."
action: 6

# State 18
# Set mean field distribution to be uniform
action: update_distribution

# State 19
# Apply action "Vehicle 0 reach a sink node or its destination."
action: 0

# State 20
# Arrived at D->E, with arrival time 8, t=10_mean_field
IsTerminal() = True
History() = [0, 2, 0, 0, 0, 5, 0, 0, 0, 6, 0]
HistoryString() = "0, 2, 0, 0, 0, 5, 0, 0, 0, 6, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.TERMINAL
InformationStateString(0) = "0, 2, 0, 0, 0, 5, 0, 0, 0, 6, 0"
ObservationString(0) = "Arrived at D->E, with arrival time 8, t=10_mean_field"
ObservationTensor(0).location: ◯◯◯◯◯◯◉◯
ObservationTensor(0).destination: ◯◯◯◯◯◯◉◯
ObservationTensor(0).time: ◯◯◯◯◯◯◯◯◯◯◉
ObservationTensor(0).waiting: ◯
Rewards() = [-4]
Returns() = [-4]
