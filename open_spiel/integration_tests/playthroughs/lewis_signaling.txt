game: lewis_signaling

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.IMPERFECT_INFORMATION
GameType.long_name = "Lewis Signaling Game"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = ["num_messages", "num_states", "payoffs"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = True
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "lewis_signaling"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 3
PolicyTensorShape() = [3]
MaxChanceOutcomes() = 3
GetParameters() = {num_messages=3,num_states=3,payoffs=1, 0, 0, 0, 1, 0, 0, 0, 1}
NumPlayers() = 2
MinUtility() = 0.0
MaxUtility() = 1.0
UtilitySum() = None
InformationStateTensorShape() = [6]
InformationStateTensorLayout() = TensorLayout.CHW
InformationStateTensorSize() = 6
ObservationTensorShape() = [6]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 6
MaxGameLength() = 2
ToString() = "lewis_signaling()"

# State 0
# Initial chance node
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
InformationStateString(0) = "ChanceNode -- no observation"
InformationStateString(1) = "ChanceNode -- no observation"
InformationStateTensor(0): ◯◯◯◯◯◯
InformationStateTensor(1): ◯◯◯◯◯◯
ObservationString(0) = "ChanceNode -- no observation"
ObservationString(1) = "ChanceNode -- no observation"
ObservationTensor(0): ◯◯◯◯◯◯
ObservationTensor(1): ◯◯◯◯◯◯
ChanceOutcomes() = [(0, 0.3333333333333333), (1, 0.3333333333333333), (2, 0.3333333333333333)]
LegalActions() = [0, 1, 2]
StringLegalActions() = ["State 0", "State 1", "State 2"]

# Apply action "State 0"
action: 0

# State 1
# State 0
IsTerminal() = False
History() = [0]
HistoryString() = "0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "Current turn: 0\nState: 0\n"
InformationStateString(1) = "Current turn: 0\nMessage: -1\n"
InformationStateTensor(0): ◉◯◯◉◯◯
InformationStateTensor(1): ◉◯◯◯◯◯
ObservationString(0) = "Current turn: 0\nState: 0\n"
ObservationString(1) = "Current turn: 0\nMessage: -1\n"
ObservationTensor(0): ◉◯◯◉◯◯
ObservationTensor(1): ◉◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2]
StringLegalActions() = ["Message 0", "Message 1", "Message 2"]

# Apply action "Message 1"
action: 1

# State 2
# State 0, Message 1
IsTerminal() = False
History() = [0, 1]
HistoryString() = "0, 1"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "Current turn: 1\nState: 0\n"
InformationStateString(1) = "Current turn: 1\nMessage: 1\n"
InformationStateTensor(0): ◯◉◯◉◯◯
InformationStateTensor(1): ◯◉◯◯◉◯
ObservationString(0) = "Current turn: 1\nState: 0\n"
ObservationString(1) = "Current turn: 1\nMessage: 1\n"
ObservationTensor(0): ◯◉◯◉◯◯
ObservationTensor(1): ◯◉◯◯◉◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [0, 1, 2]
StringLegalActions() = ["Action 0", "Action 1", "Action 2"]

# Apply action "Action 0"
action: 0

# State 3
# State 0, Message 1, Action 0
IsTerminal() = True
History() = [0, 1, 0]
HistoryString() = "0, 1, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "Current turn: 1\nState: 0\n"
InformationStateString(1) = "Current turn: 1\nMessage: 1\n"
InformationStateTensor(0): ◯◉◉◉◯◯
InformationStateTensor(1): ◯◉◉◯◉◯
ObservationString(0) = "Current turn: 1\nState: 0\n"
ObservationString(1) = "Current turn: 1\nMessage: 1\n"
ObservationTensor(0): ◯◉◉◉◯◯
ObservationTensor(1): ◯◉◉◯◉◯
Rewards() = [1.0, 1.0]
Returns() = [1.0, 1.0]
