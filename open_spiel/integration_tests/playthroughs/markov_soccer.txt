game: markov_soccer(horizon=20)

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.SIMULTANEOUS
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Markov Soccer"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = ["grid", "horizon"]
GameType.provides_information_state_string = False
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "markov_soccer"
GameType.utility = Utility.ZERO_SUM

NumDistinctActions() = 5
PolicyTensorShape() = [5]
MaxChanceOutcomes() = 4
GetParameters() = {grid=.....\n..OB.\n.AO..\n.....,horizon=20}
NumPlayers() = 2
MinUtility() = -1.0
MaxUtility() = 1.0
UtilitySum() = 0.0
ObservationTensorShape() = [6, 4, 5]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 120
MaxGameLength() = 20
ToString() = "markov_soccer(horizon=20)"

# State 0
# .....
# ...b.
# .a...
# .....
# Chance Node
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
ObservationString(0) = ".....\n...b.\n.a...\n.....\nChance Node"
ObservationString(1) = ".....\n...b.\n.a...\n.....\nChance Node"
ObservationTensor(0):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◉◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◯◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◉◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◯◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
ChanceOutcomes() = [(2,0.5), (3,0.5)]
LegalActions() = [2, 3]
StringLegalActions() = ["(ball at 1,2)", "(ball at 2,2)"]

# Apply action "(ball at 2,2)"
action: 3

# State 1
# .....
# ...b.
# .aO..
# .....
IsTerminal() = False
History() = [3]
HistoryString() = "3"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = -2
ObservationString(0) = ".....\n...b.\n.aO..\n.....\n"
ObservationString(1) = ".....\n...b.\n.aO..\n.....\n"
ObservationTensor(0):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◉◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◉◯◯  ◉◯◯◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◉◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◉◯◯  ◉◯◯◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions(0) = [0, 1, 2, 3, 4]
LegalActions(1) = [0, 1, 2, 3, 4]
StringLegalActions(0) = ["up", "down", "left", "right", "stand"]
StringLegalActions(1) = ["up", "down", "left", "right", "stand"]

# Apply joint action ["right", "up"]
actions: [3, 0]

# State 2
# .....
# ...b.
# .aO..
# .....
# Chance Node
IsTerminal() = False
History() = [3, 3, 0]
HistoryString() = "3, 3, 0"
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = -1
ObservationString(0) = ".....\n...b.\n.aO..\n.....\nChance Node"
ObservationString(1) = ".....\n...b.\n.aO..\n.....\nChance Node"
ObservationTensor(0):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◉◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◉◯◯  ◉◯◯◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◉◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◉◯◯  ◉◯◯◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
ChanceOutcomes() = [(0,0.5), (1,0.5)]
LegalActions() = [0, 1]
StringLegalActions() = ["(A's action first)", "(B's action first)"]

# Apply action "(B's action first)"
action: 1

# State 3
# ...b.
# .....
# ..A..
# .....
IsTerminal() = False
History() = [3, 3, 0, 1]
HistoryString() = "3, 3, 0, 1"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = -2
ObservationString(0) = "...b.\n.....\n..A..\n.....\n"
ObservationString(1) = "...b.\n.....\n..A..\n.....\n"
ObservationTensor(0):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◉◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◯◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◉◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◯◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions(0) = [0, 1, 2, 3, 4]
LegalActions(1) = [0, 1, 2, 3, 4]
StringLegalActions(0) = ["up", "down", "left", "right", "stand"]
StringLegalActions(1) = ["up", "down", "left", "right", "stand"]

# Apply joint action ["down", "stand"]
actions: [1, 4]

# State 4
# Apply action "(B's action first)"
action: 1

# State 5
# ...b.
# .....
# .....
# ..A..
IsTerminal() = False
History() = [3, 3, 0, 1, 1, 4, 1]
HistoryString() = "3, 3, 0, 1, 1, 4, 1"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = -2
ObservationString(0) = "...b.\n.....\n.....\n..A..\n"
ObservationString(1) = "...b.\n.....\n.....\n..A..\n"
ObservationTensor(0):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◉◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◯◉◉
ObservationTensor(1):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◉◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◯◉◉
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions(0) = [0, 1, 2, 3, 4]
LegalActions(1) = [0, 1, 2, 3, 4]
StringLegalActions(0) = ["up", "down", "left", "right", "stand"]
StringLegalActions(1) = ["up", "down", "left", "right", "stand"]

# Apply joint action ["down", "up"]
actions: [1, 0]

# State 6
# Apply action "(B's action first)"
action: 1

# State 7
# Apply joint action ["right", "left"]
actions: [3, 2]

# State 8
# Apply action "(A's action first)"
action: 0

# State 9
# Apply joint action ["down", "left"]
actions: [1, 2]

# State 10
# Apply action "(B's action first)"
action: 1

# State 11
# Apply joint action ["down", "right"]
actions: [1, 3]

# State 12
# Apply action "(A's action first)"
action: 0

# State 13
# Apply joint action ["stand", "stand"]
actions: [4, 4]

# State 14
# Apply action "(A's action first)"
action: 0

# State 15
# Apply joint action ["right", "down"]
actions: [3, 1]

# State 16
# Apply action "(A's action first)"
action: 0

# State 17
# Apply joint action ["up", "stand"]
actions: [0, 4]

# State 18
# Apply action "(A's action first)"
action: 0

# State 19
# Apply joint action ["up", "right"]
actions: [0, 3]

# State 20
# Apply action "(B's action first)"
action: 1

# State 21
# .....
# ...bA
# .....
# .....
IsTerminal() = False
History() = [3, 3, 0, 1, 1, 4, 1, 1, 0, 1, 3, 2, 0, 1, 2, 1, 1, 3, 0, 4, 4, 0, 3, 1, 0, 0, 4, 0, 0, 3, 1]
HistoryString() = "3, 3, 0, 1, 1, 4, 1, 1, 0, 1, 3, 2, 0, 1, 2, 1, 1, 3, 0, 4, 4, 0, 3, 1, 0, 0, 4, 0, 0, 3, 1"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = -2
ObservationString(0) = ".....\n...bA\n.....\n.....\n"
ObservationString(1) = ".....\n...bA\n.....\n.....\n"
ObservationTensor(0):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◉  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◯
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◉  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◯
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions(0) = [0, 1, 2, 3, 4]
LegalActions(1) = [0, 1, 2, 3, 4]
StringLegalActions(0) = ["up", "down", "left", "right", "stand"]
StringLegalActions(1) = ["up", "down", "left", "right", "stand"]

# Apply joint action ["right", "down"]
actions: [3, 1]

# State 22
# Apply action "(A's action first)"
action: 0

# State 23
# .....
# .....
# ...b.
# .....
IsTerminal() = True
History() = [3, 3, 0, 1, 1, 4, 1, 1, 0, 1, 3, 2, 0, 1, 2, 1, 1, 3, 0, 4, 4, 0, 3, 1, 0, 0, 4, 0, 0, 3, 1, 3, 1, 0]
HistoryString() = "3, 3, 0, 1, 1, 4, 1, 1, 0, 1, 3, 2, 0, 1, 2, 1, 1, 3, 0, 4, 4, 0, 3, 1, 0, 0, 4, 0, 0, 3, 1, 3, 1, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
ObservationString(0) = ".....\n.....\n...b.\n.....\n"
ObservationString(1) = ".....\n.....\n...b.\n.....\n"
ObservationTensor(0):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◉◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◯◉
◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◯◯◯◯◯  ◉◉◉◉◉
Rewards() = [1, -1]
Returns() = [1, -1]
