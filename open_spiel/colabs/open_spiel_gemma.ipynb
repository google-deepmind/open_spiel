{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [
        {
          "file_id": "https://github.com/deepmind/open_spiel/blob/master/open_spiel/colabs/install_open_spiel.ipynb",
          "timestamp": 1766151365227
        },
        {
          "file_id": "install_open_spiel.ipynb",
          "timestamp": 1629100659918
        }
      ],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "odj1Coq5H080",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1766163255317,
          "user_tz": 210,
          "elapsed": 3,
          "user": {
            "displayName": "Marc Lanctot",
            "userId": "07231238210225196049"
          }
        }
      },
      "source": [
        "#@title ##### License { display-mode: \"form\" }\n",
        "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOzDGYAZcW3"
      },
      "source": [
        "# Fine-tuning a Gemma model with Kauldron\n",
        "\n",
        "* This Colab gets you started with fine-tuning language models for game-playing.\n",
        "* Specifically, it generates a dataset of actions recommended by an MCTS bot.\n",
        "* MCTS is short for [Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search).\n",
        "* OpenSpiel is a framework for reinforcement learning in games.\n",
        "* Gemma is an open language model.\n",
        "* Kauldron is an open training library.\n",
        "* We strongly suggest using a TPU runtime. Otherwise, wait times are too high. TPU v6e1 or above is recommended)\n",
        "* This example has been adapted from [the Gemma GitHub page](https://github.com/google-deepmind/gemma).\n",
        "* For more training examples, see the [Kauldron GitHub page](https://github.com/google-research/kauldron)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6kQBzWahEF"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MuGWqDdI51FP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2_Vbijh4FlZ"
      },
      "source": [
        "Install OpenSpiel, Gemma, and Kauldron via pip:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQc12Xrn4CXU"
      },
      "source": [
        "!pip install --upgrade open_spiel gemma kauldron\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewMXCaUw8d9Q"
      },
      "source": [
        "# @title Imports\n",
        "\n",
        "import etils\n",
        "from etils import ecolab\n",
        "from etils import epy\n",
        "import grain.python as pygrain\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import optax\n",
        "import pyspiel\n",
        "import sys\n",
        "import tqdm\n",
        "import treescope\n",
        "\n",
        "from open_spiel.python.algorithms import mcts\n",
        "\n",
        "# with ecolab.adhoc():\n",
        "with etils.epy.lazy_imports():\n",
        "  from kauldron import kd\n",
        "  from gemma import gm\n",
        "\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create a tokenizer\n",
        "\n",
        "tokenizer = gm.text.Gemma3Tokenizer()\n",
        "tokenizer.encode('This is an example sentence', add_bos=True)"
      ],
      "metadata": {
        "id": "4JhQNCs24NWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create a model and sampler\n",
        "\n",
        "# This colab by default uses 1B.\n",
        "# Other options: 270M and 4B\n",
        "\n",
        "model = gm.nn.Gemma3_1B(\n",
        "    tokens=\"batch.input\",\n",
        ")\n",
        "\n",
        "params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA3_1B_IT)\n",
        "\n",
        "sampler = gm.text.ChatSampler(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    multi_turn=False,\n",
        "    print_stream=True,  # Print output as it is generated.\n",
        ")\n"
      ],
      "metadata": {
        "id": "Wel3GHbS4cbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test the sampler\n",
        "\n",
        "# Will take some time on first invocation. If it takes over a minute on a TPU\n",
        "# runtime, something is probably wrong.\n",
        "output = sampler.chat(\"Is a tomato a vegetable or a fruit?\")\n"
      ],
      "metadata": {
        "id": "m-Ok7Nw9LN9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the fine-tuning dataset via self-play MCTS (2-3 minutes)\n",
        "\n",
        "NUM_EPISODES = 1000\n",
        "MCTS_SIMS_PER_DECISION = 1000\n",
        "\n",
        "game = pyspiel.load_game('tic_tac_toe')\n",
        "mcts_bot = mcts.MCTSBot(game=game, uct_c=(2.0 * math.sqrt(2)),\n",
        "                        max_simulations=MCTS_SIMS_PER_DECISION,\n",
        "                        evaluator=mcts.RandomRolloutEvaluator())\n",
        "\n",
        "num_distinct_actions = game.num_distinct_actions()\n",
        "all_actions = np.arange(num_distinct_actions)\n",
        "\n",
        "# Use an epsilon-greedy exploration sampling strategy to ensure that we\n",
        "# sufficiently cover the space.\n",
        "epsilon = 0.1\n",
        "dataset = []\n",
        "\n",
        "def player_mark(player: int):\n",
        "  return \"x\" if player == 0 else \"o\"\n",
        "\n",
        "def expand_state_str(state_str: str):\n",
        "  # Do this so that multiple marks are not grouped into a single token.\n",
        "  state_str = state_str.replace(\"x\", \" x\")\n",
        "  state_str = state_str.replace(\"o\", \" o\")\n",
        "  state_str = state_str.replace(\".\", \" .\")\n",
        "  return state_str\n",
        "\n",
        "print(f\"Generating data using {NUM_EPISODES} episodes of self-play MCTS...\")\n",
        "for ep in tqdm.tqdm(range(NUM_EPISODES)):\n",
        "  state = game.new_initial_state()\n",
        "  while not state.is_terminal():\n",
        "    player = state.current_player()\n",
        "    # Construct an epsilon-greedy policy\n",
        "    greedy_policy = np.zeros(num_distinct_actions, dtype=float)\n",
        "    uniform_policy = np.zeros(num_distinct_actions, dtype=float)\n",
        "    legal_actions = state.legal_actions()\n",
        "    uniform_policy[legal_actions] = 1.0\n",
        "    uniform_policy /= len(legal_actions)\n",
        "    greedy_action = mcts_bot.step(state)\n",
        "    dataset.append({\n",
        "        \"state_str\": expand_state_str(str(state)),\n",
        "        \"action\": greedy_action,\n",
        "        \"action_str\": state.action_to_string(greedy_action),\n",
        "        \"player_mark\": player_mark(player),\n",
        "        \"legal_actions\": legal_actions,\n",
        "    })\n",
        "    greedy_policy[greedy_action] = 1.0\n",
        "    epsilon_greedy_policy = epsilon * uniform_policy + (1 - epsilon) *  greedy_policy\n",
        "    action_to_take = np.random.choice(all_actions, p=epsilon_greedy_policy)\n",
        "    assert action_to_take in legal_actions\n",
        "    state.apply_action(action_to_take)\n"
      ],
      "metadata": {
        "id": "pO0vcdbf4qL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save dataset as JSON to disk\n",
        "DATASET_FILE = \"/tmp/dataset.json\"\n",
        "with open(DATASET_FILE, 'w') as f:\n",
        "  f.write(json.dumps(dataset))\n",
        "  print(f\"Successfully saved {DATASET_FILE}\")"
      ],
      "metadata": {
        "id": "i1Rq5Y9YJtnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prompts and transform\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\\\n",
        "You are playing a game of Tic-Tac-Toe. The current state is:\n",
        "\n",
        "{state_str}\n",
        "\n",
        "You need to give your move in the following format: mark(row,col)\n",
        "Where mark is either \"x\" or \"o\" and row and col coordinates are 0-indexed.\n",
        "\n",
        "You are playing as player {player_mark!r}.\n",
        "What is your next move?\n",
        "Respond with: <move>YOUR_MOVE</move>\n",
        "\"\"\"\n",
        "\n",
        "def make_prompt(state: pyspiel.State) -> str:\n",
        "  return PROMPT_TEMPLATE.format(state_str=expand_state_str(str(state)),\n",
        "                                player_mark=player_mark(state.current_player()))\n",
        "\n",
        "class PromptTransform(pygrain.MapTransform):\n",
        "\n",
        "  def __init__(self, prompt_template: str):\n",
        "    self._prompt_template = prompt_template\n",
        "\n",
        "  def map(self, element):\n",
        "    \"\"\"Map a single element.\"\"\"\n",
        "    formatted_element = {}\n",
        "    formatted_element['prompt'] = self._prompt_template.format(**element)\n",
        "    formatted_element['response'] = f'<move>{element[\"action_str\"]}</move>'\n",
        "    return formatted_element\n"
      ],
      "metadata": {
        "id": "GHXaqrJbMAQJ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1766163519891,
          "user_tz": 210,
          "elapsed": 2,
          "user": {
            "displayName": "Marc Lanctot",
            "userId": "07231238210225196049"
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the Kauldron dataset and trainer\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "SEED = 42\n",
        "\n",
        "# Note! Checkpoints will be saved in here. So, if you change something and\n",
        "# restart training at some later point, it may start not start at training step\n",
        "# 0. To start training over, you need to clear this workdir directory.\n",
        "WORKDIR = \"/tmp/kauldron\"\n",
        "\n",
        "TRAINING_STEPS = 500\n",
        "\n",
        "kd_dataset = kd.data.py.Json(\n",
        "    path=DATASET_FILE,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    transforms=[\n",
        "        PromptTransform(\n",
        "            prompt_template=PROMPT_TEMPLATE\n",
        "        ),\n",
        "        gm.data.Seq2SeqTask(\n",
        "            in_prompt='prompt',\n",
        "            in_response='response',\n",
        "            out_input='input',  # Tokenized input.\n",
        "            out_target='target',  # Tokenized target.\n",
        "            out_target_mask='loss_mask',\n",
        "            tokenizer=tokenizer,\n",
        "            # Padding parameters\n",
        "            max_length=512,  # In number of tokens.\n",
        "            truncate=True,\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "trainer = kd.train.Trainer(\n",
        "    seed=SEED,\n",
        "    workdir=WORKDIR,\n",
        "    train_ds=kd_dataset,\n",
        "    model=model,\n",
        "    init_transform=gm.ckpts.LoadCheckpoint(\n",
        "        path=gm.ckpts.CheckpointPath.GEMMA3_1B_IT,\n",
        "    ),\n",
        "    num_train_steps=TRAINING_STEPS,\n",
        "    train_losses={\n",
        "        'xentropy': kd.losses.SoftmaxCrossEntropyWithIntLabels(\n",
        "            logits='preds.logits',\n",
        "            labels='batch.target',\n",
        "            mask='batch.loss_mask',\n",
        "        ),\n",
        "    },\n",
        "    optimizer=optax.adafactor(learning_rate=1e-3),\n",
        "    checkpointer=kd.ckpts.Checkpointer(\n",
        "        save_interval_steps=100,\n",
        "        max_to_keep=2,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Q56YSQ62M_ID",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1766163519914,
          "user_tz": 210,
          "elapsed": 22,
          "user": {
            "displayName": "Marc Lanctot",
            "userId": "07231238210225196049"
          }
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run the training\n",
        "\n",
        "# Takes about 20min on a TPU v6e1.\n",
        "\n",
        "# Note, first few steps take quite long but subsequent steps go faster.\n",
        "# Also note that the progress bar does not alwats update at every iteration,\n",
        "# so may appear stuck after the first step (and update again at 10%).\n",
        "# There is also a slowdown multiples of 100 iterations for saving of\n",
        "# checkpoints, but the first one is also the longest.\n",
        "\n",
        "trainer_state, aux = trainer.train()"
      ],
      "metadata": {
        "id": "PsYtjU66OOdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Make a new sampler from the trained params\n",
        "\n",
        "# Use nonzero temperature to get some variation\n",
        "TEMPERATURE = 1.0\n",
        "\n",
        "sampler = gm.text.ChatSampler(\n",
        "    model=model,\n",
        "    params=trainer_state.params,\n",
        "    sampling=gm.text.RandomSampling(temperature=TEMPERATURE),\n",
        "    multi_turn=False,\n",
        "    print_stream=False,\n",
        ")"
      ],
      "metadata": {
        "id": "UqlFNlkvaIJr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1766164644027,
          "user_tz": 210,
          "elapsed": 22,
          "user": {
            "displayName": "Marc Lanctot",
            "userId": "07231238210225196049"
          }
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Play an episode with the fine-tuned model\n",
        "\n",
        "import re\n",
        "\n",
        "def parse_action(state: pyspiel.State,\n",
        "                 response: str) -> int:\n",
        "  pattern = r\"<move>.*</move>\"\n",
        "  legal_actions = state.legal_actions()\n",
        "  match = re.search(pattern, response)\n",
        "  if match:\n",
        "    action_str = match.group().strip()\n",
        "    action_str = action_str.replace(\"<move>\", \"\")\n",
        "    action_str = action_str.replace(\"</move>\", \"\")\n",
        "    print(f\"Found action_str = {action_str}\")\n",
        "  else:\n",
        "    print(f\"Incorrect format: {response}, returning uniform legal\")\n",
        "    return np.random.choice(legal_actions)\n",
        "  for action in legal_actions:\n",
        "    if state.action_to_string(action) == action_str:\n",
        "      print(f\"Returning corresponding action: {action}\")\n",
        "      return action\n",
        "  print(\"Action not found in legal actions, returning a random legal action\")\n",
        "  return np.random.choice(legal_actions)\n",
        "\n",
        "\n",
        "state = game.new_initial_state()\n",
        "\n",
        "while not state.is_terminal():\n",
        "  print(\"\")\n",
        "  print(state)\n",
        "  prompt = make_prompt(state)\n",
        "  print(\"Generating move...\")\n",
        "  output = sampler.chat(prompt)\n",
        "  print(f\"Response: {output}\")\n",
        "  action = parse_action(state, output)\n",
        "  print(f\"Playing action: {state.action_to_string(action)}\")\n",
        "  state.apply_action(action)\n",
        "\n"
      ],
      "metadata": {
        "id": "r8BjIkiSUO6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
