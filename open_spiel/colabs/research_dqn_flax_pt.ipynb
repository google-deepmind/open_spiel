{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade open_spiel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.pytorch import dqn as dqn_pt\n",
    "from open_spiel.python.jax import dqn\n",
    "from open_spiel.python.algorithms import random_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n",
    "  \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = random_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = env.reset()\n",
    "      episode_rewards = 0\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        if env.is_turn_based:\n",
    "          agent_output = cur_agents[player_id].step(\n",
    "              time_step, is_evaluation=True)\n",
    "          action_list = [agent_output.action]\n",
    "        else:\n",
    "          agents_output = [\n",
    "              agent.step(time_step, is_evaluation=True) for agent in cur_agents\n",
    "          ]\n",
    "          action_list = [agent_output.action for agent_output in agents_output]\n",
    "        time_step = env.step(action_list)\n",
    "        episode_rewards += time_step.rewards[player_pos]\n",
    "      sum_episode_rewards[player_pos] += episode_rewards\n",
    "  return sum_episode_rewards / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_main(\n",
    "  game,\n",
    "  config,\n",
    "  checkpoint_dir,\n",
    "  num_train_episodes,\n",
    "  eval_every,\n",
    "  hidden_layers_sizes,\n",
    "  replay_buffer_capacity,\n",
    "  batch_size\n",
    "):\n",
    "  num_players = 2\n",
    "\n",
    "  env = rl_environment.Environment(game, **config)\n",
    "  info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "  num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "  # random agents for evaluation\n",
    "  random_agents = [\n",
    "      random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "      for idx in range(num_players)\n",
    "  ]\n",
    "\n",
    "\n",
    "  hidden_layers_sizes = [int(ls) for ls in hidden_layers_sizes]\n",
    "  # pylint: disable=g-complex-comprehension\n",
    "  agents = [\n",
    "    dqn_pt.DQN(\n",
    "      player_id=idx,\n",
    "      state_representation_size=info_state_size,\n",
    "      num_actions=num_actions,\n",
    "      hidden_layers_sizes=hidden_layers_sizes,\n",
    "      replay_buffer_capacity=replay_buffer_capacity,\n",
    "      batch_size=batch_size) for idx in range(num_players)\n",
    "  ]\n",
    "  result = []\n",
    "  for ep in range(num_train_episodes):\n",
    "    if (ep + 1) % eval_every == 0:\n",
    "      r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n",
    "      result.append(r_mean)\n",
    "      print(\"[%s] Mean episode rewards %s\" %(ep + 1, r_mean))\n",
    "\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      if env.is_turn_based:\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        action_list = [agent_output.action]\n",
    "      else:\n",
    "        agents_output = [agent.step(time_step) for agent in agents]\n",
    "        action_list = [agent_output.action for agent_output in agents_output]\n",
    "      time_step = env.step(action_list)\n",
    "\n",
    "    # Episode is over, step all agents with final info state.\n",
    "    for agent in agents:\n",
    "      agent.step(time_step)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_main(\n",
    "  game,\n",
    "  config,\n",
    "  checkpoint_dir,\n",
    "  num_train_episodes,\n",
    "  eval_every,\n",
    "  hidden_layers_sizes,\n",
    "  replay_buffer_capacity,\n",
    "  batch_size\n",
    "):\n",
    "  num_players = 2\n",
    "\n",
    "  env = rl_environment.Environment(game, **config)\n",
    "  info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "  num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "  # random agents for evaluation\n",
    "  random_agents = [\n",
    "      random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "      for idx in range(num_players)\n",
    "  ]\n",
    "\n",
    "  hidden_layers_sizes = [int(ls) for ls in hidden_layers_sizes]\n",
    "  # pylint: disable=g-complex-comprehension\n",
    "  agents = [\n",
    "      dqn.DQN(\n",
    "        player_id=idx,\n",
    "        state_representation_size=info_state_size,\n",
    "        num_actions=num_actions,\n",
    "        hidden_layers_sizes=hidden_layers_sizes,\n",
    "        replay_buffer_capacity=replay_buffer_capacity,\n",
    "        batch_size=batch_size\n",
    "      ) for idx in range(num_players)\n",
    "  ]\n",
    "\n",
    "  result_jax = []\n",
    "  for ep in range(num_train_episodes):\n",
    "    if (ep + 1) % eval_every == 0:\n",
    "      r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n",
    "      result_jax.append(r_mean)\n",
    "      print(\"[%s] Mean episode rewards %s\" %(ep + 1, r_mean))\n",
    "\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      if env.is_turn_based:\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        action_list = [agent_output.action]\n",
    "      else:\n",
    "        agents_output = [agent.step(time_step) for agent in agents]\n",
    "        action_list = [agent_output.action for agent_output in agents_output]\n",
    "      time_step = env.step(action_list)\n",
    "\n",
    "    # Episode is over, step all agents with final info state.\n",
    "    for agent in agents:\n",
    "      agent.step(time_step)\n",
    "  return result_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/tmp/dqn_test\"\n",
    "num_train_episodes = 10000\n",
    "eval_every = 100\n",
    "\n",
    "hidden_layers_sizes = [64, 64]\n",
    "replay_buffer_capacity = int(1e5)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAKTHROUGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"breakthrough\"\n",
    "config = {\"columns\": 5, \"rows\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_result = pt_main(    \n",
    "    game,\n",
    "    config,\n",
    "    checkpoint_dir,\n",
    "    num_train_episodes,\n",
    "    eval_every,\n",
    "    hidden_layers_sizes,\n",
    "    replay_buffer_capacity,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ep = [x for x in range(len(pt_result))]\n",
    "pt_r_mean0 = [y[0] for y in pt_result]\n",
    "pt_r_mean1 = [y[1] for y in pt_result]\n",
    "\n",
    "plt.plot(ep,pt_r_mean0, c='red')\n",
    "plt.plot(ep,pt_r_mean1, c='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_jax = jax_main(\n",
    "    game,\n",
    "    config,\n",
    "    checkpoint_dir,\n",
    "    num_train_episodes,\n",
    "    eval_every,\n",
    "    hidden_layers_sizes,\n",
    "    replay_buffer_capacity,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = [x for x in range(len(result_jax))]\n",
    "jax_r_mean0 = [y[0] for y in result_jax]\n",
    "jax_r_mean1 = [y[1] for y in result_jax]\n",
    "\n",
    "plt.plot(ep, jax_r_mean0, c='red')\n",
    "plt.plot(ep, jax_r_mean1, c='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep, pt_r_mean0, c='skyblue')\n",
    "plt.plot(ep, pt_r_mean1, c='skyblue', linestyle='dashed')\n",
    "plt.plot(ep, jax_r_mean0, c='pink')\n",
    "plt.plot(ep, jax_r_mean1, c='pink', linestyle='dashed')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIC-TAC-TOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"tic_tac_toe\"\n",
    "config = {}\n",
    "num_train_episodes = 20000\n",
    "eval_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_result = pt_main(\n",
    "    game,\n",
    "    config,\n",
    "    checkpoint_dir,\n",
    "    num_train_episodes,\n",
    "    eval_every,\n",
    "    hidden_layers_sizes,\n",
    "    replay_buffer_capacity,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ep = [x for x in range(len(pt_result))]\n",
    "pt_r_mean0 = [y[0] for y in pt_result]\n",
    "pt_r_mean1 = [y[1] for y in pt_result]\n",
    "\n",
    "plt.plot(ep,pt_r_mean0, c='red')\n",
    "plt.plot(ep,pt_r_mean1, c='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_jax = jax_main(\n",
    "    game,\n",
    "    config,\n",
    "    checkpoint_dir,\n",
    "    num_train_episodes,\n",
    "    eval_every,\n",
    "    hidden_layers_sizes,\n",
    "    replay_buffer_capacity,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = [x for x in range(len(result_jax))]\n",
    "jax_r_mean0 = [y[0] for y in result_jax]\n",
    "jax_r_mean1 = [y[1] for y in result_jax]\n",
    "\n",
    "plt.plot(ep, jax_r_mean0, c='red')\n",
    "plt.plot(ep, jax_r_mean1, c='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep, pt_r_mean0, c='skyblue')\n",
    "plt.plot(ep, pt_r_mean1, c='skyblue', linestyle='dashed')\n",
    "plt.plot(ep, jax_r_mean0, c='pink')\n",
    "plt.plot(ep, jax_r_mean1, c='pink', linestyle='dashed')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_spiel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
