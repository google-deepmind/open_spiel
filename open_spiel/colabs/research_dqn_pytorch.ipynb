{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I MODIFIED THIS AND NOW IT DOESN'T WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pyspiel\n",
    "from open_spiel.python.games import block_dominoes\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.pytorch import dqn as dqn_pt\n",
    "from open_spiel.python.algorithms import dqn\n",
    "from open_spiel.python.algorithms import random_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered games:\n",
      "['2048', 'add_noise', 'amazons', 'backgammon', 'bargaining', 'battleship', 'blackjack', 'blotto', 'breakthrough', 'bridge', 'bridge_uncontested_bidding', 'catch', 'chat_game', 'checkers', 'chess', 'cliff_walking', 'clobber', 'coin_game', 'colored_trails', 'connect_four', 'coop_box_pushing', 'coop_to_1p', 'coordinated_mp', 'crazy_eights', 'cursor_go', 'dark_chess', 'dark_hex', 'dark_hex_ir', 'deep_sea', 'dots_and_boxes', 'dou_dizhu', 'efg_game', 'euchre', 'first_sealed_auction', 'gin_rummy', 'go', 'goofspiel', 'havannah', 'hearts', 'hex', 'kriegspiel', 'kuhn_poker', 'laser_tag', 'leduc_poker', 'lewis_signaling', 'liars_dice', 'liars_dice_ir', 'maedn', 'mancala', 'markov_soccer', 'matching_pennies_3p', 'matrix_bos', 'matrix_brps', 'matrix_cd', 'matrix_coordination', 'matrix_mp', 'matrix_pd', 'matrix_rps', 'matrix_rpsw', 'matrix_sh', 'matrix_shapleys_game', 'mfg_crowd_modelling', 'mfg_crowd_modelling_2d', 'mfg_dynamic_routing', 'mfg_garnet', 'misere', 'morpion_solitaire', 'negotiation', 'nfg_game', 'nim', 'nine_mens_morris', 'normal_form_extensive_game', 'oh_hell', 'oshi_zumo', 'othello', 'oware', 'pathfinding', 'pentago', 'phantom_go', 'phantom_ttt', 'phantom_ttt_ir', 'pig', 'python_block_dominoes', 'python_dynamic_routing', 'python_iterated_prisoners_dilemma', 'python_kuhn_poker', 'python_liars_poker', 'python_tic_tac_toe', 'quoridor', 'rbc', 'repeated_game', 'restricted_nash_response', 'sheriff', 'skat', 'solitaire', 'start_at', 'stones_and_gems', 'tarok', 'tic_tac_toe', 'tiny_bridge_2p', 'tiny_bridge_4p', 'tiny_hanabi', 'trade_comm', 'turn_based_simultaneous_game', 'ultimate_tic_tac_toe', 'y', 'zerosum']\n"
     ]
    }
   ],
   "source": [
    "games_list = pyspiel.registered_names()\n",
    "\n",
    "print(\"Registered games:\")\n",
    "print(games_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n",
    "  \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = random_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = env.reset()\n",
    "      episode_rewards = 0\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        if env.is_turn_based:\n",
    "          agent_output = cur_agents[player_id].step(\n",
    "              time_step, is_evaluation=True)\n",
    "          action_list = [agent_output.action]\n",
    "        else:\n",
    "          agents_output = [\n",
    "              agent.step(time_step, is_evaluation=True) for agent in cur_agents\n",
    "          ]\n",
    "          action_list = [agent_output.action for agent_output in agents_output]\n",
    "        time_step = env.step(action_list)\n",
    "        episode_rewards += time_step.rewards[player_pos]\n",
    "      sum_episode_rewards[player_pos] += episode_rewards\n",
    "  return sum_episode_rewards / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_main(game,\n",
    "            # config,\n",
    "            checkpoint_dir,\n",
    "            num_train_episodes,\n",
    "            eval_every,\n",
    "            hidden_layers_sizes,\n",
    "            replay_buffer_capacity,\n",
    "            batch_size):\n",
    "  num_players = 2\n",
    "\n",
    "  env = rl_environment.Environment(game) # rl_environment.Environment(game, **config)\n",
    "  info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "  num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "  # random agents for evaluation\n",
    "  random_agents = [\n",
    "      random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "      for idx in range(num_players)\n",
    "  ]\n",
    "\n",
    "\n",
    "  hidden_layers_sizes = [int(l) for l in hidden_layers_sizes]\n",
    "  # pylint: disable=g-complex-comprehension\n",
    "  agents = [\n",
    "      dqn_pt.DQN(\n",
    "          player_id=idx,\n",
    "          state_representation_size=info_state_size,\n",
    "          num_actions=num_actions,\n",
    "          hidden_layers_sizes=hidden_layers_sizes,\n",
    "          replay_buffer_capacity=replay_buffer_capacity,\n",
    "          batch_size=batch_size) for idx in range(num_players)\n",
    "  ]\n",
    "  result = []\n",
    "  for ep in range(num_train_episodes):\n",
    "    if (ep + 1) % eval_every == 0:\n",
    "      r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n",
    "      result.append(r_mean)\n",
    "      print(\"[%s] Mean episode rewards %s\" %(ep + 1, r_mean))\n",
    "\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      if env.is_turn_based:\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        action_list = [agent_output.action]\n",
    "      else:\n",
    "        agents_output = [agent.step(time_step) for agent in agents]\n",
    "        action_list = [agent_output.action for agent_output in agents_output]\n",
    "      time_step = env.step(action_list)\n",
    "\n",
    "    # Episode is over, step all agents with final info state.\n",
    "    for agent in agents:\n",
    "      agent.step(time_step)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_main(game,\n",
    "            # config,\n",
    "            checkpoint_dir,\n",
    "            num_train_episodes,\n",
    "            eval_every,\n",
    "            hidden_layers_sizes,\n",
    "            replay_buffer_capacity,batch_size):\n",
    "  num_players = 2\n",
    "\n",
    "  env = rl_environment.Environment(game)# rl_environment.Environment(game, **config)\n",
    "  info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "  num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "  # random agents for evaluation\n",
    "  random_agents = [\n",
    "      random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "      for idx in range(num_players)\n",
    "  ]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    hidden_layers_sizes = [int(l) for l in hidden_layers_sizes]\n",
    "    # pylint: disable=g-complex-comprehension\n",
    "    agents = [\n",
    "        dqn.DQN(\n",
    "            session=sess,\n",
    "            player_id=idx,\n",
    "            state_representation_size=info_state_size,\n",
    "            num_actions=num_actions,\n",
    "            hidden_layers_sizes=hidden_layers_sizes,\n",
    "            replay_buffer_capacity=replay_buffer_capacity,\n",
    "            batch_size=batch_size) for idx in range(num_players)\n",
    "    ]\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result_tf = []\n",
    "    for ep in range(num_train_episodes):\n",
    "      if (ep + 1) % eval_every == 0:\n",
    "        r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n",
    "        result_tf.append(r_mean)\n",
    "        print(\"[%s] Mean episode rewards %s\" %(ep + 1, r_mean))\n",
    "\n",
    "      time_step = env.reset()\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        if env.is_turn_based:\n",
    "          agent_output = agents[player_id].step(time_step)\n",
    "          action_list = [agent_output.action]\n",
    "        else:\n",
    "          agents_output = [agent.step(time_step) for agent in agents]\n",
    "          action_list = [agent_output.action for agent_output in agents_output]\n",
    "        time_step = env.step(action_list)\n",
    "\n",
    "      # Episode is over, step all agents with final info state.\n",
    "      for agent in agents:\n",
    "        agent.step(time_step)\n",
    "  return result_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/tmp/dqn_test\"\n",
    "num_train_episodes = 10000\n",
    "eval_every = 100\n",
    "\n",
    "hidden_layers_sizes = [64, 64]\n",
    "replay_buffer_capacity = int(1e5)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAKTHROUGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game = \"breakthrough\"\n",
    "# config = {\"columns\": 5, \"rows\": 5}\n",
    "game = 'python_block_dominoes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] Mean episode rewards [ 3.648 -1.1  ]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "masked_fill_ only supports boolean masks, but got mask with dtype float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pt_result \u001b[38;5;241m=\u001b[39m \u001b[43mpt_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# config,\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_train_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43meval_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhidden_layers_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mreplay_buffer_capacity\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 44\u001b[0m, in \u001b[0;36mpt_main\u001b[0;34m(game, checkpoint_dir, num_train_episodes, eval_every, hidden_layers_sizes, replay_buffer_capacity, batch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m player_id \u001b[38;5;241m=\u001b[39m time_step\u001b[38;5;241m.\u001b[39mobservations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_player\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mis_turn_based:\n\u001b[0;32m---> 44\u001b[0m   agent_output \u001b[38;5;241m=\u001b[39m \u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m   action_list \u001b[38;5;241m=\u001b[39m [agent_output\u001b[38;5;241m.\u001b[39maction]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/open_spiel/open_spiel/python/pytorch/dqn.py:220\u001b[0m, in \u001b[0;36mDQN.step\u001b[0;34m(self, time_step, is_evaluation, add_transition_record)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learn_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 220\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_target_network_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    223\u001b[0m   \u001b[38;5;66;03m# state_dict method returns a dictionary containing a whole state of the\u001b[39;00m\n\u001b[1;32m    224\u001b[0m   \u001b[38;5;66;03m# module.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_q_network\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_q_network\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[0;32m~/Documents/GitHub/open_spiel/open_spiel/python/pytorch/dqn.py:328\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_q_network(next_info_states)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    327\u001b[0m illegal_actions_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m legal_actions_mask\n\u001b[0;32m--> 328\u001b[0m legal_target_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_q_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43millegal_actions_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mILLEGAL_ACTION_LOGITS_PENALTY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m max_next_q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(legal_target_q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    332\u001b[0m target \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    333\u001b[0m     rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m are_final_steps) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discount_factor \u001b[38;5;241m*\u001b[39m max_next_q)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: masked_fill_ only supports boolean masks, but got mask with dtype float"
     ]
    }
   ],
   "source": [
    "pt_result = pt_main(game,\n",
    "                    # config,\n",
    "                    checkpoint_dir,\n",
    "                    num_train_episodes,\n",
    "                    eval_every,\n",
    "                    hidden_layers_sizes,\n",
    "                    replay_buffer_capacity,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ep \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pt_result))]\n\u001b[1;32m      4\u001b[0m pt_r_mean0 \u001b[38;5;241m=\u001b[39m [y[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m pt_result]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ep = [x for x in range(len(pt_result))]\n",
    "pt_r_mean0 = [y[0] for y in pt_result]\n",
    "pt_r_mean1 = [y[1] for y in pt_result]\n",
    "\n",
    "plt.plot(ep,pt_r_mean0, c='red')\n",
    "plt.plot(ep,pt_r_mean1, c='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 11:06:59.135850: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_3/bias_2/Assign' id:458 op device:{requested: '', assigned: ''} def:{{{node mlp_3/bias_2/Assign}} = Assign[T=DT_FLOAT, _class=[\"loc:@mlp_3/bias_2\"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_3/bias_2, mlp_3/zeros_2)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] Mean episode rewards [ 2.995 -3.653]\n",
      "[200] Mean episode rewards [ 3.55  -2.823]\n",
      "[300] Mean episode rewards [ 4.374 -3.707]\n",
      "[400] Mean episode rewards [ 4.252 -3.171]\n",
      "[500] Mean episode rewards [ 4.304 -2.263]\n",
      "[600] Mean episode rewards [ 5.56  -3.963]\n",
      "[700] Mean episode rewards [ 5.596 -2.491]\n",
      "[800] Mean episode rewards [ 5.739 -2.559]\n",
      "[900] Mean episode rewards [ 4.884 -3.927]\n",
      "[1000] Mean episode rewards [ 4.238 -2.987]\n",
      "[1100] Mean episode rewards [ 3.849 -2.482]\n",
      "[1200] Mean episode rewards [ 5.432 -1.558]\n",
      "[1300] Mean episode rewards [ 4.324 -2.152]\n",
      "[1400] Mean episode rewards [ 6.353 -2.16 ]\n",
      "[1500] Mean episode rewards [ 5.341 -2.335]\n",
      "[1600] Mean episode rewards [ 5.71  -2.491]\n",
      "[1700] Mean episode rewards [ 5.051 -2.521]\n",
      "[1800] Mean episode rewards [ 5.021 -2.838]\n",
      "[1900] Mean episode rewards [ 6.509 -2.764]\n",
      "[2000] Mean episode rewards [ 6.3   -2.247]\n",
      "[2100] Mean episode rewards [ 6.246 -1.54 ]\n",
      "[2200] Mean episode rewards [ 6.062 -1.967]\n",
      "[2300] Mean episode rewards [ 5.728 -1.25 ]\n",
      "[2400] Mean episode rewards [ 5.061 -1.912]\n",
      "[2500] Mean episode rewards [ 6.247 -0.57 ]\n",
      "[2600] Mean episode rewards [ 5.667 -1.981]\n",
      "[2700] Mean episode rewards [ 5.181 -2.464]\n",
      "[2800] Mean episode rewards [ 6.558 -0.871]\n",
      "[2900] Mean episode rewards [ 5.872 -1.401]\n",
      "[3000] Mean episode rewards [ 4.979 -1.936]\n",
      "[3100] Mean episode rewards [ 4.425 -1.907]\n",
      "[3200] Mean episode rewards [ 4.321 -0.812]\n",
      "[3300] Mean episode rewards [ 5.069 -1.292]\n",
      "[3400] Mean episode rewards [ 6.149 -0.62 ]\n",
      "[3500] Mean episode rewards [ 6.604 -1.597]\n",
      "[3600] Mean episode rewards [ 4.396 -0.778]\n",
      "[3700] Mean episode rewards [ 3.575 -1.075]\n",
      "[3800] Mean episode rewards [ 5.314 -0.553]\n",
      "[3900] Mean episode rewards [ 5.603 -0.398]\n",
      "[4000] Mean episode rewards [5.839 0.086]\n",
      "[4100] Mean episode rewards [ 6.565 -1.458]\n",
      "[4200] Mean episode rewards [ 7.062 -0.938]\n",
      "[4300] Mean episode rewards [ 6.545 -0.646]\n",
      "[4400] Mean episode rewards [ 5.64  -2.092]\n",
      "[4500] Mean episode rewards [ 6.087 -1.009]\n",
      "[4600] Mean episode rewards [ 6.37  -1.664]\n",
      "[4700] Mean episode rewards [ 6.482 -1.716]\n",
      "[4800] Mean episode rewards [ 6.839 -1.129]\n",
      "[4900] Mean episode rewards [ 5.988 -1.347]\n",
      "[5000] Mean episode rewards [ 6.314 -0.653]\n",
      "[5100] Mean episode rewards [ 5.825 -0.563]\n",
      "[5200] Mean episode rewards [ 6.548 -1.387]\n",
      "[5300] Mean episode rewards [6.077 0.636]\n",
      "[5400] Mean episode rewards [6.429 0.115]\n",
      "[5500] Mean episode rewards [ 6.716 -0.759]\n",
      "[5600] Mean episode rewards [6.222 0.038]\n",
      "[5700] Mean episode rewards [6.528 0.601]\n",
      "[5800] Mean episode rewards [ 6.129 -0.829]\n",
      "[5900] Mean episode rewards [ 6.027 -0.818]\n",
      "[6000] Mean episode rewards [ 6.23  -0.172]\n",
      "[6100] Mean episode rewards [ 4.447 -0.839]\n",
      "[6200] Mean episode rewards [ 4.605 -0.386]\n",
      "[6300] Mean episode rewards [ 5.44 -0.3 ]\n",
      "[6400] Mean episode rewards [ 5.515 -0.068]\n",
      "[6500] Mean episode rewards [ 5.095 -0.809]\n",
      "[6600] Mean episode rewards [5.27  0.154]\n",
      "[6700] Mean episode rewards [5.642e+00 1.000e-03]\n",
      "[6800] Mean episode rewards [ 5.737 -0.518]\n",
      "[6900] Mean episode rewards [5.955 0.61 ]\n",
      "[7000] Mean episode rewards [ 6.433 -0.696]\n",
      "[7100] Mean episode rewards [ 5.261 -0.169]\n",
      "[7200] Mean episode rewards [ 6.126 -0.223]\n",
      "[7300] Mean episode rewards [ 6.744 -0.315]\n",
      "[7400] Mean episode rewards [ 6.833 -0.639]\n",
      "[7500] Mean episode rewards [6.264 0.159]\n",
      "[7600] Mean episode rewards [5.455 0.205]\n",
      "[7700] Mean episode rewards [6.12  0.716]\n",
      "[7800] Mean episode rewards [6.23  0.141]\n",
      "[7900] Mean episode rewards [ 6.918 -0.691]\n",
      "[8000] Mean episode rewards [ 5.549 -0.264]\n",
      "[8100] Mean episode rewards [ 7.324 -0.836]\n",
      "[8200] Mean episode rewards [6.574 0.48 ]\n",
      "[8300] Mean episode rewards [ 6.31  -0.395]\n",
      "[8400] Mean episode rewards [ 6.795 -0.87 ]\n",
      "[8500] Mean episode rewards [6.758 0.118]\n",
      "[8600] Mean episode rewards [6.861 0.115]\n",
      "[8700] Mean episode rewards [ 6.74  -0.584]\n",
      "[8800] Mean episode rewards [ 6.157 -0.632]\n",
      "[8900] Mean episode rewards [ 6.694 -0.914]\n",
      "[9000] Mean episode rewards [ 5.787e+00 -1.000e-03]\n",
      "[9100] Mean episode rewards [6.289 0.172]\n",
      "[9200] Mean episode rewards [ 5.807 -0.214]\n",
      "[9300] Mean episode rewards [ 6.825 -0.04 ]\n",
      "[9400] Mean episode rewards [6.194 0.412]\n",
      "[9500] Mean episode rewards [5.167 0.545]\n",
      "[9600] Mean episode rewards [3.817 0.388]\n",
      "[9700] Mean episode rewards [ 6.719 -0.428]\n",
      "[9800] Mean episode rewards [ 5.865 -0.113]\n",
      "[9900] Mean episode rewards [6.445 0.385]\n",
      "[10000] Mean episode rewards [6.806 0.47 ]\n"
     ]
    }
   ],
   "source": [
    "result_tf = tf_main(game,\n",
    "                    # config,\n",
    "                    checkpoint_dir,\n",
    "                    num_train_episodes,\n",
    "                    eval_every,\n",
    "                    hidden_layers_sizes,\n",
    "                    replay_buffer_capacity,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ep \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mresult_tf\u001b[49m))]\n\u001b[1;32m      2\u001b[0m tf_r_mean0 \u001b[38;5;241m=\u001b[39m [y[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m result_tf]\n\u001b[1;32m      3\u001b[0m tf_r_mean1 \u001b[38;5;241m=\u001b[39m [y[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m result_tf]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_tf' is not defined"
     ]
    }
   ],
   "source": [
    "ep = [x for x in range(len(result_tf))]\n",
    "tf_r_mean0 = [y[0] for y in result_tf]\n",
    "tf_r_mean1 = [y[1] for y in result_tf]\n",
    "\n",
    "plt.plot(ep,tf_r_mean0, c='red')\n",
    "plt.plot(ep,tf_r_mean1, c='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep,pt_r_mean0, c='skyblue')\n",
    "plt.plot(ep,pt_r_mean1, c='skyblue', linestyle='dashed')\n",
    "plt.plot(ep,tf_r_mean0, c='pink')\n",
    "plt.plot(ep,tf_r_mean1, c='pink', linestyle='dashed')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIC-TAC-TOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"tic_tac_toe\"\n",
    "config = {}\n",
    "num_train_episodes = 20000\n",
    "eval_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000] Mean episode rewards [0.823 0.112]\n",
      "[2000] Mean episode rewards [0.769 0.089]\n",
      "[3000] Mean episode rewards [0.883 0.161]\n",
      "[4000] Mean episode rewards [0.723 0.229]\n",
      "[5000] Mean episode rewards [0.424 0.125]\n",
      "[6000] Mean episode rewards [0.54  0.246]\n",
      "[7000] Mean episode rewards [0.637 0.244]\n",
      "[8000] Mean episode rewards [0.794 0.236]\n",
      "[9000] Mean episode rewards [0.643 0.148]\n",
      "[10000] Mean episode rewards [0.813 0.148]\n",
      "[11000] Mean episode rewards [0.626 0.17 ]\n",
      "[12000] Mean episode rewards [0.622 0.188]\n",
      "[13000] Mean episode rewards [0.874 0.244]\n",
      "[14000] Mean episode rewards [0.856 0.183]\n",
      "[15000] Mean episode rewards [0.825 0.28 ]\n",
      "[16000] Mean episode rewards [0.82  0.361]\n",
      "[17000] Mean episode rewards [0.847 0.241]\n",
      "[18000] Mean episode rewards [0.869 0.296]\n",
      "[19000] Mean episode rewards [0.904 0.261]\n",
      "[20000] Mean episode rewards [0.858 0.277]\n"
     ]
    }
   ],
   "source": [
    "pt_result = pt_main(game,\n",
    "                    config,\n",
    "                    checkpoint_dir,\n",
    "                    num_train_episodes,\n",
    "                    eval_every,\n",
    "                    hidden_layers_sizes,\n",
    "                    replay_buffer_capacity,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ep = [x for x in range(len(pt_result))]\n",
    "pt_r_mean0 = [y[0] for y in pt_result]\n",
    "pt_r_mean1 = [y[1] for y in pt_result]\n",
    "\n",
    "plt.plot(ep,pt_r_mean0, c='red')\n",
    "plt.plot(ep,pt_r_mean1, c='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000] Mean episode rewards [ 0.493 -0.204]\n",
      "[2000] Mean episode rewards [ 0.346 -0.244]\n",
      "[3000] Mean episode rewards [ 0.537 -0.054]\n",
      "[4000] Mean episode rewards [ 0.464 -0.059]\n",
      "[5000] Mean episode rewards [0.46  0.144]\n",
      "[6000] Mean episode rewards [0.442 0.08 ]\n",
      "[7000] Mean episode rewards [0.606 0.068]\n",
      "[8000] Mean episode rewards [0.447 0.165]\n",
      "[9000] Mean episode rewards [0.702 0.196]\n",
      "[10000] Mean episode rewards [0.694 0.227]\n",
      "[11000] Mean episode rewards [0.757 0.213]\n",
      "[12000] Mean episode rewards [0.829 0.149]\n",
      "[13000] Mean episode rewards [0.733 0.186]\n",
      "[14000] Mean episode rewards [0.8   0.318]\n",
      "[15000] Mean episode rewards [0.849 0.308]\n",
      "[16000] Mean episode rewards [0.789 0.198]\n",
      "[17000] Mean episode rewards [0.825 0.353]\n",
      "[18000] Mean episode rewards [0.804 0.367]\n",
      "[19000] Mean episode rewards [0.827 0.355]\n",
      "[20000] Mean episode rewards [0.796 0.368]\n"
     ]
    }
   ],
   "source": [
    "result_tf = tf_main(game,\n",
    "                    config,\n",
    "                    checkpoint_dir,\n",
    "                    num_train_episodes,\n",
    "                    eval_every,\n",
    "                    hidden_layers_sizes,\n",
    "                    replay_buffer_capacity,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = [x for x in range(len(result_tf))]\n",
    "tf_r_mean0 = [y[0] for y in result_tf]\n",
    "tf_r_mean1 = [y[1] for y in result_tf]\n",
    "\n",
    "plt.plot(ep,tf_r_mean0, c='red')\n",
    "plt.plot(ep,tf_r_mean1, c='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep,pt_r_mean0, c='skyblue')\n",
    "plt.plot(ep,pt_r_mean1, c='skyblue', linestyle='dashed')\n",
    "plt.plot(ep,tf_r_mean0, c='pink')\n",
    "plt.plot(ep,tf_r_mean1, c='pink', linestyle='dashed')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
