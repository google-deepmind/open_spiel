{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kNT4QZ3k6tk"
      },
      "source": [
        "# Setup\n",
        "\n",
        "We use [OpenSpiel](https://github.com/deepmind/open_spiel) library for this setting. OpenSpiel is a collection of environments and algorithms for research in general reinforcement learning and search/planning in games."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKAod1ARM0vi"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Import the OpenSpiel and other auxiliary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeLv5Ukxj8sR"
      },
      "outputs": [],
      "source": [
        "\"\"\"Useful imports\"\"\"\n",
        "\n",
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G9298ghC6f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import dataclasses\n",
        "import math\n",
        "import re\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "\n",
        "import datetime\n",
        "from matplotlib import animation\n",
        "from matplotlib import cm\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from open_spiel.python import policy\n",
        "from open_spiel.python import policy as policy_std\n",
        "from open_spiel.python.mfg import distribution as distribution_std\n",
        "from open_spiel.python.mfg import value as value_std\n",
        "from open_spiel.python.mfg.algorithms import best_response_value\n",
        "from open_spiel.python.mfg.algorithms import boltzmann_policy_iteration\n",
        "from open_spiel.python.mfg.algorithms import distribution\n",
        "from open_spiel.python.mfg.algorithms import fictitious_play\n",
        "from open_spiel.python.mfg.algorithms import fixed_point\n",
        "from open_spiel.python.mfg.algorithms import greedy_policy\n",
        "from open_spiel.python.mfg.algorithms import mirror_descent\n",
        "from open_spiel.python.mfg.algorithms import munchausen_mirror_descent\n",
        "from open_spiel.python.mfg.algorithms import nash_conv\n",
        "from open_spiel.python.mfg.algorithms import policy_value\n",
        "from open_spiel.python.mfg.games import factory\n",
        "import pyspiel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaPOvThZRCB4"
      },
      "source": [
        "## Forbidden states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d_Z8Dq_RDKH"
      },
      "outputs": [],
      "source": [
        "forbidden_states_grid = [\n",
        "    '#############',\n",
        "    '#     #     #',\n",
        "    '#     #     #',\n",
        "    '#           #',\n",
        "    '#     #     #',\n",
        "    '#     #     #',\n",
        "    '### ##### ###',\n",
        "    '#     #     #',\n",
        "    '#     #     #',\n",
        "    '#           #',\n",
        "    '#     #     #',\n",
        "    '#     #     #',\n",
        "    '#############',\n",
        "]\n",
        "\n",
        "def grid_to_forbidden_states(grid):\n",
        "  \"\"\"Converts a grid into string representation of forbidden states.\n",
        "\n",
        "  Args:\n",
        "    grid: Rows of the grid. '#' character denotes a forbidden state. All rows\n",
        "      should have the same number of columns, i.e. cells.\n",
        "\n",
        "  Returns:\n",
        "    String representation of forbidden states in the form of x (column) and y\n",
        "    (row) pairs, e.g. [1|1;0|2].\n",
        "  \"\"\"\n",
        "  forbidden_states = []\n",
        "  num_cols = len(grid[0])\n",
        "  for y, row in enumerate(grid):\n",
        "    assert len(row) == num_cols, f'Number of columns should be {num_cols}.'\n",
        "    for x, cell in enumerate(row):\n",
        "      if cell == '#':\n",
        "        forbidden_states.append(f'{x}|{y}')\n",
        "  return '[' + ';'.join(forbidden_states) + ']'\n",
        "\n",
        "FOUR_ROOMS_FORBIDDEN_STATES = grid_to_forbidden_states(forbidden_states_grid)\n",
        "forbidden_states_indicator = np.array([[math.nan if c=='#' else 0 for c in [*row]] for row in forbidden_states_grid])\n",
        "\n",
        "four_rooms_default_setting = {\n",
        "    'forbidden_states': FOUR_ROOMS_FORBIDDEN_STATES,\n",
        "    'horizon': 41,\n",
        "    'initial_distribution': '[1|1]',\n",
        "    'initial_distribution_value': '[1.0]',\n",
        "    'size': 13,\n",
        "    'only_distribution_reward': True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmiZH91CQpcL"
      },
      "source": [
        "## Helper methods for visualization\n",
        "\n",
        "The state representation and distribution of each game would be different. OpenSpiel does not provide any built in visualization capabilities. We define some basic methods for displaying the two-dimensional grid and the distribution for our game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_znsAseM7zD"
      },
      "outputs": [],
      "source": [
        "\"\"\"Helper methods for visualization. These are game specific.\"\"\"\n",
        "\n",
        "\n",
        "def decode_distribution(game: pyspiel.Game,\n",
        "                        dist: Dict[str, float],\n",
        "                        nans: bool = True) -\u003e np.ndarray:\n",
        "  \"\"\"Decodes the distribution of a 2D crowd modelling game from a dictionary.\"\"\"\n",
        "  # Extract the size of the distribution from the game parameters. Time, i.e.\n",
        "  # horizon is the leading dimension so that we can easily present the temporal\n",
        "  # aspect.\n",
        "  params = game.get_parameters()\n",
        "  dist_size = (params['horizon'], params['size'], params['size'])\n",
        "  decoded = np.zeros(dist_size)\n",
        "\n",
        "  for key, value in dist.items():\n",
        "    m = re.fullmatch(r'\\((?P\u003cx\u003e\\d+),\\s*(?P\u003cy\u003e\\d+),\\s*(?P\u003ct\u003e\\d+)\\)', key)\n",
        "    if m:\n",
        "      g = m.group\n",
        "      decoded[(int(g('t')), int(g('y')), int(g('x')))] = value\n",
        "\n",
        "  return decoded\n",
        "\n",
        "\n",
        "def get_policy_distribution(game: pyspiel.Game,\n",
        "                            policy: policy_std.Policy) -\u003e np.ndarray:\n",
        "  \"\"\"Returns the distribution of the policy.\"\"\"\n",
        "  dist_policy = distribution.DistributionPolicy(game, policy)\n",
        "  return decode_distribution(game, dist_policy.distribution)\n",
        "\n",
        "\n",
        "def animate_distributions(dists: np.ndarray,\n",
        "                          fixed_cbar: bool = False) -\u003e animation.FuncAnimation:\n",
        "  \"\"\"Animates the given distributions.\n",
        "\n",
        "  Args:\n",
        "    dists: An np.ndarray of batched distributions.\n",
        "    fixed_cbar: If true, then the color bar will have a fixed scale over all\n",
        "      distributions.\n",
        "\n",
        "  Returns:\n",
        "    A function animation.\n",
        "  \"\"\"\n",
        "  if fixed_cbar:\n",
        "    vmin = np.min(dists)\n",
        "    vmax = np.max(dists)\n",
        "  else:\n",
        "    vmin, vmax = None, None\n",
        "\n",
        "  def frame(i):\n",
        "    ax.cla()\n",
        "    sns.heatmap(\n",
        "        dists[i, ...],\n",
        "        square=True,\n",
        "        cmap=plt.cm.viridis,\n",
        "        linecolor='white',\n",
        "        linewidths=0.1,\n",
        "        ax=ax,\n",
        "        cbar=True,\n",
        "        cbar_ax=cbar_ax,\n",
        "        vmin=vmin,\n",
        "        vmax=vmax)\n",
        "\n",
        "  grid_kws = {'width_ratios': (0.9, 0.05), 'wspace': 0.2}\n",
        "  fig, (ax, cbar_ax) = plt.subplots(1, 2, gridspec_kw=grid_kws, figsize=(7, 5))\n",
        "  anim = animation.FuncAnimation(\n",
        "      fig=fig, func=frame, frames=dists.shape[0], interval=50, blit=False)\n",
        "  # This prevents plot output at each frame.\n",
        "  plt.close()\n",
        "  return anim\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class RunResult:\n",
        "  \"\"\"Holds the result of running an algorithm.\n",
        "\n",
        "  Attributes:\n",
        "    policy: The resulting policy.\n",
        "    dists: An np.ndarray that contains the distributions at horizon for each\n",
        "      iteration.\n",
        "    nash_convs: Nash Conv metrics at each iteration.\n",
        "    last_dist: The distribution for the last iteration of the algorithm.\n",
        "  \"\"\"\n",
        "  policy: policy_std.Policy\n",
        "  dists: np.ndarray\n",
        "  nash_convs: np.ndarray\n",
        "  last_dist: np.ndarray\n",
        "\n",
        "\n",
        "\n",
        "def run_algorithm(game: pyspiel.Game, algo, num_iterations: int,\n",
        "                  learning_rate=None, init_policy=None):\n",
        "  \"\"\"Runs the algorithm for specified number of iterations.\n",
        "\n",
        "  Args:\n",
        "    game: An MFG.\n",
        "    algo: Algorithm to use.\n",
        "    num_iterations: Number of iterations.\n",
        "\n",
        "  Returns:\n",
        "    The final policy and the Nash Conv values at each iteration.\n",
        "  \"\"\"\n",
        "  nash_convs = []\n",
        "  dists = []\n",
        "  current_policy = init_policy\n",
        "  dist = None\n",
        "  # Added to save the initialization\n",
        "  startt = time.time()\n",
        "  if not current_policy:\n",
        "    current_policy = algo.get_policy()\n",
        "  nash_convs.append(nash_conv.NashConv(game, current_policy).nash_conv())\n",
        "  dist = get_policy_distribution(game, current_policy)\n",
        "  # dists.append(dist[-1, :]) # if single population\n",
        "  dists.append(dist)\n",
        "  print(\"Done iteration = 0, \\ttime = \", time.time() - startt, \"\\tnash_conv = \", nash_convs[-1])\n",
        "  for i in range(num_iterations):\n",
        "    startt = time.time()\n",
        "    if learning_rate:\n",
        "      algo.iteration(learning_rate=learning_rate)\n",
        "    else:\n",
        "      algo.iteration()\n",
        "    current_policy = algo.get_policy()\n",
        "    nash_convs.append(nash_conv.NashConv(game, current_policy).nash_conv())\n",
        "    dist = get_policy_distribution(game, current_policy)\n",
        "    dists.append(dist)\n",
        "    if (i+1)%2==0:\n",
        "      print(\"Done iteration = \", i+1, \"\\ttime = \", time.time() - startt, \"\\tnash_conv = \", nash_convs[-1])\n",
        "    # print(\"run_algorithm: distribution: \", dists[-1])\n",
        "\n",
        "  return RunResult(\n",
        "      policy=current_policy,\n",
        "      dists=np.stack(dists),\n",
        "      nash_convs=np.array(nash_convs),\n",
        "      last_dist=dist)\n",
        "\n",
        "\n",
        "def display_result(result: RunResult):\n",
        "  \"\"\"Displays the run results.\"\"\"\n",
        "  sns.set(rc={'figure.figsize':(10, 6)})\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(result.nash_convs)\n",
        "  ax.set_xlabel('iteration')\n",
        "  ax.set_ylabel('Nash Conv')\n",
        "  return HTML(animate_distributions(result.dists).to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qeYHadHRvP_"
      },
      "outputs": [],
      "source": [
        "# Exploitability\n",
        "# Comparison of exploitability.\n",
        "ft_size = 20\n",
        "def display_exploitability(results: Dict[str, RunResult]):\n",
        "  fig_exploitabilities = plt.gcf()\n",
        "  nash_conv_df = pd.DataFrame.from_dict({name: result.nash_convs for name, result in results.items()})\n",
        "\n",
        "  sns.set(rc={'figure.figsize':(15,8)})\n",
        "  sns.set_theme(style=\"whitegrid\")\n",
        "  ax = sns.lineplot(data=nash_conv_df, palette=\"tab10\", linewidth=2.5)\n",
        "  ax.set_yscale('log')\n",
        "  ax.set_xlabel('iterations', fontsize=ft_size)\n",
        "  ax.set_ylabel('exploitability', fontsize=ft_size)\n",
        "  plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, fontsize=ft_size)\n",
        "  ax.set_xticklabels(ax.get_xticks(), size = ft_size)\n",
        "  ax.set_yticklabels(ax.get_yticks(), size = ft_size)\n",
        "  fig_exploitabilities.tight_layout()\n",
        "  return fig_exploitabilities\n",
        "# Usage:\n",
        "# display_exploitability(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fT1ChrlRxW3"
      },
      "outputs": [],
      "source": [
        "# Usage:\n",
        "# n_steps = game.get_parameters()['horizon']\n",
        "# steps = range(0,n_steps,2)\n",
        "# fig_distributions = display_distribution_at_steps(results, steps, size=2)\n",
        "ft_size = 20\n",
        "def display_distribution_at_steps(results, steps, size=4, forbidden_states_indicator=None):\n",
        "  num_steps = len(steps)\n",
        "  num_results = len(results)\n",
        "  fig, axs = plt.subplots(\n",
        "      num_results,\n",
        "      num_steps,\n",
        "      sharex='col',\n",
        "      sharey='row',\n",
        "      figsize=(num_steps * size, num_results * size))\n",
        "  for row, (name, result) in enumerate(results.items()):\n",
        "    for i, step in enumerate(steps):\n",
        "      d = result.last_dist[step]\n",
        "      minval = round(np.amin(d), 3)\n",
        "      maxval=round(np.amax(d), 3)\n",
        "      if forbidden_states_indicator is not None:\n",
        "        d = d + forbidden_states_indicator\n",
        "      masked_array = np.ma.array (d, mask=np.isnan(d))\n",
        "      cmap = plt.cm.viridis\n",
        "      cmap.set_bad('grey',1.)\n",
        "      ax = axs[row][i]\n",
        "      ax.axis('off')\n",
        "      ax.set_title(str(name) + \"\\n\" + str(i) if not i else str(step), size = ft_size)\n",
        "      im = ax.imshow(\n",
        "          d,\n",
        "          interpolation='nearest',\n",
        "          cmap=plt.cm.viridis, vmin=minval, vmax=maxval)\n",
        "      ticks=[round(minval + i*(maxval-minval)/4.0, 3) for i in range(5)]\n",
        "      cbar = plt.colorbar(im, ax=ax, fraction=0.046, ticks=ticks)\n",
        "      cbar.ax.tick_params(labelsize=ft_size)\n",
        "      ax.set_xticklabels(ax.get_xticks(), size = ft_size)\n",
        "      ax.set_yticklabels(ax.get_yticks(), size = ft_size)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  return fig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyfIW0FbF_9J"
      },
      "source": [
        "# Run algos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QArHwBjvvkyJ"
      },
      "outputs": [],
      "source": [
        "settings = {\n",
        "    # \"with_large_noise\": {\"noise_intensity\": 1.0},\n",
        "    # \"with_medium_noise\": {\"noise_intensity\": 0.5},\n",
        "    \"with_small_noise\": {\"noise_intensity\": 0.1},\n",
        "    # \"with_no_noise\": {\"noise_intensity\": 0.0},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq_nBAh9F_eE"
      },
      "outputs": [],
      "source": [
        "num_iterations = 300\n",
        "\n",
        "setting_results = {}\n",
        "\n",
        "for (sk,sv) in settings.items():\n",
        "  print(\"\\n\\n\\n Setting {}: noise_intensity={}\\n\\n\\n\".format(sk, sv.get(\"noise_intensity\")))\n",
        "\n",
        "  four_rooms_default_setting.update([(\"noise_intensity\", sv.get(\"noise_intensity\"))])\n",
        "  game_name = 'mfg_crowd_modelling_2d'\n",
        "  game_name_setting = 'mfg_crowd_modelling_2d_four_rooms_exploration'\n",
        "  game = pyspiel.load_game(game_name, four_rooms_default_setting)\n",
        "  init_policy = None\n",
        "  #####\n",
        "  print(\"start_time = \", datetime.datetime.now())\n",
        "  start_time = time.time()\n",
        "  print(\"start_time = \", start_time)\n",
        "  ######\n",
        "  start_time = time.time()\n",
        "  fp = fictitious_play.FictitiousPlay(game)\n",
        "  fp_result = run_algorithm(game, fp, num_iterations, init_policy=init_policy)\n",
        "  print(\"FP DONE, time = \", time.time() - start_time)\n",
        "  start_time = time.time()\n",
        "  md = mirror_descent.MirrorDescent(game, lr=0.05)\n",
        "  md_result = run_algorithm(game, md, num_iterations, init_policy=init_policy)\n",
        "  print(\"OMD LR 0.1 DONE, time = \", time.time() - start_time)\n",
        "  # start_time = time.time()\n",
        "  # munchausen_md = munchausen_mirror_descent.MunchausenMirrorDescent(game, lr=0.1)\n",
        "  # munchausen_md_result = run_algorithm(game, munchausen_md, num_iterations, init_policy=init_policy)\n",
        "  # print(\"MOMD DONE, time = \", time.time() - start_time)\n",
        "  start_time = time.time()\n",
        "  fixedp = fixed_point.FixedPoint(game)\n",
        "  fixedp_result = run_algorithm(game, fixedp, num_iterations, init_policy=init_policy)\n",
        "  print(\"FixedP DONE, time = \", time.time() - start_time)\n",
        "  start_time = time.time()\n",
        "  fpd = fictitious_play.FictitiousPlay(game, lr=0.01)\n",
        "  fpd_result = run_algorithm(game, fpd, num_iterations, init_policy=init_policy)\n",
        "  print(\"Damped FP DONE, time = \", time.time() - start_time)\n",
        "  start_time = time.time()\n",
        "  fixedp_softmax = fixed_point.FixedPoint(game, temperature=0.1)\n",
        "  fixedp_softmax_result = run_algorithm(game, fixedp_softmax, num_iterations, init_policy=init_policy)\n",
        "  print(\"FixedP softmax DONE, time = \", time.time() - start_time)\n",
        "  start_time = time.time()\n",
        "  fpsoft = fictitious_play.FictitiousPlay(game, temperature=0.1)\n",
        "  fpsoft_result = run_algorithm(game, fpsoft, num_iterations, init_policy=init_policy)\n",
        "  print(\"FP softmax DONE, time = \", time.time() - start_time)\n",
        "  start_time =  time.time()\n",
        "  bpi = boltzmann_policy_iteration.BoltzmannPolicyIteration(game, lr=0.1)\n",
        "  bpi_result = run_algorithm(game, bpi, num_iterations, init_policy=init_policy)\n",
        "  print(\"BPI DONE, time = \", time.time() - start_time)\n",
        "  ###\n",
        "  results = {\n",
        "    'Fictitious Play': fp_result,\n",
        "    'Online Mirror Descent': md_result,\n",
        "    # 'Munchausen OMD': munchausen_md_result,\n",
        "    'Fixed Point': fixedp_result,\n",
        "    'Damped Fixed Point': fpd_result,\n",
        "    'Softmax Fixed Point': fixedp_softmax_result,\n",
        "    'Softmax Fictitious Play': fpsoft_result,\n",
        "    'Boltzmann Policy Iteration': bpi_result,\n",
        "  }\n",
        "  setting_results.update([(sk, results)])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0zxyA1xDFBZ"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dOCKlc_UdNf"
      },
      "source": [
        "## Save data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY1kHvSFM7vl"
      },
      "outputs": [],
      "source": [
        "from colabtools import fileedit\n",
        "\n",
        "\n",
        "# # Downloading the results\n",
        "# np.savez('/tmp/{}-setting_results.npz'.format(game_name_setting), setting_results=setting_results)\n",
        "# # %download_file /tmp/setting_results.npz\n",
        "# fileedit.download_file('/tmp/{}-setting_results.npz'.format(game_name_setting), ephemeral=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCzslCs0UeU5"
      },
      "source": [
        "## Exploitability\n",
        "\n",
        "It seems that we need to run this piece of code twice in order to have the correct figure size. The first time, the figure is smaller than expected. I suspect that the size is not well defined / fixed in the function display_exploitability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1_SFNYYDIjC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Plotting the results\n",
        "for (sk, results) in setting_results.items():\n",
        "  print(\"\\n\\n\\n Setting {}\\n\\n\\n\".format(sk))\n",
        "  s_sk = settings[sk]\n",
        "  fig_exploitabilities = display_exploitability(results)\n",
        "  fig_exploitabilities.savefig('/tmp/{}-noise{}_exploitabilities.pdf'.format(game_name_setting, s_sk.get(\"noise_intensity\")))\n",
        "  fileedit.download_file('/tmp/{}-noise{}_exploitabilities.pdf'.format(game_name_setting, s_sk.get(\"noise_intensity\")), ephemeral=True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4jYHQmjUgHV"
      },
      "source": [
        "## Distributions\n",
        "\n",
        "In this version, the plotting function has been modified to take extra parameters for the colorbar. If no parameters are given, then we are going to use the smallest and largest values of the distribution (beware that if there is a forbidden state, the smallest value is always 0 because there is no mass on forbidden states)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSYRJvn6DKRs"
      },
      "outputs": [],
      "source": [
        "# Plotting the results\n",
        "for (sk, results) in setting_results.items():\n",
        "  print(\"\\n\\n\\n Setting {}\\n\\n\\n\".format(sk))\n",
        "  s_sk = settings[sk]\n",
        "  fig_distributions = display_distribution_at_steps(results, range(0, 41, 5), 5, forbidden_states_indicator)\n",
        "  fig_distributions.savefig('/tmp/{}-noise{}_distributions.pdf'.format(game_name_setting, s_sk.get(\"noise_intensity\")))\n",
        "  fileedit.download_file('/tmp/{}-noise{}_distributions.pdf'.format(game_name_setting, s_sk.get(\"noise_intensity\")), ephemeral=True)\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "10Pq-xQltz7r9F9ms_rdOcmedUJg4sxPk",
          "timestamp": 1703171920274
        },
        {
          "file_id": "1D-v9ERt1IYFNe_2stvBbNurI54Gmrm0p",
          "timestamp": 1703167054504
        },
        {
          "file_id": "1_HpSbPqfF4iehxIzgQ8bpHmEEN0JNx_U",
          "timestamp": 1689468319981
        },
        {
          "file_id": "1Hyiw9oWOqMrVDBFfzSDOAdt0L9m2jaYp",
          "timestamp": 1689453000205
        },
        {
          "file_id": "1MsoPiJKf05k7civpTndix3YYgoVOhf4G",
          "timestamp": 1688043948116
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
