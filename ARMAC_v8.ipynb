{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbrenton/open_spiel/blob/master/ARMAC_v8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TJ-hY3WERdD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6892456-aa9e-42c4-babc-f438a5b0fe58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▎                             | 10 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 30 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 61 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 81 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 92 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 145 kB 4.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install chex\n",
        "!pip -q install dm-haiku\n",
        "!pip -q install open_spiel\n",
        "!pip -q install optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YFwX1CXgRjsR"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import chex\n",
        "import optax\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import pyspiel\n",
        "from open_spiel.python import policy\n",
        "from copy import deepcopy\n",
        "from open_spiel.python import rl_agent\n",
        "from open_spiel.python import rl_environment\n",
        "from open_spiel.python.algorithms import exploitability\n",
        "from open_spiel.python.rl_environment import StepType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0dMQGm8QY2W"
      },
      "source": [
        "# Armac Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TY0DMhMzRodw"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "class DoubleReLU(hk.Module):\n",
        "    \"\"\" Double relu according to https://arxiv.org/pdf/1603.05201.pdf \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = jnp.concatenate([x, -x])\n",
        "        return jax.nn.relu(x)\n",
        "\n",
        "class InfoStateRepresentation(hk.Module):\n",
        "    \"\"\" Composable block for mapping observations\n",
        "     to information state representation. As described\n",
        "     in paragraph 1 of Appendix F. \"\"\"\n",
        "\n",
        "    def __init__(self, linear_size):\n",
        "        super().__init__()\n",
        "        self.linear_size = linear_size\n",
        "        self.double_relu = DoubleReLU()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = hk.Linear(self.linear_size)(x)\n",
        "        x = self.double_relu(x)\n",
        "        # LSTM goes here instead of linear \n",
        "        x = hk.Linear(self.linear_size)(x)\n",
        "        info_rep = jax.nn.relu(x)\n",
        "        return info_rep\n",
        "\n",
        "class ArchitectureB(hk.Module):\n",
        "    \"\"\" Architecture B(x), as defined\n",
        "     in paragraph 2 of Appendix F. \n",
        "     \n",
        "     Question: there is not an activation layer descriped between\n",
        "      LSTM in InfoStateRepresentation and h1 in B(x) in Appendix F.\n",
        "      Was this intentional? I have used regular relu on output\n",
        "      of InfoStateRepresentation for now.\"\"\"\n",
        "\n",
        "    def __init__(self, linear_size):\n",
        "        super().__init__()\n",
        "        self.linear_size = linear_size\n",
        "        self.double_relu = DoubleReLU()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = hk.Linear(self.linear_size)(x)\n",
        "        h2 = self.double_relu(h1)\n",
        "        h3 = h1 + hk.Linear(self.linear_size)(h2)\n",
        "        return self.double_relu(h3)\n",
        "\n",
        "class CriticNetwork(hk.Module):\n",
        "    \"\"\" Global critic network maps players' info states to Q-values \n",
        "    for each player \"\"\"\n",
        "    def __init__(self, size, num_actions, name):\n",
        "        super().__init__(name=name)\n",
        "        self.num_actions = num_actions\n",
        "        self.n_A = hk.Linear(size)\n",
        "        self.n_B = hk.Linear(size)\n",
        "        self.b_block = ArchitectureB(size)\n",
        "\n",
        "    def __call__(self, history):\n",
        "        # are activations correct here?\n",
        "        s_0, s_1 = history\n",
        "        a_0 = jax.nn.relu(self.n_A(s_0) + self.n_B(s_1))\n",
        "        a_1 = jax.nn.relu(self.n_B(s_0) + self.n_A(s_1))\n",
        "        h1 = jnp.concatenate([a_0, a_1])\n",
        "        h2 = self.b_block(h1)\n",
        "\n",
        "        q_0 = hk.Linear(self.num_actions)(h2)\n",
        "        q_1 = hk.Linear(self.num_actions)(h2)\n",
        "\n",
        "        return jnp.stack([q_0, q_1])\n",
        "\n",
        "class PlayerNetwork(hk.Module):\n",
        "    \"\"\" Composable block for mapping single info state to average regret \n",
        "    head (W_bar), mean policy head (pi_bar). See paragraph 3 of Appendix F. \"\"\"\n",
        "    def __init__(self, layers, num_actions, name):\n",
        "        super().__init__(name=name)\n",
        "        self.num_actions = num_actions\n",
        "        self.info_state_rep_block = InfoStateRepresentation(layers[0])\n",
        "        self.b_block = ArchitectureB(layers[1])\n",
        "\n",
        "    def __call__(self, info_state):\n",
        "        info_rep = self.info_state_rep_block(info_state)\n",
        "        b_out = self.b_block(info_rep) \n",
        "\n",
        "        W_bar = hk.Linear(self.num_actions)(b_out)\n",
        "        Pi_bar = hk.Linear(self.num_actions)(b_out)\n",
        "\n",
        "        return ActorOutput(W_bar=W_bar,\n",
        "                           Pi_bar=Pi_bar)\n",
        "@chex.dataclass\n",
        "class ActorOutput: # NamedTuple\n",
        "    W_bar : chex.Array\n",
        "    Pi_bar : chex.Array\n",
        "\n",
        "@chex.dataclass\n",
        "class PlayerOuput:\n",
        "    avg_regret : chex.Array\n",
        "    mean_policy : chex.Array\n",
        "    q_values : chex.Array\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bpO8ZAE8dtm5"
      },
      "outputs": [],
      "source": [
        "def armac_network():\n",
        "    actor = PlayerNetwork([256, 128], 3, name='actor')\n",
        "    critic = CriticNetwork(128, 3, name='critic')\n",
        "\n",
        "    def init(history):\n",
        "        actor0_head = actor(history[0])\n",
        "        actor1_head = actor(history[1])\n",
        "        critic_head = critic(history)\n",
        "        \n",
        "        p0_out = PlayerOuput(avg_regret=actor0_head.W_bar,\n",
        "                             mean_policy=actor0_head.Pi_bar,\n",
        "                             q_values=critic_head[0])\n",
        "        p1_out = PlayerOuput(avg_regret=actor1_head.W_bar,\n",
        "                             mean_policy=actor1_head.Pi_bar,\n",
        "                             q_values=critic_head[1])\n",
        "        \n",
        "        return p0_out, p1_out\n",
        "    return init, (init, actor, critic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_-8wa7Zy-WB"
      },
      "source": [
        "# Resevoir Buffer & Network Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vOhVqJgIQvkl"
      },
      "outputs": [],
      "source": [
        "class NetworkBuffer:\n",
        "    \"\"\" Stores network parameters every epoch in buffer \"\"\"\n",
        "    def __init__(self, max_len):  \n",
        "        self.buffer = []\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.buffer[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def add(self, params):\n",
        "        \"\"\" Only keep the most recent networks \"\"\"\n",
        "        if len(self.buffer) >= self.max_len:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(params)\n",
        "\n",
        "\n",
        "# This can be transitioned into a stand alone replay buffer that contains \n",
        "# the methods currently implemented as methods of ARMAC class\n",
        "@chex.dataclass\n",
        "class JaxFriendlyBuffer:\n",
        "    i: chex.Array\n",
        "    j: chex.Array\n",
        "    history: chex.Array\n",
        "    prev_history: chex.Array\n",
        "    info_state: chex.Array\n",
        "    action: chex.Array\n",
        "    legal_actions_mask: chex.Array\n",
        "    acting_player: chex.Array\n",
        "    regret: chex.Array\n",
        "    policy_j: chex.Array\n",
        "    discount: chex.Array\n",
        "    rewards: chex.Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ga08So68Onp"
      },
      "source": [
        "# ARMAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4wLo47ZAarz_"
      },
      "outputs": [],
      "source": [
        "# exploitability.nash_conv expects policy.Policy, using seperate class accordingly\n",
        "# note instance of policy.Policy is not explicitly check for\n",
        "class MeanPolicyEvaluation(policy.Policy):\n",
        "    def __init__(self, network, params):\n",
        "        game = pyspiel.load_game('leduc_poker')\n",
        "        all_players = list(range(game.num_players()))\n",
        "        \n",
        "        super().__init__(game, all_players)\n",
        "        self.network = network # actor network\n",
        "        self.params = params\n",
        "    \n",
        "    def action_probabilities(self, state):\n",
        "        current_player = state.current_player()\n",
        "        legal_actions = state.legal_actions(current_player)\n",
        "        legal_actions_mask = state.legal_actions_mask(current_player)\n",
        "        info_state_vector = jnp.array( # actor net only takes single player info_state\n",
        "            state.information_state_tensor(current_player), dtype=jnp.float32)\n",
        "        \n",
        "        policy = self.network(self.params, None, info_state_vector).Pi_bar\n",
        "        policy = np.where(legal_actions_mask, policy, 10e-20)\n",
        "        policy = jax.nn.softmax(policy)\n",
        "\n",
        "        return {action: policy[action] for action in range(len(policy))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xbW5eII37tl9"
      },
      "outputs": [],
      "source": [
        "class ARMAC(rl_agent.AbstractAgent):\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 layers = [256, 128],\n",
        "                 num_epochs = 100,\n",
        "                 num_episodes = 5000,\n",
        "                 min_steps_per_epoch = 5000,\n",
        "                 learning_steps = 100,\n",
        "                 learning_rate = 0.001,\n",
        "                 batch_size = 64,\n",
        "                 trajectory_length = 32,\n",
        "                 update_target_params_every = 1000,\n",
        "                 network_buffer_max_len = 1024,\n",
        "                 critic_loss = 'mse',\n",
        "                 mean_policy_loss = 'cross_entropy',\n",
        "                 gamma= 0.99):\n",
        "        self._rngkey = jax.random.PRNGKey(42)\n",
        "        \n",
        "        # Environment\n",
        "        self._env = env\n",
        "        self._dummy_time_step = env.reset()\n",
        "        self._dummy_history = self._dummy_time_step.observations['info_state'] # note open speil's use of 'info_state' == use of 'history' in ARMAC\n",
        "        self._num_players = env.num_players\n",
        "        self._player_iter = range(self._num_players)\n",
        "        self._num_actions = env.action_spec()['num_actions']\n",
        "        self._info_state_shape = env.observation_spec()['info_state']\n",
        "        \n",
        "        # Network\n",
        "        self._learning_rate = learning_rate\n",
        "        self._network_layers = layers\n",
        "        self._network_input = jnp.array(self._dummy_history)\n",
        "        self._network_input_shape = np.array(self._network_input).shape\n",
        "        self._update_target_params_every = update_target_params_every\n",
        "        self._gamma = gamma\n",
        "\n",
        "        # Initialize 'main()' method variables\n",
        "        self._num_epochs = num_epochs\n",
        "        self._num_episodes = num_episodes\n",
        "        self._min_steps_per_epoch = min_steps_per_epoch\n",
        "        self._player = 1\n",
        "        self._learning_steps = learning_steps\n",
        "        self._batch_size = batch_size\n",
        "        self._trajectory_len = trajectory_length\n",
        "        self._prev_history = self._dummy_history\n",
        "\n",
        "        # Inititialize Buffers\n",
        "        self._network_buffer = NetworkBuffer(network_buffer_max_len)\n",
        "        self._replay_buffer = self._reset_replay_buffer()\n",
        "        self._reset_network_and_optimizer()\n",
        "        \n",
        "        # Jit network update and forward pass\n",
        "        self._jitted_update_step = self._get_jitted_update_step()\n",
        "        self._jitted_matched_regrets = self._get_jitted_matched_regrets()\n",
        "\n",
        "        # Losses (critic, estimated advantage, mean policy)\n",
        "        self._critic_loss = optax.huber_loss if critic_loss == 'huber' else optax.l2_loss \n",
        "        self._adv_loss = optax.l2_loss\n",
        "        if mean_policy_loss == 'mse':\n",
        "            self._mean_policy_loss = optax.l2_loss\n",
        "        elif mean_policy_loss == 'cross_entropy':\n",
        "            self._mean_policy_loss = optax.softmax_cross_entropy\n",
        "        else:\n",
        "            raise ValueError(f'{mean_policy_loss} note supported.\\\n",
        "             Please select from [mse, cross_entropy]')\n",
        "            \n",
        "        # Experiment tracking\n",
        "        self._nash_convs = []\n",
        "        self._losses = {'actor':[], 'critic':[]}\n",
        "        self._total_train_steps = 0\n",
        "\n",
        "        self.game = pyspiel.load_game('leduc_poker')\n",
        "    \n",
        "    def _next_rng_key(self):\n",
        "        \"\"\"Get the next rng subkey from class rngkey.\"\"\"\n",
        "        self._rngkey, subkey = jax.random.split(self._rngkey)\n",
        "        return subkey\n",
        "\n",
        "    def _reset_network_and_optimizer(self):\n",
        "        def armac_network():\n",
        "            actor = PlayerNetwork(self._network_layers, self._num_actions, name='actor')\n",
        "            critic = CriticNetwork(self._network_layers[1], self._num_actions, name='critic')\n",
        "            def init(history):\n",
        "                actor0_head = actor(history[0])\n",
        "                actor1_head = actor(history[1])\n",
        "                critic_head = critic(history)\n",
        "                p0_out = PlayerOuput(avg_regret=actor0_head.W_bar,\n",
        "                                    mean_policy=actor0_head.Pi_bar,\n",
        "                                    q_values=critic_head[0])\n",
        "                p1_out = PlayerOuput(avg_regret=actor1_head.W_bar,\n",
        "                                    mean_policy=actor1_head.Pi_bar,\n",
        "                                    q_values=critic_head[1])\n",
        "                return p0_out, p1_out\n",
        "            return init, (init, actor, critic)\n",
        "        # initialize network\n",
        "        network = hk.multi_transform(armac_network)\n",
        "        self._current_params = network.init(self._next_rng_key(), self._network_input)\n",
        "        self._target_params = network.init(self._next_rng_key(), self._network_input)\n",
        "        self._network, self._actor_net, self._critic_net = network.apply\n",
        "        \n",
        "        # initialize optimizer\n",
        "        self._opt_init, self._opt_update = optax.adam(self._learning_rate)\n",
        "        self._opt_state = self._opt_init(self._current_params)\n",
        "\n",
        "    def _get_jitted_matched_regrets(self):\n",
        "        \n",
        "        @partial(jax.jit, static_argnums=1)\n",
        "        def matched_regrets(params, player, legal_actions_mask, history, rng):\n",
        "            output = self._network(params, rng, jnp.array(history).astype(jnp.float32))[player]\n",
        "            advs = output.avg_regret * legal_actions_mask\n",
        "            advantages = jax.nn.relu(advs) # legal_actions_mask\n",
        "            summed_regrets = jnp.sum(advantages)\n",
        "            matched_regrets = jax.lax.cond(\n",
        "                summed_regrets > 0, lambda _: advantages / summed_regrets,\n",
        "                lambda _: jnp.ones((self._num_actions,)) / self._num_actions, None)\n",
        "\n",
        "            return advantages, matched_regrets, output.q_values\n",
        "\n",
        "        return matched_regrets\n",
        "\n",
        "    def _get_j_params(self):\n",
        "        \"\"\" Samples previous network at iteration j that is used by opponent(s) \"\"\"\n",
        "        network_buffer_length = len(self._network_buffer)\n",
        "        if network_buffer_length == 0:\n",
        "            self._j = 0\n",
        "            return self._current_params\n",
        "        self._j = np.random.randint(0, network_buffer_length) # epoch is zero indexed i.e. (j != T)\n",
        "        return self._network_buffer[self._j]\n",
        "\n",
        "    def _get_exploratory_params(self):\n",
        "        \"\"\" Selects current epoch with 50% probability or uniformly random\n",
        "            selects previous epoch params. \"\"\"\n",
        "        network_buffer_length = len(self._network_buffer)\n",
        "        if network_buffer_length == 0:\n",
        "            return self._current_params\n",
        "        if np.random.rand() >= .50:\n",
        "            return self._current_params\n",
        "        else:\n",
        "            idx = np.random.randint(0, network_buffer_length)\n",
        "            return self._network_buffer[idx]\n",
        "            \n",
        "    def _get_adv_derived_policy(self, params, player: int, legal_action_mask, history):\n",
        "        advantages, adv_derived_policy, q_values = self._jitted_matched_regrets(params, player, legal_action_mask, history, self._next_rng_key())\n",
        "        adv_derived_policy = np.array(adv_derived_policy) * legal_action_mask\n",
        "        adv_derived_policy /= adv_derived_policy.sum() \n",
        "        return adv_derived_policy, q_values\n",
        "    \n",
        "    def _epsilon_greedy(self, policy, epsilon, legal_actions):\n",
        "        \"\"\" There are 4 possible mutations made to the params selected by \n",
        "        '_get_exploratory_params() method.'\n",
        "            i) uniform random policy\n",
        "           ii) policies derived by q_{\\theta}^t(h, a) - h_{\\theta}^t(h)\n",
        "               for the current epoch plus levels of exploration\n",
        "          iii) policies defined by mean regret \\bar{W} plus levels\n",
        "               of exploration\n",
        "           iv) average policy (acotor network output)\"\"\"\n",
        "        \n",
        "        probs = np.zeros(self._num_actions)\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(legal_actions)\n",
        "            probs[legal_actions] = 1.0 / len(legal_actions)\n",
        "        else:\n",
        "            # should this be traditional epislon greedy where we take argmax?\n",
        "            # action = np.random.choice(range(self._num_actions), p=policy) \n",
        "            # probs = policy\n",
        "            action = np.argmax(policy) # np.random.choice(range(self._num_actions), p=policy)\n",
        "            probs = policy\n",
        "        return action, probs\n",
        "\n",
        "    def _sample_action_from_advatange(self, params, player: int, legal_actions, legal_action_mask, history):\n",
        "        policy, _ = self._get_adv_derived_policy(params, player, legal_action_mask, history)\n",
        "        action, policy = self._epsilon_greedy(policy, 0.05, legal_actions)\n",
        "        return action, policy\n",
        "\n",
        "    def _get_regrets(self, params, player, legal_action_mask, history):\n",
        "        policy_j, q_values_j = self._get_adv_derived_policy(params, player, legal_action_mask, history)\n",
        "        policy_ev = np.sum(q_values_j * policy_j)\n",
        "        regrets = ((q_values_j - policy_ev) * legal_action_mask) if player == self._player else None # if i == tau(s)\n",
        "        return regrets, policy_j\n",
        "    \n",
        "    def _rollout_episode(self):\n",
        "        time_step = self._env.reset()\n",
        "        while True:\n",
        "            current_player = time_step.observations[\"current_player\"]\n",
        "            agent_output = self.step(time_step)\n",
        "            time_step = self._env.step([agent_output.action])\n",
        "            \n",
        "            if time_step.last():\n",
        "                self._replay_buffer['rewards'][-1] = time_step.rewards \n",
        "                break\n",
        "    \n",
        "    def main(self):\n",
        "        for epoch in range(self._num_epochs):\n",
        "            self._reset_network_and_optimizer()\n",
        "            self._reset_replay_buffer()\n",
        "            for episode in range(self._num_episodes):\n",
        "                self._player = (self._player + 1) % self._num_players\n",
        "                self._j_params = self._get_j_params()\n",
        "                self._behavior_params = self._get_exploratory_params()\n",
        "                self._rollout_episode()\n",
        "                \n",
        "                # what was the criteria used in original implementation\n",
        "                if len(self._replay_buffer['i']) > self._min_steps_per_epoch:\n",
        "                    print('broke', len(self._replay_buffer['i']))\n",
        "                    break\n",
        "            \n",
        "            self._prepare_buffer()\n",
        "            for learning_step in range(self._learning_steps):\n",
        "                actor_loss, critic_loss = self._learn_step()\n",
        "                if self._total_train_steps % 25 == 0:\n",
        "                    print(f'epoch: {epoch} | step: {learning_step} | actor: {actor_loss} | critic: {critic_loss} | update steps:{self._total_train_steps}')\n",
        "\n",
        "            self._network_buffer.add(deepcopy(self._current_params))\n",
        "            \n",
        "            eval_policy = MeanPolicyEvaluation(self._actor_net, self._current_params)\n",
        "            conv = exploitability.nash_conv(self.game, policy.python_policy_to_pyspiel_policy(policy.tabular_policy_from_callable(self.game, eval_policy.action_probabilities)))\n",
        "            print(epoch, conv)\n",
        "            \n",
        "            # logging \n",
        "            self._nash_convs.append(conv)\n",
        "            self._losses['actor'].append(actor_loss)\n",
        "            self._losses['actor'].append(critic_loss)\n",
        "\n",
        "            del eval_policy, self._replay_buffer\n",
        "\n",
        "           \n",
        "\n",
        "    def _get_legal_actions_mask(self, legal_actions):\n",
        "        legal_actions_mask = np.zeros(self._num_actions)\n",
        "        legal_actions_mask[legal_actions] = 1.0\n",
        "        return legal_actions_mask\n",
        "\n",
        "    def step(self, time_step):\n",
        "        \"\"\" Processes a single time step in a trajectory that is added to\n",
        "        replay buffer for training network.\n",
        "        \n",
        "        Args:\n",
        "            time_step: an instance of rl_environment.TimeStep.\n",
        "        \n",
        "        Returns:\n",
        "            A `rl_agent.StepOutput` containing the action probs and chosen action.\n",
        "        \"\"\"\n",
        "        current_player = time_step.observations['current_player']\n",
        "        history = time_step.observations['info_state']\n",
        "        info_state = history[current_player]\n",
        "        legal_actions = time_step.observations['legal_actions'][current_player]\n",
        "        legal_actions_mask = self._get_legal_actions_mask(legal_actions)\n",
        "        discount = 0 if time_step.step_type == StepType.FIRST else self._gamma # we don't want to bootstrap across episode boundries\n",
        "\n",
        "        regrets, policy_j = self._get_regrets(self._j_params, current_player, legal_actions_mask, history) # always using j params\n",
        "        behavior_params = self._behavior_params if current_player == self._player else self._j_params\n",
        "        action, probs = self._sample_action_from_advatange(behavior_params, current_player, legal_actions, legal_actions_mask, history)\n",
        "        agent_output = rl_agent.StepOutput(action=action, probs=probs)\n",
        "        \n",
        "        self._add_transition(history, deepcopy(self._prev_history), info_state, action, legal_actions_mask, current_player, regrets, policy_j, discount, time_step.rewards)\n",
        "        self._prev_history = history\n",
        "        \n",
        "        return agent_output\n",
        "\n",
        "    def _add_transition(self, history, prev_history, info_state, action, legal_action_mask, acting_player, regrets, policy_j, discount, rewards):\n",
        "        self._replay_buffer['i'].append(deepcopy(self._player))\n",
        "        self._replay_buffer['j'].append(deepcopy(self._j))\n",
        "        self._replay_buffer['history'].append(history)\n",
        "        self._replay_buffer['prev_history'].append(prev_history)\n",
        "        self._replay_buffer['info_state'].append(info_state)\n",
        "        self._replay_buffer['action'].append(action)\n",
        "        self._replay_buffer['legal_actions_mask'].append(legal_action_mask)\n",
        "        self._replay_buffer['acting_player'].append(acting_player)\n",
        "        self._replay_buffer['regret'].append(list(regrets)) if regrets is not None \\\n",
        "         else self._replay_buffer['regret'].append(jnp.array([None for _ in range(self._num_actions)]))\n",
        "        self._replay_buffer['policy_j'].append(list(policy_j))\n",
        "        self._replay_buffer['discount'].append(discount)\n",
        "        self._replay_buffer['rewards'].append(rewards if rewards is not None else [0. for _ in range(self._num_players)])\n",
        "        \n",
        "    def _reset_replay_buffer(self): \n",
        "        self._replay_buffer = {'i': [],\n",
        "                     'j': [],\n",
        "                     'history': [],\n",
        "                     'prev_history': [],\n",
        "                     'info_state': [],\n",
        "                     'action': [],\n",
        "                     'legal_actions_mask': [],\n",
        "                     'acting_player': [],\n",
        "                     'regret': [],\n",
        "                     'policy_j': [],\n",
        "                     'discount': [],\n",
        "                     'rewards': []\n",
        "                     }\n",
        "    \n",
        "    def _prepare_buffer(self):\n",
        "        \"\"\" Converts replay_buffer of type dict to a chex.dataclass to simplfy\n",
        "            use of Jax primitives \"\"\"\n",
        "        def set_type(k, v):\n",
        "            keep_types = ['action', 'acting_player']\n",
        "            return jnp.array(v).astype(jnp.float32) if k not in keep_types else jnp.array(v) \n",
        "        \n",
        "        self._len_of_replay_buffer = len(self._replay_buffer['i'])\n",
        "        replay_buffer = {key: set_type(key, value) for key, value in self._replay_buffer.items()}\n",
        "        self._replay_buffer = JaxFriendlyBuffer(**replay_buffer)\n",
        "\n",
        "    def _sample_from_replay_buffer(self, batched=True):\n",
        "        \"\"\" Samples a batch of trajectories uniformly random from replay buffer \"\"\"\n",
        "        def sample(buffer, index):\n",
        "            sample = jax.tree_map(lambda x: jax.lax.dynamic_slice_in_dim(x, index, self._trajectory_len), buffer)\n",
        "            return sample\n",
        "        \n",
        "        if not batched:\n",
        "            return sample(self._replay_buffer, 0)\n",
        "\n",
        "        index = jax.random.randint(self._next_rng_key(), (self._batch_size,), 0, self._len_of_replay_buffer - self._trajectory_len) \n",
        "        batch = jax.vmap(sample, in_axes=(None, 0))(self._replay_buffer, index)\n",
        "        return batch\n",
        "\n",
        "    def _learn_step(self):\n",
        "        batch = self._sample_from_replay_buffer() #(64, 28, attr.shape)\n",
        "        self._current_params, self._opt_state, actor_loss, critic_loss =  self._jitted_update_step(self._current_params, self._target_params, self._opt_state, batch)\n",
        "        self._total_train_steps += 1\n",
        "\n",
        "        if self._total_train_steps % self._update_target_params_every == 0:\n",
        "            self._target_params = jax.tree_map(lambda x: x.copy(), self._current_params)\n",
        "            print('target params updated')\n",
        "        return actor_loss, critic_loss\n",
        "\n",
        "    def _get_jitted_update_step(self):\n",
        "        \n",
        "        def actor_loss_fn(params, data):  \n",
        "            def advantage_loss(preds, labels):\n",
        "                loss = self._adv_loss(preds.W_bar, labels.regret)\n",
        "                return loss.mean()\n",
        "            def policy_loss(logits, data): \n",
        "                preds = jnp.where(data.legal_actions_mask, logits.Pi_bar, -10e20)\n",
        "                labels = jax.nn.one_hot(jnp.argmax(data.policy_j), len(data.policy_j))\n",
        "                loss = self._mean_policy_loss(preds, labels) \n",
        "                return loss.mean()\n",
        "\n",
        "            preds = self._actor_net(params, None, data.info_state) \n",
        "            loss = jax.lax.cond(data.i == data.acting_player, advantage_loss, policy_loss, preds, data) # train W head when i == tau(s) else train policy head\n",
        "            return loss\n",
        "\n",
        "        def critic_loss_fn(params, target_params, data):\n",
        "            acting_player = data.acting_player\n",
        "            q_tm1 = self._critic_net(params, None, data.prev_history)[acting_player]\n",
        "            q_t = self._critic_net(target_params, None, data.history)[acting_player]\n",
        "            max_q_t = jnp.max(q_t + (1 - data.legal_actions_mask) * -1e9) \n",
        "            q_tm1_target = data.rewards[acting_player] + data.discount * max_q_t \n",
        "            q_tm1_target = jax.lax.stop_gradient(q_tm1_target)\n",
        "            loss = self._critic_loss(q_tm1[data.action], q_tm1_target)\n",
        "            return loss \n",
        "\n",
        "        def network_grads(params, target_params, data):\n",
        "            actor_loss, actor_grads = jax.vmap(\n",
        "                jax.vmap(jax.value_and_grad(actor_loss_fn), in_axes=(None, 0)),\n",
        "                 in_axes=(None, 0))(params, data) \n",
        "            critic_loss, critic_grads = jax.vmap(\n",
        "                jax.vmap(jax.value_and_grad(critic_loss_fn), in_axes=(None, None, 0)),\n",
        "                         in_axes=(None, None, 0))(params, target_params, data) \n",
        "            # split gradients\n",
        "            actor_grads, _ = hk.data_structures.partition(lambda module_name, n, v: 'actor' in module_name, actor_grads)\n",
        "            critic_grads, _ = hk.data_structures.partition(lambda module_name, n, v: 'critic' in module_name, critic_grads)\n",
        "            # mean across batch and time dim\n",
        "            actor_grads = jax.tree_map(lambda x: jnp.mean(x, axis=(0,1)), actor_grads)\n",
        "            critic_grads = jax.tree_map(lambda x: jnp.mean(x, axis=(0,1)), critic_grads)\n",
        "            # join grads\n",
        "            grads = hk.data_structures.merge(actor_grads, critic_grads)\n",
        "            return actor_loss.mean(), critic_loss.mean(), grads\n",
        "        \n",
        "        @jax.jit\n",
        "        def update_step(params, target_params, opt_state, data):        \n",
        "            actor_loss, critic_loss, grads = network_grads(params, target_params, data)\n",
        "            updates, new_opt_state = self._opt_update(grads, opt_state)\n",
        "            new_params = optax.apply_updates(params, updates)\n",
        "            return new_params, new_opt_state, actor_loss, critic_loss\n",
        "        return update_step \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mfBBZSHV2ub-"
      },
      "outputs": [],
      "source": [
        "layers = [256, 128]\n",
        "num_epochs = 1000\n",
        "num_episodes = 5000\n",
        "learning_steps = 200\n",
        "learning_rate = 0.00001\n",
        "batch_size = 64\n",
        "trajectory_length = 32\n",
        "update_target_params_every = 1000\n",
        "\n",
        "env = rl_environment.Environment(\"leduc_poker\")\n",
        "\n",
        "armac = ARMAC(env=env,\n",
        "              layers=layers,\n",
        "              num_epochs=num_epochs,\n",
        "              num_episodes=num_episodes,\n",
        "              learning_steps=learning_steps,\n",
        "              learning_rate=learning_rate,\n",
        "              batch_size=batch_size,\n",
        "              trajectory_length=trajectory_length,\n",
        "              update_target_params_every=update_target_params_every)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "543ek4P88WwM",
        "outputId": "d5f972ad-c330-480a-b84c-3b517ef8f23e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "broke 5001\n",
            "epoch: 0 | step: 24 | actor: 0.372161865234375 | critic: 1.8039942979812622 | update steps:25\n",
            "epoch: 0 | step: 49 | actor: 0.3565458059310913 | critic: 1.740241289138794 | update steps:50\n",
            "epoch: 0 | step: 74 | actor: 0.3368304967880249 | critic: 1.595205307006836 | update steps:75\n",
            "epoch: 0 | step: 99 | actor: 0.3109152317047119 | critic: 1.4693701267242432 | update steps:100\n",
            "epoch: 0 | step: 124 | actor: 0.28830036520957947 | critic: 1.46065354347229 | update steps:125\n",
            "epoch: 0 | step: 149 | actor: 0.27051180601119995 | critic: 1.4654654264450073 | update steps:150\n",
            "epoch: 0 | step: 174 | actor: 0.25159764289855957 | critic: 1.3199143409729004 | update steps:175\n",
            "epoch: 0 | step: 199 | actor: 0.24760717153549194 | critic: 1.2429791688919067 | update steps:200\n",
            "0 4.605663200995976\n",
            "broke 5002\n",
            "epoch: 1 | step: 24 | actor: 0.4404430389404297 | critic: 1.2658119201660156 | update steps:225\n",
            "epoch: 1 | step: 49 | actor: 0.4251537621021271 | critic: 1.5270321369171143 | update steps:250\n",
            "epoch: 1 | step: 74 | actor: 0.40346434712409973 | critic: 1.363425612449646 | update steps:275\n",
            "epoch: 1 | step: 99 | actor: 0.39880305528640747 | critic: 1.4848463535308838 | update steps:300\n",
            "epoch: 1 | step: 124 | actor: 0.39234331250190735 | critic: 1.3055765628814697 | update steps:325\n",
            "epoch: 1 | step: 149 | actor: 0.3726285696029663 | critic: 1.3351166248321533 | update steps:350\n",
            "epoch: 1 | step: 174 | actor: 0.36718878149986267 | critic: 1.2714974880218506 | update steps:375\n",
            "epoch: 1 | step: 199 | actor: 0.35150665044784546 | critic: 1.2240753173828125 | update steps:400\n",
            "1 3.2837905096906677\n",
            "broke 5006\n",
            "epoch: 2 | step: 24 | actor: 0.44376683235168457 | critic: 2.931058406829834 | update steps:425\n",
            "epoch: 2 | step: 49 | actor: 0.4289619028568268 | critic: 2.813711404800415 | update steps:450\n",
            "epoch: 2 | step: 74 | actor: 0.4186830520629883 | critic: 2.8640217781066895 | update steps:475\n",
            "epoch: 2 | step: 99 | actor: 0.4101274609565735 | critic: 3.1506335735321045 | update steps:500\n",
            "epoch: 2 | step: 124 | actor: 0.39478635787963867 | critic: 2.8150153160095215 | update steps:525\n",
            "epoch: 2 | step: 149 | actor: 0.38803935050964355 | critic: 2.8419055938720703 | update steps:550\n",
            "epoch: 2 | step: 174 | actor: 0.3789002299308777 | critic: 2.4583892822265625 | update steps:575\n",
            "epoch: 2 | step: 199 | actor: 0.37234175205230713 | critic: 2.847233295440674 | update steps:600\n",
            "2 2.751196465963213\n",
            "broke 5001\n",
            "epoch: 3 | step: 24 | actor: 0.38314035534858704 | critic: 3.228940010070801 | update steps:625\n",
            "epoch: 3 | step: 49 | actor: 0.3765488266944885 | critic: 3.153453826904297 | update steps:650\n",
            "epoch: 3 | step: 74 | actor: 0.3685460686683655 | critic: 2.931553363800049 | update steps:675\n",
            "epoch: 3 | step: 99 | actor: 0.3619479537010193 | critic: 3.3844690322875977 | update steps:700\n",
            "epoch: 3 | step: 124 | actor: 0.3550686240196228 | critic: 3.1473162174224854 | update steps:725\n",
            "epoch: 3 | step: 149 | actor: 0.3536239564418793 | critic: 3.2862613201141357 | update steps:750\n",
            "epoch: 3 | step: 174 | actor: 0.34383004903793335 | critic: 2.981750726699829 | update steps:775\n",
            "epoch: 3 | step: 199 | actor: 0.34665781259536743 | critic: 3.071493625640869 | update steps:800\n",
            "3 2.990896276583253\n",
            "broke 5004\n",
            "epoch: 4 | step: 24 | actor: 0.38308537006378174 | critic: 1.843397617340088 | update steps:825\n",
            "epoch: 4 | step: 49 | actor: 0.37763214111328125 | critic: 2.0669069290161133 | update steps:850\n",
            "epoch: 4 | step: 74 | actor: 0.361383318901062 | critic: 1.6913723945617676 | update steps:875\n",
            "epoch: 4 | step: 99 | actor: 0.36413857340812683 | critic: 1.723354458808899 | update steps:900\n",
            "epoch: 4 | step: 124 | actor: 0.3614691197872162 | critic: 1.7479517459869385 | update steps:925\n",
            "epoch: 4 | step: 149 | actor: 0.35267096757888794 | critic: 1.9941388368606567 | update steps:950\n",
            "epoch: 4 | step: 174 | actor: 0.3591832220554352 | critic: 1.743054747581482 | update steps:975\n",
            "target params updated\n",
            "epoch: 4 | step: 199 | actor: 0.34970492124557495 | critic: 2.0083489418029785 | update steps:1000\n",
            "4 2.6813007023980155\n",
            "broke 5002\n",
            "epoch: 5 | step: 24 | actor: 0.3713618516921997 | critic: 3.1606388092041016 | update steps:1025\n",
            "epoch: 5 | step: 49 | actor: 0.36440032720565796 | critic: 3.0833258628845215 | update steps:1050\n",
            "epoch: 5 | step: 74 | actor: 0.37277594208717346 | critic: 3.1575026512145996 | update steps:1075\n",
            "epoch: 5 | step: 99 | actor: 0.3743080496788025 | critic: 3.198361873626709 | update steps:1100\n",
            "epoch: 5 | step: 124 | actor: 0.3592368960380554 | critic: 3.19549560546875 | update steps:1125\n",
            "epoch: 5 | step: 149 | actor: 0.3532910943031311 | critic: 3.2256014347076416 | update steps:1150\n",
            "epoch: 5 | step: 174 | actor: 0.35538387298583984 | critic: 3.0266003608703613 | update steps:1175\n",
            "epoch: 5 | step: 199 | actor: 0.35339149832725525 | critic: 3.1407313346862793 | update steps:1200\n",
            "5 2.7645377897764485\n",
            "broke 5001\n",
            "epoch: 6 | step: 24 | actor: 0.39913254976272583 | critic: 2.8446176052093506 | update steps:1225\n",
            "epoch: 6 | step: 49 | actor: 0.38596636056900024 | critic: 2.7136499881744385 | update steps:1250\n",
            "epoch: 6 | step: 74 | actor: 0.38586848974227905 | critic: 2.542142868041992 | update steps:1275\n",
            "epoch: 6 | step: 99 | actor: 0.38320451974868774 | critic: 2.658579111099243 | update steps:1300\n",
            "epoch: 6 | step: 124 | actor: 0.37487053871154785 | critic: 2.6729164123535156 | update steps:1325\n",
            "epoch: 6 | step: 149 | actor: 0.37231016159057617 | critic: 2.4052443504333496 | update steps:1350\n",
            "epoch: 6 | step: 174 | actor: 0.367431640625 | critic: 2.733433723449707 | update steps:1375\n",
            "epoch: 6 | step: 199 | actor: 0.36000901460647583 | critic: 2.701673984527588 | update steps:1400\n",
            "6 2.8249635974638476\n",
            "broke 5006\n",
            "epoch: 7 | step: 24 | actor: 0.4461151659488678 | critic: 3.0358874797821045 | update steps:1425\n",
            "epoch: 7 | step: 49 | actor: 0.4343757629394531 | critic: 2.5524230003356934 | update steps:1450\n",
            "epoch: 7 | step: 74 | actor: 0.43138566613197327 | critic: 2.847576379776001 | update steps:1475\n",
            "epoch: 7 | step: 99 | actor: 0.4199681580066681 | critic: 2.5684258937835693 | update steps:1500\n",
            "epoch: 7 | step: 124 | actor: 0.41207221150398254 | critic: 2.9696543216705322 | update steps:1525\n",
            "epoch: 7 | step: 149 | actor: 0.41144704818725586 | critic: 2.6829731464385986 | update steps:1550\n",
            "epoch: 7 | step: 174 | actor: 0.4026252329349518 | critic: 2.796025276184082 | update steps:1575\n",
            "epoch: 7 | step: 199 | actor: 0.3979034125804901 | critic: 2.576514482498169 | update steps:1600\n",
            "7 2.7331644373905553\n",
            "broke 5006\n",
            "epoch: 8 | step: 24 | actor: 0.43426063656806946 | critic: 3.1294355392456055 | update steps:1625\n",
            "epoch: 8 | step: 49 | actor: 0.4197092652320862 | critic: 2.703484535217285 | update steps:1650\n",
            "epoch: 8 | step: 74 | actor: 0.4234149158000946 | critic: 2.8285083770751953 | update steps:1675\n",
            "epoch: 8 | step: 99 | actor: 0.407323956489563 | critic: 2.9561123847961426 | update steps:1700\n",
            "epoch: 8 | step: 124 | actor: 0.41752076148986816 | critic: 2.847029685974121 | update steps:1725\n",
            "epoch: 8 | step: 149 | actor: 0.41072019934654236 | critic: 2.8339927196502686 | update steps:1750\n",
            "epoch: 8 | step: 174 | actor: 0.38925880193710327 | critic: 2.8744325637817383 | update steps:1775\n",
            "epoch: 8 | step: 199 | actor: 0.39974546432495117 | critic: 2.619645357131958 | update steps:1800\n",
            "8 2.8475239505973757\n",
            "broke 5006\n",
            "epoch: 9 | step: 24 | actor: 0.3946956694126129 | critic: 3.344907283782959 | update steps:1825\n",
            "epoch: 9 | step: 49 | actor: 0.4059019386768341 | critic: 2.987691879272461 | update steps:1850\n",
            "epoch: 9 | step: 74 | actor: 0.3867456614971161 | critic: 3.4907851219177246 | update steps:1875\n",
            "epoch: 9 | step: 99 | actor: 0.3751204013824463 | critic: 3.0541515350341797 | update steps:1900\n",
            "epoch: 9 | step: 124 | actor: 0.37933969497680664 | critic: 3.3622725009918213 | update steps:1925\n",
            "epoch: 9 | step: 149 | actor: 0.37392640113830566 | critic: 3.0086989402770996 | update steps:1950\n",
            "epoch: 9 | step: 174 | actor: 0.3775000274181366 | critic: 3.15267276763916 | update steps:1975\n",
            "target params updated\n",
            "epoch: 9 | step: 199 | actor: 0.3660382628440857 | critic: 3.2259020805358887 | update steps:2000\n",
            "9 2.877075337150995\n",
            "broke 5002\n",
            "epoch: 10 | step: 24 | actor: 0.3751126825809479 | critic: 2.1724560260772705 | update steps:2025\n",
            "epoch: 10 | step: 49 | actor: 0.3770957887172699 | critic: 2.0750064849853516 | update steps:2050\n",
            "epoch: 10 | step: 74 | actor: 0.36943066120147705 | critic: 1.9745756387710571 | update steps:2075\n",
            "epoch: 10 | step: 99 | actor: 0.37146562337875366 | critic: 1.9821476936340332 | update steps:2100\n",
            "epoch: 10 | step: 124 | actor: 0.3659311532974243 | critic: 2.1153202056884766 | update steps:2125\n",
            "epoch: 10 | step: 149 | actor: 0.35808950662612915 | critic: 1.7618626356124878 | update steps:2150\n",
            "epoch: 10 | step: 174 | actor: 0.3529362678527832 | critic: 2.116058349609375 | update steps:2175\n",
            "epoch: 10 | step: 199 | actor: 0.35476186871528625 | critic: 2.238582134246826 | update steps:2200\n",
            "10 3.025120723692314\n",
            "broke 5006\n",
            "epoch: 11 | step: 24 | actor: 0.41095632314682007 | critic: 2.6878931522369385 | update steps:2225\n",
            "epoch: 11 | step: 49 | actor: 0.41171571612358093 | critic: 3.1194825172424316 | update steps:2250\n",
            "epoch: 11 | step: 74 | actor: 0.40020135045051575 | critic: 2.812070369720459 | update steps:2275\n",
            "epoch: 11 | step: 99 | actor: 0.39995741844177246 | critic: 2.7706573009490967 | update steps:2300\n",
            "epoch: 11 | step: 124 | actor: 0.3906261622905731 | critic: 2.668886661529541 | update steps:2325\n",
            "epoch: 11 | step: 149 | actor: 0.3927992582321167 | critic: 2.8186187744140625 | update steps:2350\n",
            "epoch: 11 | step: 174 | actor: 0.3821451663970947 | critic: 2.563450336456299 | update steps:2375\n",
            "epoch: 11 | step: 199 | actor: 0.38691413402557373 | critic: 2.488084316253662 | update steps:2400\n",
            "11 2.859331601788859\n",
            "broke 5004\n",
            "epoch: 12 | step: 24 | actor: 0.4286026954650879 | critic: 3.1064910888671875 | update steps:2425\n",
            "epoch: 12 | step: 49 | actor: 0.4170514941215515 | critic: 3.0294418334960938 | update steps:2450\n",
            "epoch: 12 | step: 74 | actor: 0.4109683930873871 | critic: 3.0130505561828613 | update steps:2475\n",
            "epoch: 12 | step: 99 | actor: 0.4050672650337219 | critic: 3.2423489093780518 | update steps:2500\n",
            "epoch: 12 | step: 124 | actor: 0.4020771384239197 | critic: 2.8699703216552734 | update steps:2525\n",
            "epoch: 12 | step: 149 | actor: 0.4082843065261841 | critic: 2.861288070678711 | update steps:2550\n",
            "epoch: 12 | step: 174 | actor: 0.394830584526062 | critic: 2.8744072914123535 | update steps:2575\n",
            "epoch: 12 | step: 199 | actor: 0.38333040475845337 | critic: 3.1362485885620117 | update steps:2600\n",
            "12 2.7898380965040523\n",
            "broke 5003\n",
            "epoch: 13 | step: 24 | actor: 0.38594189286231995 | critic: 2.2921290397644043 | update steps:2625\n",
            "epoch: 13 | step: 49 | actor: 0.3881172239780426 | critic: 2.3240439891815186 | update steps:2650\n",
            "epoch: 13 | step: 74 | actor: 0.3733755350112915 | critic: 2.5904855728149414 | update steps:2675\n",
            "epoch: 13 | step: 99 | actor: 0.3822686970233917 | critic: 2.2471089363098145 | update steps:2700\n",
            "epoch: 13 | step: 124 | actor: 0.3728586435317993 | critic: 2.5379581451416016 | update steps:2725\n",
            "epoch: 13 | step: 149 | actor: 0.3690645396709442 | critic: 2.1348655223846436 | update steps:2750\n",
            "epoch: 13 | step: 174 | actor: 0.37048250436782837 | critic: 2.2683520317077637 | update steps:2775\n",
            "epoch: 13 | step: 199 | actor: 0.3606010973453522 | critic: 2.518886089324951 | update steps:2800\n",
            "13 3.0002322149345\n",
            "broke 5004\n",
            "epoch: 14 | step: 24 | actor: 0.4465833902359009 | critic: 3.097097873687744 | update steps:2825\n",
            "epoch: 14 | step: 49 | actor: 0.44081932306289673 | critic: 2.8849234580993652 | update steps:2850\n",
            "epoch: 14 | step: 74 | actor: 0.4356362223625183 | critic: 3.0933678150177 | update steps:2875\n",
            "epoch: 14 | step: 99 | actor: 0.42504528164863586 | critic: 2.9357824325561523 | update steps:2900\n",
            "epoch: 14 | step: 124 | actor: 0.4177579879760742 | critic: 3.153959035873413 | update steps:2925\n",
            "epoch: 14 | step: 149 | actor: 0.41663995385169983 | critic: 3.123244285583496 | update steps:2950\n",
            "epoch: 14 | step: 174 | actor: 0.4064823389053345 | critic: 2.930880546569824 | update steps:2975\n",
            "target params updated\n",
            "epoch: 14 | step: 199 | actor: 0.410645067691803 | critic: 2.8229074478149414 | update steps:3000\n",
            "14 2.7451998297120994\n",
            "broke 5004\n",
            "epoch: 15 | step: 24 | actor: 0.37319818139076233 | critic: 3.0542614459991455 | update steps:3025\n",
            "epoch: 15 | step: 49 | actor: 0.36132892966270447 | critic: 3.1549642086029053 | update steps:3050\n",
            "epoch: 15 | step: 74 | actor: 0.3589285612106323 | critic: 3.2896170616149902 | update steps:3075\n",
            "epoch: 15 | step: 99 | actor: 0.35186856985092163 | critic: 3.501488208770752 | update steps:3100\n",
            "epoch: 15 | step: 124 | actor: 0.35226568579673767 | critic: 3.2264978885650635 | update steps:3125\n",
            "epoch: 15 | step: 149 | actor: 0.3522897958755493 | critic: 2.8954062461853027 | update steps:3150\n",
            "epoch: 15 | step: 174 | actor: 0.355435848236084 | critic: 3.3297603130340576 | update steps:3175\n",
            "epoch: 15 | step: 199 | actor: 0.34812313318252563 | critic: 3.233024835586548 | update steps:3200\n",
            "15 3.1222415393262892\n",
            "broke 5001\n",
            "epoch: 16 | step: 24 | actor: 0.41971004009246826 | critic: 2.7764792442321777 | update steps:3225\n",
            "epoch: 16 | step: 49 | actor: 0.41835254430770874 | critic: 2.9353599548339844 | update steps:3250\n",
            "epoch: 16 | step: 74 | actor: 0.40694576501846313 | critic: 2.7296066284179688 | update steps:3275\n",
            "epoch: 16 | step: 99 | actor: 0.39915531873703003 | critic: 2.6189050674438477 | update steps:3300\n",
            "epoch: 16 | step: 124 | actor: 0.3943467140197754 | critic: 2.714082956314087 | update steps:3325\n",
            "epoch: 16 | step: 149 | actor: 0.3835332691669464 | critic: 2.9803590774536133 | update steps:3350\n",
            "epoch: 16 | step: 174 | actor: 0.38267678022384644 | critic: 2.7093396186828613 | update steps:3375\n",
            "epoch: 16 | step: 199 | actor: 0.37549227476119995 | critic: 2.5385661125183105 | update steps:3400\n",
            "16 3.047765149141571\n",
            "broke 5005\n",
            "epoch: 17 | step: 24 | actor: 0.4047352075576782 | critic: 2.7369399070739746 | update steps:3425\n",
            "epoch: 17 | step: 49 | actor: 0.401542603969574 | critic: 2.402679681777954 | update steps:3450\n",
            "epoch: 17 | step: 74 | actor: 0.39822864532470703 | critic: 2.2981045246124268 | update steps:3475\n",
            "epoch: 17 | step: 99 | actor: 0.3881257474422455 | critic: 2.6349802017211914 | update steps:3500\n",
            "epoch: 17 | step: 124 | actor: 0.38722699880599976 | critic: 2.232004165649414 | update steps:3525\n",
            "epoch: 17 | step: 149 | actor: 0.38554465770721436 | critic: 2.750694513320923 | update steps:3550\n",
            "epoch: 17 | step: 174 | actor: 0.37830865383148193 | critic: 2.6826791763305664 | update steps:3575\n",
            "epoch: 17 | step: 199 | actor: 0.36811429262161255 | critic: 2.529899835586548 | update steps:3600\n",
            "17 2.804427320868527\n",
            "broke 5003\n",
            "epoch: 18 | step: 24 | actor: 0.4007956385612488 | critic: 3.773353099822998 | update steps:3625\n",
            "epoch: 18 | step: 49 | actor: 0.40079593658447266 | critic: 3.6639866828918457 | update steps:3650\n",
            "epoch: 18 | step: 74 | actor: 0.38801664113998413 | critic: 3.200003147125244 | update steps:3675\n",
            "epoch: 18 | step: 99 | actor: 0.39130693674087524 | critic: 3.2465288639068604 | update steps:3700\n",
            "epoch: 18 | step: 124 | actor: 0.3816143870353699 | critic: 3.3327531814575195 | update steps:3725\n",
            "epoch: 18 | step: 149 | actor: 0.37843260169029236 | critic: 3.400223970413208 | update steps:3750\n",
            "epoch: 18 | step: 174 | actor: 0.3751526176929474 | critic: 3.841845989227295 | update steps:3775\n",
            "epoch: 18 | step: 199 | actor: 0.3747352957725525 | critic: 3.5989792346954346 | update steps:3800\n",
            "18 2.9748431558223483\n",
            "broke 5002\n",
            "epoch: 19 | step: 24 | actor: 0.37788283824920654 | critic: 2.4442310333251953 | update steps:3825\n",
            "epoch: 19 | step: 49 | actor: 0.3778563141822815 | critic: 2.1555347442626953 | update steps:3850\n",
            "epoch: 19 | step: 74 | actor: 0.37491416931152344 | critic: 2.6436891555786133 | update steps:3875\n",
            "epoch: 19 | step: 99 | actor: 0.3625563085079193 | critic: 2.0423245429992676 | update steps:3900\n",
            "epoch: 19 | step: 124 | actor: 0.35376691818237305 | critic: 1.9328761100769043 | update steps:3925\n",
            "epoch: 19 | step: 149 | actor: 0.35987910628318787 | critic: 2.2196168899536133 | update steps:3950\n",
            "epoch: 19 | step: 174 | actor: 0.3467761278152466 | critic: 2.302122116088867 | update steps:3975\n",
            "target params updated\n",
            "epoch: 19 | step: 199 | actor: 0.34419089555740356 | critic: 2.3478035926818848 | update steps:4000\n",
            "19 2.89976845075547\n",
            "broke 5003\n",
            "epoch: 20 | step: 24 | actor: 0.4244140386581421 | critic: 2.3779408931732178 | update steps:4025\n",
            "epoch: 20 | step: 49 | actor: 0.40917107462882996 | critic: 2.256058931350708 | update steps:4050\n",
            "epoch: 20 | step: 74 | actor: 0.4129003882408142 | critic: 2.126321315765381 | update steps:4075\n",
            "epoch: 20 | step: 99 | actor: 0.39599549770355225 | critic: 2.2265143394470215 | update steps:4100\n",
            "epoch: 20 | step: 124 | actor: 0.39925065636634827 | critic: 2.463470458984375 | update steps:4125\n",
            "epoch: 20 | step: 149 | actor: 0.39507240056991577 | critic: 2.454399347305298 | update steps:4150\n",
            "epoch: 20 | step: 174 | actor: 0.38407230377197266 | critic: 2.0420451164245605 | update steps:4175\n",
            "epoch: 20 | step: 199 | actor: 0.37615182995796204 | critic: 2.119344711303711 | update steps:4200\n",
            "20 2.8622729215430867\n",
            "broke 5004\n",
            "epoch: 21 | step: 24 | actor: 0.397352397441864 | critic: 2.8308775424957275 | update steps:4225\n",
            "epoch: 21 | step: 49 | actor: 0.3950960040092468 | critic: 2.745743989944458 | update steps:4250\n",
            "epoch: 21 | step: 74 | actor: 0.388451486825943 | critic: 2.745511054992676 | update steps:4275\n",
            "epoch: 21 | step: 99 | actor: 0.3875039219856262 | critic: 2.3375487327575684 | update steps:4300\n",
            "epoch: 21 | step: 124 | actor: 0.38330763578414917 | critic: 2.6622841358184814 | update steps:4325\n",
            "epoch: 21 | step: 149 | actor: 0.37445369362831116 | critic: 2.6954598426818848 | update steps:4350\n",
            "epoch: 21 | step: 174 | actor: 0.3681744337081909 | critic: 2.476447343826294 | update steps:4375\n",
            "epoch: 21 | step: 199 | actor: 0.3709242045879364 | critic: 2.5594687461853027 | update steps:4400\n",
            "21 2.885529891557284\n",
            "broke 5001\n",
            "epoch: 22 | step: 24 | actor: 0.46118974685668945 | critic: 3.467313289642334 | update steps:4425\n",
            "epoch: 22 | step: 49 | actor: 0.45118415355682373 | critic: 3.6179285049438477 | update steps:4450\n",
            "epoch: 22 | step: 74 | actor: 0.43427300453186035 | critic: 3.374051332473755 | update steps:4475\n",
            "epoch: 22 | step: 99 | actor: 0.43215811252593994 | critic: 3.3738341331481934 | update steps:4500\n",
            "epoch: 22 | step: 124 | actor: 0.42905673384666443 | critic: 2.9919822216033936 | update steps:4525\n",
            "epoch: 22 | step: 149 | actor: 0.42286166548728943 | critic: 3.2246971130371094 | update steps:4550\n",
            "epoch: 22 | step: 174 | actor: 0.4104420244693756 | critic: 3.298959970474243 | update steps:4575\n",
            "epoch: 22 | step: 199 | actor: 0.4131525158882141 | critic: 3.278012275695801 | update steps:4600\n",
            "22 2.640781928075997\n",
            "broke 5002\n",
            "epoch: 23 | step: 24 | actor: 0.39399564266204834 | critic: 2.3557143211364746 | update steps:4625\n",
            "epoch: 23 | step: 49 | actor: 0.390170693397522 | critic: 2.6422226428985596 | update steps:4650\n",
            "epoch: 23 | step: 74 | actor: 0.3850082457065582 | critic: 2.364983081817627 | update steps:4675\n",
            "epoch: 23 | step: 99 | actor: 0.3796403706073761 | critic: 2.2451541423797607 | update steps:4700\n",
            "epoch: 23 | step: 124 | actor: 0.37712496519088745 | critic: 2.321432113647461 | update steps:4725\n",
            "epoch: 23 | step: 149 | actor: 0.3694283366203308 | critic: 2.5700159072875977 | update steps:4750\n",
            "epoch: 23 | step: 174 | actor: 0.37359172105789185 | critic: 2.3668346405029297 | update steps:4775\n",
            "epoch: 23 | step: 199 | actor: 0.3655674457550049 | critic: 2.4770119190216064 | update steps:4800\n",
            "23 2.8405374126292693\n",
            "broke 5002\n",
            "epoch: 24 | step: 24 | actor: 0.39758428931236267 | critic: 2.379270553588867 | update steps:4825\n",
            "epoch: 24 | step: 49 | actor: 0.39225319027900696 | critic: 2.0806074142456055 | update steps:4850\n",
            "epoch: 24 | step: 74 | actor: 0.38884875178337097 | critic: 2.259753942489624 | update steps:4875\n",
            "epoch: 24 | step: 99 | actor: 0.38255298137664795 | critic: 1.9052531719207764 | update steps:4900\n",
            "epoch: 24 | step: 124 | actor: 0.3760988712310791 | critic: 2.132599115371704 | update steps:4925\n",
            "epoch: 24 | step: 149 | actor: 0.3727056682109833 | critic: 2.3063206672668457 | update steps:4950\n",
            "epoch: 24 | step: 174 | actor: 0.3652973771095276 | critic: 2.2049765586853027 | update steps:4975\n",
            "target params updated\n",
            "epoch: 24 | step: 199 | actor: 0.3639574646949768 | critic: 2.3063087463378906 | update steps:5000\n",
            "24 2.8588436983213397\n",
            "broke 5006\n",
            "epoch: 25 | step: 24 | actor: 0.43636950850486755 | critic: 3.0499191284179688 | update steps:5025\n",
            "epoch: 25 | step: 49 | actor: 0.4261226952075958 | critic: 3.275606632232666 | update steps:5050\n",
            "epoch: 25 | step: 74 | actor: 0.41923171281814575 | critic: 3.015078067779541 | update steps:5075\n",
            "epoch: 25 | step: 99 | actor: 0.40962159633636475 | critic: 2.813983917236328 | update steps:5100\n",
            "epoch: 25 | step: 124 | actor: 0.40916746854782104 | critic: 3.1685261726379395 | update steps:5125\n",
            "epoch: 25 | step: 149 | actor: 0.40837299823760986 | critic: 3.0436747074127197 | update steps:5150\n",
            "epoch: 25 | step: 174 | actor: 0.3893051743507385 | critic: 2.861156702041626 | update steps:5175\n",
            "epoch: 25 | step: 199 | actor: 0.3930967152118683 | critic: 2.8327102661132812 | update steps:5200\n",
            "25 2.863091507678038\n",
            "broke 5001\n",
            "epoch: 26 | step: 24 | actor: 0.36812883615493774 | critic: 2.6193768978118896 | update steps:5225\n",
            "epoch: 26 | step: 49 | actor: 0.36526498198509216 | critic: 2.8657798767089844 | update steps:5250\n",
            "epoch: 26 | step: 74 | actor: 0.3672436475753784 | critic: 2.7778499126434326 | update steps:5275\n",
            "epoch: 26 | step: 99 | actor: 0.3644064962863922 | critic: 2.556091785430908 | update steps:5300\n",
            "epoch: 26 | step: 124 | actor: 0.36756348609924316 | critic: 2.9290454387664795 | update steps:5325\n",
            "epoch: 26 | step: 149 | actor: 0.3534112572669983 | critic: 2.7635550498962402 | update steps:5350\n",
            "epoch: 26 | step: 174 | actor: 0.3575422167778015 | critic: 2.447641372680664 | update steps:5375\n",
            "epoch: 26 | step: 199 | actor: 0.3506709635257721 | critic: 2.341205358505249 | update steps:5400\n",
            "26 2.8697927108304273\n",
            "broke 5003\n",
            "epoch: 27 | step: 24 | actor: 0.42149582505226135 | critic: 2.7352943420410156 | update steps:5425\n",
            "epoch: 27 | step: 49 | actor: 0.40831488370895386 | critic: 2.67183256149292 | update steps:5450\n",
            "epoch: 27 | step: 74 | actor: 0.40892452001571655 | critic: 2.388983726501465 | update steps:5475\n",
            "epoch: 27 | step: 99 | actor: 0.3958834409713745 | critic: 2.3071212768554688 | update steps:5500\n",
            "epoch: 27 | step: 124 | actor: 0.3965265452861786 | critic: 2.233506202697754 | update steps:5525\n",
            "epoch: 27 | step: 149 | actor: 0.3868502974510193 | critic: 2.179880142211914 | update steps:5550\n",
            "epoch: 27 | step: 174 | actor: 0.3731817901134491 | critic: 2.3831002712249756 | update steps:5575\n",
            "epoch: 27 | step: 199 | actor: 0.38903287053108215 | critic: 2.1562042236328125 | update steps:5600\n",
            "27 2.8948902670151035\n",
            "broke 5006\n",
            "epoch: 28 | step: 24 | actor: 0.3733537793159485 | critic: 2.379218101501465 | update steps:5625\n",
            "epoch: 28 | step: 49 | actor: 0.3765602111816406 | critic: 2.4737679958343506 | update steps:5650\n",
            "epoch: 28 | step: 74 | actor: 0.36218881607055664 | critic: 2.632969856262207 | update steps:5675\n",
            "epoch: 28 | step: 99 | actor: 0.3562721908092499 | critic: 2.4123220443725586 | update steps:5700\n",
            "epoch: 28 | step: 124 | actor: 0.35323452949523926 | critic: 2.488309144973755 | update steps:5725\n",
            "epoch: 28 | step: 149 | actor: 0.34769898653030396 | critic: 2.3307666778564453 | update steps:5750\n",
            "epoch: 28 | step: 174 | actor: 0.3469915986061096 | critic: 2.563340187072754 | update steps:5775\n",
            "epoch: 28 | step: 199 | actor: 0.34254127740859985 | critic: 2.4904513359069824 | update steps:5800\n",
            "28 2.919625500413938\n",
            "broke 5006\n",
            "epoch: 29 | step: 24 | actor: 0.39976102113723755 | critic: 2.8998947143554688 | update steps:5825\n",
            "epoch: 29 | step: 49 | actor: 0.40956294536590576 | critic: 2.537344217300415 | update steps:5850\n",
            "epoch: 29 | step: 74 | actor: 0.39617201685905457 | critic: 2.694305896759033 | update steps:5875\n",
            "epoch: 29 | step: 99 | actor: 0.3942084312438965 | critic: 2.6945199966430664 | update steps:5900\n",
            "epoch: 29 | step: 124 | actor: 0.39323189854621887 | critic: 2.748154640197754 | update steps:5925\n",
            "epoch: 29 | step: 149 | actor: 0.39275962114334106 | critic: 2.498680353164673 | update steps:5950\n",
            "epoch: 29 | step: 174 | actor: 0.38349199295043945 | critic: 2.7345638275146484 | update steps:5975\n",
            "target params updated\n",
            "epoch: 29 | step: 199 | actor: 0.3844223618507385 | critic: 2.5588319301605225 | update steps:6000\n",
            "29 2.817847099864786\n",
            "broke 5001\n",
            "epoch: 30 | step: 24 | actor: 0.374302476644516 | critic: 1.683265209197998 | update steps:6025\n",
            "epoch: 30 | step: 49 | actor: 0.3645539879798889 | critic: 1.5606682300567627 | update steps:6050\n",
            "epoch: 30 | step: 74 | actor: 0.3667893409729004 | critic: 1.7528531551361084 | update steps:6075\n",
            "epoch: 30 | step: 99 | actor: 0.3583807945251465 | critic: 1.7504799365997314 | update steps:6100\n",
            "epoch: 30 | step: 124 | actor: 0.3593505024909973 | critic: 1.7759710550308228 | update steps:6125\n",
            "epoch: 30 | step: 149 | actor: 0.36186879873275757 | critic: 1.5165843963623047 | update steps:6150\n",
            "epoch: 30 | step: 174 | actor: 0.3490273356437683 | critic: 1.7152700424194336 | update steps:6175\n",
            "epoch: 30 | step: 199 | actor: 0.3549586534500122 | critic: 1.5826159715652466 | update steps:6200\n",
            "30 2.8606171245574217\n",
            "broke 5004\n",
            "epoch: 31 | step: 24 | actor: 0.37881287932395935 | critic: 2.5718398094177246 | update steps:6225\n",
            "epoch: 31 | step: 49 | actor: 0.37147581577301025 | critic: 2.4162659645080566 | update steps:6250\n",
            "epoch: 31 | step: 74 | actor: 0.37494713068008423 | critic: 2.6118037700653076 | update steps:6275\n",
            "epoch: 31 | step: 99 | actor: 0.36925333738327026 | critic: 2.256834030151367 | update steps:6300\n",
            "epoch: 31 | step: 124 | actor: 0.3666192293167114 | critic: 2.1072733402252197 | update steps:6325\n",
            "epoch: 31 | step: 149 | actor: 0.3641625642776489 | critic: 2.276874542236328 | update steps:6350\n",
            "epoch: 31 | step: 174 | actor: 0.3598335385322571 | critic: 2.3468191623687744 | update steps:6375\n",
            "epoch: 31 | step: 199 | actor: 0.3521692156791687 | critic: 2.2386059761047363 | update steps:6400\n",
            "31 2.7423485207269485\n",
            "broke 5002\n",
            "epoch: 32 | step: 24 | actor: 0.38421040773391724 | critic: 1.9451707601547241 | update steps:6425\n",
            "epoch: 32 | step: 49 | actor: 0.3817545771598816 | critic: 2.102992534637451 | update steps:6450\n",
            "epoch: 32 | step: 74 | actor: 0.37045416235923767 | critic: 2.0030016899108887 | update steps:6475\n",
            "epoch: 32 | step: 99 | actor: 0.3658106327056885 | critic: 2.204705238342285 | update steps:6500\n",
            "epoch: 32 | step: 124 | actor: 0.3731115162372589 | critic: 2.2486345767974854 | update steps:6525\n",
            "epoch: 32 | step: 149 | actor: 0.3634029030799866 | critic: 1.9052311182022095 | update steps:6550\n",
            "epoch: 32 | step: 174 | actor: 0.35619431734085083 | critic: 2.184222936630249 | update steps:6575\n",
            "epoch: 32 | step: 199 | actor: 0.3514522910118103 | critic: 2.095388412475586 | update steps:6600\n",
            "32 2.839866010090136\n",
            "broke 5005\n",
            "epoch: 33 | step: 24 | actor: 0.4059315323829651 | critic: 2.1570487022399902 | update steps:6625\n",
            "epoch: 33 | step: 49 | actor: 0.40171727538108826 | critic: 2.173610210418701 | update steps:6650\n",
            "epoch: 33 | step: 74 | actor: 0.3867097496986389 | critic: 2.1438989639282227 | update steps:6675\n",
            "epoch: 33 | step: 99 | actor: 0.3882654905319214 | critic: 2.1341991424560547 | update steps:6700\n",
            "epoch: 33 | step: 124 | actor: 0.38555213809013367 | critic: 2.0608415603637695 | update steps:6725\n",
            "epoch: 33 | step: 149 | actor: 0.379058837890625 | critic: 1.9335808753967285 | update steps:6750\n",
            "epoch: 33 | step: 174 | actor: 0.3769901990890503 | critic: 2.264458656311035 | update steps:6775\n",
            "epoch: 33 | step: 199 | actor: 0.37360450625419617 | critic: 2.004905939102173 | update steps:6800\n",
            "33 2.728120302732647\n",
            "broke 5003\n",
            "epoch: 34 | step: 24 | actor: 0.3873355984687805 | critic: 2.023094654083252 | update steps:6825\n",
            "epoch: 34 | step: 49 | actor: 0.381363183259964 | critic: 2.1250011920928955 | update steps:6850\n",
            "epoch: 34 | step: 74 | actor: 0.3678971529006958 | critic: 1.8725361824035645 | update steps:6875\n",
            "epoch: 34 | step: 99 | actor: 0.368544340133667 | critic: 2.174118995666504 | update steps:6900\n",
            "epoch: 34 | step: 124 | actor: 0.3582038879394531 | critic: 2.1385622024536133 | update steps:6925\n",
            "epoch: 34 | step: 149 | actor: 0.36237022280693054 | critic: 1.996579647064209 | update steps:6950\n",
            "epoch: 34 | step: 174 | actor: 0.35376134514808655 | critic: 2.124871253967285 | update steps:6975\n",
            "target params updated\n",
            "epoch: 34 | step: 199 | actor: 0.34112244844436646 | critic: 1.84855055809021 | update steps:7000\n",
            "34 2.8622633828742403\n",
            "broke 5003\n",
            "epoch: 35 | step: 24 | actor: 0.3742183446884155 | critic: 2.485136032104492 | update steps:7025\n",
            "epoch: 35 | step: 49 | actor: 0.3676339387893677 | critic: 2.2338380813598633 | update steps:7050\n",
            "epoch: 35 | step: 74 | actor: 0.3702120780944824 | critic: 2.621427297592163 | update steps:7075\n",
            "epoch: 35 | step: 99 | actor: 0.3600994944572449 | critic: 2.466282367706299 | update steps:7100\n",
            "epoch: 35 | step: 124 | actor: 0.36128851771354675 | critic: 2.4710655212402344 | update steps:7125\n",
            "epoch: 35 | step: 149 | actor: 0.3584887981414795 | critic: 2.1989927291870117 | update steps:7150\n",
            "epoch: 35 | step: 174 | actor: 0.35712331533432007 | critic: 2.4663853645324707 | update steps:7175\n",
            "epoch: 35 | step: 199 | actor: 0.34799519181251526 | critic: 2.433865547180176 | update steps:7200\n",
            "35 2.782666809729614\n",
            "broke 5003\n",
            "epoch: 36 | step: 24 | actor: 0.3739214539527893 | critic: 1.6116044521331787 | update steps:7225\n",
            "epoch: 36 | step: 49 | actor: 0.3672991991043091 | critic: 1.4941309690475464 | update steps:7250\n",
            "epoch: 36 | step: 74 | actor: 0.358578622341156 | critic: 1.5831363201141357 | update steps:7275\n",
            "epoch: 36 | step: 99 | actor: 0.35202935338020325 | critic: 1.483982801437378 | update steps:7300\n",
            "epoch: 36 | step: 124 | actor: 0.35710352659225464 | critic: 1.8213424682617188 | update steps:7325\n",
            "epoch: 36 | step: 149 | actor: 0.3508700132369995 | critic: 1.729654312133789 | update steps:7350\n",
            "epoch: 36 | step: 174 | actor: 0.3436935544013977 | critic: 1.526807427406311 | update steps:7375\n",
            "epoch: 36 | step: 199 | actor: 0.34672024846076965 | critic: 1.5291268825531006 | update steps:7400\n",
            "36 2.6743011964556254\n",
            "broke 5004\n",
            "epoch: 37 | step: 24 | actor: 0.4310612082481384 | critic: 2.4604053497314453 | update steps:7425\n",
            "epoch: 37 | step: 49 | actor: 0.41343843936920166 | critic: 2.752162456512451 | update steps:7450\n",
            "epoch: 37 | step: 74 | actor: 0.40000221133232117 | critic: 2.38590669631958 | update steps:7475\n",
            "epoch: 37 | step: 99 | actor: 0.3989452123641968 | critic: 2.5355384349823 | update steps:7500\n",
            "epoch: 37 | step: 124 | actor: 0.38205385208129883 | critic: 2.559229612350464 | update steps:7525\n",
            "epoch: 37 | step: 149 | actor: 0.3833457827568054 | critic: 2.519629955291748 | update steps:7550\n",
            "epoch: 37 | step: 174 | actor: 0.3725794553756714 | critic: 2.5379552841186523 | update steps:7575\n",
            "epoch: 37 | step: 199 | actor: 0.37571096420288086 | critic: 2.3770642280578613 | update steps:7600\n",
            "37 2.6344968839548346\n",
            "broke 5004\n",
            "epoch: 38 | step: 24 | actor: 0.4130847454071045 | critic: 3.672677993774414 | update steps:7625\n",
            "epoch: 38 | step: 49 | actor: 0.40591755509376526 | critic: 3.6263010501861572 | update steps:7650\n",
            "epoch: 38 | step: 74 | actor: 0.3954671621322632 | critic: 3.5932064056396484 | update steps:7675\n",
            "epoch: 38 | step: 99 | actor: 0.3988274335861206 | critic: 3.5739922523498535 | update steps:7700\n",
            "epoch: 38 | step: 124 | actor: 0.3896138370037079 | critic: 3.399426221847534 | update steps:7725\n",
            "epoch: 38 | step: 149 | actor: 0.390760213136673 | critic: 3.5579981803894043 | update steps:7750\n",
            "epoch: 38 | step: 174 | actor: 0.3841592073440552 | critic: 3.3405628204345703 | update steps:7775\n",
            "epoch: 38 | step: 199 | actor: 0.3893364369869232 | critic: 3.1910905838012695 | update steps:7800\n",
            "38 2.7561357617667497\n",
            "broke 5004\n",
            "epoch: 39 | step: 24 | actor: 0.40525928139686584 | critic: 3.051198959350586 | update steps:7825\n",
            "epoch: 39 | step: 49 | actor: 0.39880138635635376 | critic: 2.8017125129699707 | update steps:7850\n",
            "epoch: 39 | step: 74 | actor: 0.3848137855529785 | critic: 2.854524612426758 | update steps:7875\n",
            "epoch: 39 | step: 99 | actor: 0.3932560086250305 | critic: 2.8641200065612793 | update steps:7900\n",
            "epoch: 39 | step: 124 | actor: 0.37945258617401123 | critic: 2.940237045288086 | update steps:7925\n",
            "epoch: 39 | step: 149 | actor: 0.3776330351829529 | critic: 2.7267448902130127 | update steps:7950\n",
            "epoch: 39 | step: 174 | actor: 0.3666074275970459 | critic: 2.7687315940856934 | update steps:7975\n",
            "target params updated\n",
            "epoch: 39 | step: 199 | actor: 0.3631497323513031 | critic: 3.0646800994873047 | update steps:8000\n",
            "39 2.717170812375009\n",
            "broke 5002\n",
            "epoch: 40 | step: 24 | actor: 0.3939141631126404 | critic: 2.8461318016052246 | update steps:8025\n",
            "epoch: 40 | step: 49 | actor: 0.3825753927230835 | critic: 2.7494349479675293 | update steps:8050\n",
            "epoch: 40 | step: 74 | actor: 0.384080708026886 | critic: 2.7226662635803223 | update steps:8075\n",
            "epoch: 40 | step: 99 | actor: 0.3768301010131836 | critic: 2.769965648651123 | update steps:8100\n",
            "epoch: 40 | step: 124 | actor: 0.36492109298706055 | critic: 3.1063032150268555 | update steps:8125\n",
            "epoch: 40 | step: 149 | actor: 0.3640901446342468 | critic: 2.524268627166748 | update steps:8150\n",
            "epoch: 40 | step: 174 | actor: 0.3577173352241516 | critic: 2.5811219215393066 | update steps:8175\n",
            "epoch: 40 | step: 199 | actor: 0.34837087988853455 | critic: 2.8342254161834717 | update steps:8200\n",
            "40 2.740489664671089\n",
            "broke 5002\n",
            "epoch: 41 | step: 24 | actor: 0.39822226762771606 | critic: 2.1183342933654785 | update steps:8225\n",
            "epoch: 41 | step: 49 | actor: 0.39601361751556396 | critic: 2.2888636589050293 | update steps:8250\n",
            "epoch: 41 | step: 74 | actor: 0.3919537663459778 | critic: 2.1038150787353516 | update steps:8275\n",
            "epoch: 41 | step: 99 | actor: 0.3927586078643799 | critic: 2.3001880645751953 | update steps:8300\n",
            "epoch: 41 | step: 124 | actor: 0.37613263726234436 | critic: 2.410085678100586 | update steps:8325\n",
            "epoch: 41 | step: 149 | actor: 0.36838066577911377 | critic: 2.6054763793945312 | update steps:8350\n",
            "epoch: 41 | step: 174 | actor: 0.37286293506622314 | critic: 2.1329753398895264 | update steps:8375\n",
            "epoch: 41 | step: 199 | actor: 0.3817410171031952 | critic: 2.086737871170044 | update steps:8400\n",
            "41 2.695927195696039\n",
            "broke 5002\n",
            "epoch: 42 | step: 24 | actor: 0.4078938066959381 | critic: 2.5397098064422607 | update steps:8425\n",
            "epoch: 42 | step: 49 | actor: 0.399202823638916 | critic: 2.5726819038391113 | update steps:8450\n",
            "epoch: 42 | step: 74 | actor: 0.3957063853740692 | critic: 2.7616686820983887 | update steps:8475\n",
            "epoch: 42 | step: 99 | actor: 0.39090514183044434 | critic: 2.5446510314941406 | update steps:8500\n",
            "epoch: 42 | step: 124 | actor: 0.4004315137863159 | critic: 2.6906356811523438 | update steps:8525\n",
            "epoch: 42 | step: 149 | actor: 0.3849104940891266 | critic: 2.8410520553588867 | update steps:8550\n",
            "epoch: 42 | step: 174 | actor: 0.3808788061141968 | critic: 2.6242213249206543 | update steps:8575\n",
            "epoch: 42 | step: 199 | actor: 0.37587183713912964 | critic: 2.7555696964263916 | update steps:8600\n",
            "42 2.659176095176974\n",
            "broke 5001\n",
            "epoch: 43 | step: 24 | actor: 0.39366066455841064 | critic: 2.0331497192382812 | update steps:8625\n",
            "epoch: 43 | step: 49 | actor: 0.3949658274650574 | critic: 1.973473072052002 | update steps:8650\n",
            "epoch: 43 | step: 74 | actor: 0.3802338242530823 | critic: 1.8208446502685547 | update steps:8675\n",
            "epoch: 43 | step: 99 | actor: 0.3824276924133301 | critic: 2.1048617362976074 | update steps:8700\n",
            "epoch: 43 | step: 124 | actor: 0.38052207231521606 | critic: 1.8586360216140747 | update steps:8725\n",
            "epoch: 43 | step: 149 | actor: 0.37723004817962646 | critic: 1.9751981496810913 | update steps:8750\n",
            "epoch: 43 | step: 174 | actor: 0.3652813732624054 | critic: 1.7103285789489746 | update steps:8775\n",
            "epoch: 43 | step: 199 | actor: 0.3747817277908325 | critic: 1.9126123189926147 | update steps:8800\n",
            "43 2.6206727946210178\n",
            "broke 5002\n",
            "epoch: 44 | step: 24 | actor: 0.3854001760482788 | critic: 3.1154661178588867 | update steps:8825\n",
            "epoch: 44 | step: 49 | actor: 0.37057292461395264 | critic: 2.8703088760375977 | update steps:8850\n",
            "epoch: 44 | step: 74 | actor: 0.3741080164909363 | critic: 2.935786247253418 | update steps:8875\n",
            "epoch: 44 | step: 99 | actor: 0.3700571656227112 | critic: 2.661566972732544 | update steps:8900\n",
            "epoch: 44 | step: 124 | actor: 0.3623213469982147 | critic: 2.712522506713867 | update steps:8925\n",
            "epoch: 44 | step: 149 | actor: 0.3563205897808075 | critic: 2.6368799209594727 | update steps:8950\n",
            "epoch: 44 | step: 174 | actor: 0.3557432293891907 | critic: 2.6446688175201416 | update steps:8975\n",
            "target params updated\n",
            "epoch: 44 | step: 199 | actor: 0.3615155816078186 | critic: 2.498056173324585 | update steps:9000\n",
            "44 2.6905885415834256\n",
            "broke 5001\n",
            "epoch: 45 | step: 24 | actor: 0.3746339678764343 | critic: 1.9655898809432983 | update steps:9025\n",
            "epoch: 45 | step: 49 | actor: 0.3673195540904999 | critic: 2.095224380493164 | update steps:9050\n",
            "epoch: 45 | step: 74 | actor: 0.3683573603630066 | critic: 2.03763484954834 | update steps:9075\n",
            "epoch: 45 | step: 99 | actor: 0.3670560419559479 | critic: 2.1711292266845703 | update steps:9100\n",
            "epoch: 45 | step: 124 | actor: 0.3548039495944977 | critic: 1.9651554822921753 | update steps:9125\n",
            "epoch: 45 | step: 149 | actor: 0.35340416431427 | critic: 2.0661728382110596 | update steps:9150\n",
            "epoch: 45 | step: 174 | actor: 0.3547675311565399 | critic: 2.24687123298645 | update steps:9175\n",
            "epoch: 45 | step: 199 | actor: 0.338015615940094 | critic: 1.8516275882720947 | update steps:9200\n",
            "45 2.665775442637626\n",
            "broke 5002\n",
            "epoch: 46 | step: 24 | actor: 0.3654021918773651 | critic: 2.0369458198547363 | update steps:9225\n",
            "epoch: 46 | step: 49 | actor: 0.3634466528892517 | critic: 2.0426390171051025 | update steps:9250\n",
            "epoch: 46 | step: 74 | actor: 0.3642466366291046 | critic: 2.233365774154663 | update steps:9275\n",
            "epoch: 46 | step: 99 | actor: 0.3591986894607544 | critic: 1.8070367574691772 | update steps:9300\n",
            "epoch: 46 | step: 124 | actor: 0.35175904631614685 | critic: 2.0595359802246094 | update steps:9325\n",
            "epoch: 46 | step: 149 | actor: 0.3509666323661804 | critic: 2.190629005432129 | update steps:9350\n",
            "epoch: 46 | step: 174 | actor: 0.35250329971313477 | critic: 2.2105085849761963 | update steps:9375\n",
            "epoch: 46 | step: 199 | actor: 0.34420472383499146 | critic: 2.0365641117095947 | update steps:9400\n",
            "46 2.649343739311051\n",
            "broke 5004\n",
            "epoch: 47 | step: 24 | actor: 0.42039018869400024 | critic: 2.1810877323150635 | update steps:9425\n",
            "epoch: 47 | step: 49 | actor: 0.4159228205680847 | critic: 2.3375446796417236 | update steps:9450\n",
            "epoch: 47 | step: 74 | actor: 0.4077747166156769 | critic: 2.4837050437927246 | update steps:9475\n",
            "epoch: 47 | step: 99 | actor: 0.397108256816864 | critic: 2.510468006134033 | update steps:9500\n",
            "epoch: 47 | step: 124 | actor: 0.38537758588790894 | critic: 2.3104376792907715 | update steps:9525\n",
            "epoch: 47 | step: 149 | actor: 0.38930827379226685 | critic: 2.269479751586914 | update steps:9550\n",
            "epoch: 47 | step: 174 | actor: 0.3817601799964905 | critic: 2.5337843894958496 | update steps:9575\n",
            "epoch: 47 | step: 199 | actor: 0.3812379837036133 | critic: 2.181497573852539 | update steps:9600\n",
            "47 2.6375771530922325\n",
            "broke 5004\n",
            "epoch: 48 | step: 24 | actor: 0.3945728540420532 | critic: 2.3841607570648193 | update steps:9625\n",
            "epoch: 48 | step: 49 | actor: 0.3874993920326233 | critic: 2.098794937133789 | update steps:9650\n",
            "epoch: 48 | step: 74 | actor: 0.3839559555053711 | critic: 2.382178544998169 | update steps:9675\n",
            "epoch: 48 | step: 99 | actor: 0.37842392921447754 | critic: 2.3278799057006836 | update steps:9700\n",
            "epoch: 48 | step: 124 | actor: 0.3780713379383087 | critic: 2.084192991256714 | update steps:9725\n",
            "epoch: 48 | step: 149 | actor: 0.3791161775588989 | critic: 2.454148292541504 | update steps:9750\n",
            "epoch: 48 | step: 174 | actor: 0.3613799214363098 | critic: 1.9816980361938477 | update steps:9775\n",
            "epoch: 48 | step: 199 | actor: 0.36502790451049805 | critic: 2.301023006439209 | update steps:9800\n",
            "48 2.6087006295080375\n",
            "broke 5004\n",
            "epoch: 49 | step: 24 | actor: 0.44556477665901184 | critic: 3.8256711959838867 | update steps:9825\n",
            "epoch: 49 | step: 49 | actor: 0.43687903881073 | critic: 3.6930060386657715 | update steps:9850\n",
            "epoch: 49 | step: 74 | actor: 0.4346466660499573 | critic: 3.697016954421997 | update steps:9875\n",
            "epoch: 49 | step: 99 | actor: 0.4212915301322937 | critic: 3.8242363929748535 | update steps:9900\n",
            "epoch: 49 | step: 124 | actor: 0.41406917572021484 | critic: 3.952129364013672 | update steps:9925\n",
            "epoch: 49 | step: 149 | actor: 0.4090059995651245 | critic: 3.761378765106201 | update steps:9950\n",
            "epoch: 49 | step: 174 | actor: 0.4030967950820923 | critic: 3.35237193107605 | update steps:9975\n",
            "target params updated\n",
            "epoch: 49 | step: 199 | actor: 0.3937036991119385 | critic: 3.36921763420105 | update steps:10000\n",
            "49 2.6510256335722953\n",
            "broke 5004\n",
            "epoch: 50 | step: 24 | actor: 0.4315453767776489 | critic: 3.0271408557891846 | update steps:10025\n",
            "epoch: 50 | step: 49 | actor: 0.42219462990760803 | critic: 2.9147493839263916 | update steps:10050\n",
            "epoch: 50 | step: 74 | actor: 0.40858399868011475 | critic: 3.681389570236206 | update steps:10075\n",
            "epoch: 50 | step: 99 | actor: 0.40278488397598267 | critic: 3.2719340324401855 | update steps:10100\n",
            "epoch: 50 | step: 124 | actor: 0.4047122001647949 | critic: 3.3925728797912598 | update steps:10125\n",
            "epoch: 50 | step: 149 | actor: 0.3953009247779846 | critic: 2.9820830821990967 | update steps:10150\n",
            "epoch: 50 | step: 174 | actor: 0.39287057518959045 | critic: 3.0939977169036865 | update steps:10175\n",
            "epoch: 50 | step: 199 | actor: 0.3791944980621338 | critic: 3.266854763031006 | update steps:10200\n",
            "50 2.5822251167988957\n",
            "broke 5003\n",
            "epoch: 51 | step: 24 | actor: 0.41111046075820923 | critic: 2.2801356315612793 | update steps:10225\n",
            "epoch: 51 | step: 49 | actor: 0.39916473627090454 | critic: 2.4339261054992676 | update steps:10250\n",
            "epoch: 51 | step: 74 | actor: 0.3924623131752014 | critic: 2.3016722202301025 | update steps:10275\n",
            "epoch: 51 | step: 99 | actor: 0.38756275177001953 | critic: 2.0287110805511475 | update steps:10300\n",
            "epoch: 51 | step: 124 | actor: 0.3782816529273987 | critic: 2.5252952575683594 | update steps:10325\n",
            "epoch: 51 | step: 149 | actor: 0.3730661869049072 | critic: 2.26741623878479 | update steps:10350\n",
            "epoch: 51 | step: 174 | actor: 0.3702656030654907 | critic: 2.3284707069396973 | update steps:10375\n",
            "epoch: 51 | step: 199 | actor: 0.3748605251312256 | critic: 1.9583041667938232 | update steps:10400\n",
            "51 2.603667842401687\n",
            "broke 5004\n",
            "epoch: 52 | step: 24 | actor: 0.3690861463546753 | critic: 1.735953688621521 | update steps:10425\n",
            "epoch: 52 | step: 49 | actor: 0.37398386001586914 | critic: 2.0322155952453613 | update steps:10450\n",
            "epoch: 52 | step: 74 | actor: 0.36609435081481934 | critic: 1.9099006652832031 | update steps:10475\n",
            "epoch: 52 | step: 99 | actor: 0.3515990674495697 | critic: 1.5475579500198364 | update steps:10500\n",
            "epoch: 52 | step: 124 | actor: 0.35800033807754517 | critic: 1.8756611347198486 | update steps:10525\n",
            "epoch: 52 | step: 149 | actor: 0.345331072807312 | critic: 1.5708335638046265 | update steps:10550\n",
            "epoch: 52 | step: 174 | actor: 0.3441287577152252 | critic: 1.7853500843048096 | update steps:10575\n",
            "epoch: 52 | step: 199 | actor: 0.3418801426887512 | critic: 1.794107437133789 | update steps:10600\n",
            "52 2.6513829693040116\n",
            "broke 5002\n",
            "epoch: 53 | step: 24 | actor: 0.46865028142929077 | critic: 3.589165210723877 | update steps:10625\n",
            "epoch: 53 | step: 49 | actor: 0.45368459820747375 | critic: 3.022705078125 | update steps:10650\n",
            "epoch: 53 | step: 74 | actor: 0.44367730617523193 | critic: 3.4000158309936523 | update steps:10675\n",
            "epoch: 53 | step: 99 | actor: 0.4373302161693573 | critic: 3.5062618255615234 | update steps:10700\n",
            "epoch: 53 | step: 124 | actor: 0.42461180686950684 | critic: 3.1064131259918213 | update steps:10725\n",
            "epoch: 53 | step: 149 | actor: 0.4194795489311218 | critic: 2.98703932762146 | update steps:10750\n",
            "epoch: 53 | step: 174 | actor: 0.41061970591545105 | critic: 3.345203161239624 | update steps:10775\n",
            "epoch: 53 | step: 199 | actor: 0.4167707562446594 | critic: 3.004976749420166 | update steps:10800\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-7f57d656593e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0meval_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeanPolicyEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexploitability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnash_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_policy_to_pyspiel_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabular_policy_from_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/open_spiel/python/policy.py\u001b[0m in \u001b[0;36mtabular_policy_from_callable\u001b[0;34m(game, callable_policy, players)\u001b[0m\n\u001b[1;32m    491\u001b[0m   \u001b[0mtabular_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabularPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstate_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabular_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0maction_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     infostate_policy = [\n\u001b[1;32m    495\u001b[0m         \u001b[0maction_probabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e6b77ea3e46b>\u001b[0m in \u001b[0;36maction_probabilities\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     16\u001b[0m             state.information_state_tensor(current_player), dtype=jnp.float32)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_state_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPi_bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegal_actions_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10e-20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haiku/_src/multi_transform.py\u001b[0m in \u001b[0;36mapply_fn\u001b[0;34m(params, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mapply_without_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_apply_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m       \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_apply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haiku/_src/transform.py\u001b[0m in \u001b[0;36mapply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedTracerError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedTracerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munexpected_tracer_hint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haiku/_src/multi_transform.py\u001b[0m in \u001b[0;36mapply_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0;34m\"\"\"Applies the transformed function at the given inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_leaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-7f57d656593e>\u001b[0m in \u001b[0;36marmac_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset_network_and_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0marmac_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlayerNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'actor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCriticNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'critic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haiku/_src/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     if (config.get_config().module_auto_repr and\n\u001b[1;32m    126\u001b[0m         getattr(module, \"AUTO_REPR\", True)):\n\u001b[0;32m--> 127\u001b[0;31m       \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haiku/_src/utils.py\u001b[0m in \u001b[0;36mauto_repr\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m       (name + \"=\", kwargs[name]) for name in kwargs if name not in argspec.args)\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m   single_line = cls.__name__ + \"({})\".format(\", \".join(\n\u001b[0m\u001b[1;32m     88\u001b[0m       name + repr(value) for name, value in names_and_values))\n\u001b[1;32m     89\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_line\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%time armac.main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "armac._nash_convs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fJsESlqHJKF",
        "outputId": "bbf8957c-d1c3-4e3c-99be-4813da1e983b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.862598978915228,\n",
              " 2.943193575683223,\n",
              " 2.416704425986689,\n",
              " 2.5779702130968296,\n",
              " 2.4184459723890614,\n",
              " 2.3552043621815555,\n",
              " 2.5085078838738086,\n",
              " 2.565827591273673,\n",
              " 2.4650944734523828,\n",
              " 2.5718096796791396,\n",
              " 2.470876263906133,\n",
              " 2.540481987078114,\n",
              " 2.431816435554695,\n",
              " 2.4654010990694792,\n",
              " 2.483255933202364]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = armac._network_buffer[-3]"
      ],
      "metadata": {
        "id": "SIMDgegFIwGN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looking At Network Predictions"
      ],
      "metadata": {
        "id": "6zZsmquVAVTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = rl_environment.Environment(\"leduc_poker\")\n",
        "time_step = env.reset()\n",
        "while True:\n",
        "    current_player = time_step.observations['current_player']\n",
        "    history = jnp.array(time_step.observations['info_state'])\n",
        "    output = armac._network(params, None, history)\n",
        "    output = output[current_player]\n",
        "    \n",
        "    mean_policy = jax.nn.softmax(output.mean_policy)\n",
        "    q_values = output.q_values\n",
        "    regrets = output.avg_regret\n",
        "    advs = jax.nn.relu(regrets)\n",
        "    adv_policy = advs / advs.sum()\n",
        "    samp_regret = q_values - np.sum(q_values * adv_policy)\n",
        "    print(f'regrets: {regrets}')\n",
        "    print(f'adv_policy {adv_policy}') \n",
        "    print(f'q_values {q_values}')\n",
        "    print(f'samp_regret {samp_regret}')\n",
        "    print(f'mean_policy {mean_policy}')\n",
        "\n",
        "    action = np.random.choice([0, 1, 2], p=mean_policy)\n",
        "\n",
        "    if time_step.last():\n",
        "        print(time_step.observations['rewards'])\n"
      ],
      "metadata": {
        "id": "uvCFd_K_I28V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z37CGIIrHtLC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO0dsayyn1O8J4Xj3SoLTvw",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}